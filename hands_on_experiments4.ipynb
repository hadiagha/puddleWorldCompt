{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_puddle\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.dqn import MlpPolicy as DQNPolicy\n",
    "from stable_baselines3.ppo import MlpPolicy as PPOPolicy\n",
    "\n",
    "import time\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython import display\n",
    "import pyvirtualdisplay\n",
    "import cv2\n",
    "\n",
    "import libs.tiles3 as tc\n",
    "import random\n",
    "\n",
    "\n",
    "# ingore warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_seed = 0\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#some functions to help the visualization and interaction wit the environment\n",
    "\n",
    "def visualize(frames, video_name = \"/Video/video.mp4\"):\n",
    "    # Saves the frames as an mp4 video using cv2\n",
    "    video_path = video_name\n",
    "    height, width, _ = frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    video_writer = cv2.VideoWriter(video_path, fourcc, 30, (width, height))\n",
    "    for frame in frames:\n",
    "        video_writer.write(frame)\n",
    "    video_writer.release()\n",
    "\n",
    "def online_rendering(image):\n",
    "    #Visualize one frame of the image in a display\n",
    "    ax.axis('off')\n",
    "    img_with_frame = np.zeros((image.shape[0]+2, image.shape[1]+2, 3), dtype=np.uint8)\n",
    "    img_with_frame[1:-1, 1:-1, :] = image\n",
    "    ax.imshow(img_with_frame)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "\n",
    "def prepare_display():\n",
    "  #Prepares display for onine rendering of the frames in the game\n",
    "  _display = pyvirtualdisplay.Display(visible=False,size=(1400, 900))\n",
    "  _ = _display.start()\n",
    "  fig, ax = plt.subplots(figsize=(5, 5))\n",
    "  ax.axis('off')\n",
    "\n",
    "\n",
    "def get_action():\n",
    "    action = None\n",
    "    while action not in [\"w\", \"a\", \"s\", \"d\", \"W\", \"A\", \"S\", \"D\"]:\n",
    "        action = input(\"Enter action (w/a/s/d): \")\n",
    "    if action == \"w\":\n",
    "        return 3\n",
    "    elif action == \"a\":\n",
    "        return 0\n",
    "    elif action == \"s\":\n",
    "        return 2\n",
    "    elif action == \"d\":\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Different Environment Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "json_file = f\"/Users/hadiaghazadeh/Library/CloudStorage/OneDrive-UniversityofCalgary/@upperboundCompetition/gym-puddle/gym_puddle/env_configs/pw1.json\"\n",
    "\n",
    "with open(json_file) as f:\n",
    "  env_setup = json.load(f)\n",
    "\n",
    "\n",
    "env = gym.make(\n",
    "  \"PuddleWorld-v0\",\n",
    "  start=env_setup[\"start\"],\n",
    "  goal=env_setup[\"goal\"],\n",
    "  goal_threshold=env_setup[\"goal_threshold\"],\n",
    "  noise=env_setup[\"noise\"],\n",
    "  thrust=env_setup[\"thrust\"],\n",
    "  puddle_top_left=env_setup[\"puddle_top_left\"],\n",
    "  puddle_width=env_setup[\"puddle_width\"],\n",
    ")\n",
    "\n",
    "\n",
    "obs, info = env.reset()\n",
    "image = env.render()\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "online_rendering(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start position: [0.2 0.4]\n",
      "goal position: [1. 1.]\n",
      "goal threshold: 0.1\n",
      "action noise: 0.01\n",
      "agent's thrust: 0.05\n",
      "puddle top left positions: [array([0.  , 0.85]), array([0.35, 0.9 ])]\n",
      "puddle widths and heights: [array([0.55, 0.2 ]), array([0.2, 0.6])]\n",
      "action space: [array([-0.05,  0.  ]), array([0.05, 0.  ]), array([ 0.  , -0.05]), array([0.  , 0.05])]\n",
      "observation space: Box(0.0, 1.0, (2,), float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"start position:\", env.get_wrapper_attr(\"start\"))\n",
    "print(\"goal position:\", env.get_wrapper_attr(\"goal\"))\n",
    "print(\"goal threshold:\", env.get_wrapper_attr(\"goal_threshold\"))\n",
    "print(\"action noise:\", env.get_wrapper_attr(\"noise\"))\n",
    "print(\"agent's thrust:\", env.get_wrapper_attr(\"thrust\"))\n",
    "print(\"puddle top left positions:\", env.get_wrapper_attr(\"puddle_top_left\"))\n",
    "print(\"puddle widths and heights:\", env.get_wrapper_attr(\"puddle_width\"))\n",
    "print(\"action space:\", env.get_wrapper_attr(\"actions\"))\n",
    "print(\"observation space:\", env.get_wrapper_attr(\"observation_space\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kanerva coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([466, 465, 468, 467, 470, 464, 472])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from util.kanerva_ucb import BaseKanervaCoder\n",
    "num_features = 700\n",
    "n_closest = 7\n",
    "rep = BaseKanervaCoder(env.observation_space, n_prototypes= num_features, n_closest= n_closest, seed= selected_seed)\n",
    "rep.get_features_UBC(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tabularQlearning:\n",
    "    def __init__(self, num_feature, num_actions, alpha=0.1, gamma=0.99, epsilon=0.1, seed = selected_seed):\n",
    "        self.num_feature = num_feature\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Initialize Q-table with zeros\n",
    "        self.q_table = np.zeros((self.num_feature, num_actions))\n",
    "\n",
    "        self.seed = seed    \n",
    "        # Set random seed\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Choose an action using epsilon-greedy policy\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            q = self.q_table[state].sum(axis=0)\n",
    "            return q.argmax()\n",
    "    \n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"Update the Q-table using the Q-learning update rule\"\"\"\n",
    "        self.q_table[state, action] += self.alpha * (reward + self.gamma * np.max(self.q_table[next_state]) - self.q_table[state, action])\n",
    "    \n",
    "    def get_q_table(self):\n",
    "        return self.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -8629.625603117183\n",
      "Episode 20, Total Reward: -81540.8951954049\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m---> 13\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(state)\n\u001b[1;32m     14\u001b[0m     next_obs, reward, done, trunc, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     15\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m rep\u001b[38;5;241m.\u001b[39mget_features_UBC(next_obs)\n",
      "Cell \u001b[0;32mIn[42], line 16\u001b[0m, in \u001b[0;36mtabularQlearning.choose_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Set random seed\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoose_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Choose an action using epsilon-greedy policy\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## simulare the agent in the environment\n",
    "num_actions = len(env.get_wrapper_attr(\"actions\"))\n",
    "agent = tabularQlearning(num_feature= num_features, num_actions=num_actions)\n",
    "\n",
    "num_episodes = 10000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, info = env.reset()\n",
    "    state = rep.get_features_UBC(obs)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_obs, reward, done, trunc, _ = env.step(action)\n",
    "        next_state = rep.get_features_UBC(next_obs)\n",
    "        agent.update(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    if episode % 20 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " t: 0, observation: [0.1907037  0.35549508], reward: -1\n",
      " t: 1, observation: [0.19181291 0.31102303], reward: -1\n",
      " t: 2, observation: [0.2507901  0.32485311], reward: -1\n",
      " t: 3, observation: [0.27157438 0.28510275], reward: -1\n",
      " t: 4, observation: [0.33879697 0.28520139], reward: -1\n",
      " t: 5, observation: [0.34851775 0.24364274], reward: -1\n",
      " t: 6, observation: [0.36682751 0.18932022], reward: -1\n",
      " t: 7, observation: [0.43761117 0.18776349], reward: -1\n",
      " t: 8, observation: [0.47977061 0.1952417 ], reward: -1\n",
      " t: 9, observation: [0.5384836 0.2152755], reward: -1\n",
      " t: 10, observation: [0.5927603 0.2266266], reward: -1\n",
      " t: 11, observation: [0.64278781 0.21707318], reward: -1\n",
      " t: 12, observation: [0.66101731 0.28126611], reward: -1\n",
      " t: 13, observation: [0.65496028 0.2364173 ], reward: -1\n",
      " t: 14, observation: [0.66386472 0.28334474], reward: -1\n",
      " t: 15, observation: [0.65711715 0.32951246], reward: -1\n",
      " t: 16, observation: [0.71031787 0.32400919], reward: -1\n",
      " t: 17, observation: [0.71803706 0.38640076], reward: -1\n",
      " t: 18, observation: [0.73505639 0.43352256], reward: -1\n",
      " t: 19, observation: [0.73607114 0.47378109], reward: -1\n",
      " t: 20, observation: [0.77271821 0.47531591], reward: -1\n",
      " t: 21, observation: [0.81836877 0.47196267], reward: -1\n",
      " t: 22, observation: [0.77757628 0.47470932], reward: -1\n",
      " t: 23, observation: [0.82624481 0.46651544], reward: -1\n",
      " t: 24, observation: [0.87079964 0.48078393], reward: -1\n",
      " t: 25, observation: [0.92009858 0.481397  ], reward: -1\n",
      " t: 26, observation: [0.92460247 0.53974801], reward: -1\n",
      " t: 27, observation: [0.93833609 0.585082  ], reward: -1\n",
      " t: 28, observation: [0.91936534 0.63444896], reward: -1\n",
      " t: 29, observation: [0.87733239 0.64394576], reward: -1\n",
      " t: 30, observation: [0.88416122 0.71104577], reward: -1\n",
      " t: 31, observation: [0.88502736 0.78052057], reward: -1\n",
      " t: 32, observation: [0.88410958 0.84087321], reward: -1\n",
      " t: 33, observation: [0.88722747 0.88138075], reward: -1\n",
      " t: 34, observation: [0.87115495 0.93695137], reward: -1\n",
      " t: 35, observation: [0.86663466 0.98765367], reward: -1\n",
      " t: 36, observation: [0.91764022 0.99673071], reward: 0\n",
      "total reward in this episode: -36\n",
      "episode 0: reward: -36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Test the trained model\n",
    "obs, info = env.reset()\n",
    "total_reward = 0\n",
    "episode_rewards = []\n",
    "frames = []\n",
    "observation = obs\n",
    "\n",
    "max_video_length = 120\n",
    "\n",
    "\n",
    "def greedy_policy(state):\n",
    "    state_index = agent.encode_state(state)\n",
    "    return np.argmax(agent.q_table[state_index])\n",
    "\n",
    "for time_step in range(max_video_length):\n",
    "    \n",
    "    frames.append(env.render())\n",
    "\n",
    "    #action = greedy_policy(get_features(observation))\n",
    "    action = agent.choose_action(rep.get_features(observation))\n",
    "    observation, reward, done, trunc, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    image = env.render()\n",
    "    #online_rendering(image) #uncomment this line to see the online rendering of the environment frame by frame\n",
    "    frames.append(image)\n",
    "\n",
    "    print(f\" t: {time_step}, observation: {observation}, reward: {reward}\") #uncomment this line to see the environment-agent interaction details\n",
    "\n",
    "    if done:\n",
    "      print(f\"total reward in this episode: {total_reward}\")\n",
    "      episode_rewards.append(total_reward)\n",
    "      total_reward = 0\n",
    "      break\n",
    "\n",
    "env.close()\n",
    "\n",
    "if episode_rewards == []:\n",
    "  print(\"no episode finished in this run.\")\n",
    "else:\n",
    "  for i, reward in enumerate(episode_rewards):\n",
    "    print(f\"episode {i}: reward: {reward}\")\n",
    "\n",
    "visualize(frames, \"./Video/q_learning_Kan.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning with Eligibility Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tabularQlearningET:\n",
    "    def __init__(self, num_feature, num_actions, alpha=0.1, gamma=0.99, epsilon=0.1, lambda_=0.9, seed = selected_seed):\n",
    "        self.num_feature = num_feature\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "        # Initialize Q-table with zeros\n",
    "        self.q_table = np.zeros((self.num_feature, num_actions))\n",
    "        self.eligibility = np.zeros(self.num_feature)\n",
    "\n",
    "        self.seed = seed\n",
    "        # Set random seed\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Choose an action using epsilon-greedy policy\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            q = self.q_table[state].sum(axis=0)\n",
    "            return q.argmax()\n",
    "    \n",
    "    def update(self, state, action, reward, next_state):\n",
    "        self.eligibility *= self.gamma * self.lambda_\n",
    "        self.eligibility[state] =1\n",
    "\n",
    "        v_next = self.gamma * self.q_table[next_state].sum(axis=0).max()\n",
    "\n",
    "        td_error = reward + v_next - self.q_table[state, action].sum()\n",
    "\n",
    "        alpha = self.alpha / len(state)\n",
    "\n",
    "        self.q_table[:, action] += alpha * td_error * self.eligibility\n",
    "\n",
    "    def erase_eligibility(self):\n",
    "        self.eligibility = np.zeros(self.num_feature)\n",
    "        \n",
    "    def get_q_table(self):\n",
    "        return self.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -2689.912530835113\n",
      "Episode 20, Total Reward: -72\n",
      "Episode 40, Total Reward: -44\n",
      "Episode 60, Total Reward: -56\n",
      "Episode 80, Total Reward: -64\n",
      "Episode 100, Total Reward: -74\n",
      "Episode 120, Total Reward: -66\n",
      "Episode 140, Total Reward: -66\n",
      "Episode 160, Total Reward: -376\n",
      "Episode 180, Total Reward: -74\n",
      "Episode 200, Total Reward: -52\n",
      "Episode 220, Total Reward: -55\n",
      "Episode 240, Total Reward: -48\n",
      "Episode 260, Total Reward: -41\n",
      "Episode 280, Total Reward: -44\n",
      "Episode 300, Total Reward: -50\n",
      "Episode 320, Total Reward: -278.1864330418383\n",
      "Episode 340, Total Reward: -49\n",
      "Episode 360, Total Reward: -54\n",
      "Episode 380, Total Reward: -46\n",
      "Episode 400, Total Reward: -44\n",
      "Episode 420, Total Reward: -48\n",
      "Episode 440, Total Reward: -50\n",
      "Episode 460, Total Reward: -52\n",
      "Episode 480, Total Reward: -47\n",
      "Episode 500, Total Reward: -43\n",
      "Episode 520, Total Reward: -44\n",
      "Episode 540, Total Reward: -43\n",
      "Episode 560, Total Reward: -48\n",
      "Episode 580, Total Reward: -45\n",
      "Episode 600, Total Reward: -42\n",
      "Episode 620, Total Reward: -41\n",
      "Episode 640, Total Reward: -41\n",
      "Episode 660, Total Reward: -45\n",
      "Episode 680, Total Reward: -47\n",
      "Episode 700, Total Reward: -46\n",
      "Episode 720, Total Reward: -54\n",
      "Episode 740, Total Reward: -45\n",
      "Episode 760, Total Reward: -46\n",
      "Episode 780, Total Reward: -57\n",
      "Episode 800, Total Reward: -47\n",
      "Episode 820, Total Reward: -42\n",
      "Episode 840, Total Reward: -42\n",
      "Episode 860, Total Reward: -40\n",
      "Episode 880, Total Reward: -38\n",
      "Episode 900, Total Reward: -56\n",
      "Episode 920, Total Reward: -45\n",
      "Episode 940, Total Reward: -48\n",
      "Episode 960, Total Reward: -52\n",
      "Episode 980, Total Reward: -42\n",
      "Episode 1000, Total Reward: -42\n",
      "Episode 1020, Total Reward: -41\n",
      "Episode 1040, Total Reward: -42\n",
      "Episode 1060, Total Reward: -43\n",
      "Episode 1080, Total Reward: -37\n",
      "Episode 1100, Total Reward: -39\n",
      "Episode 1120, Total Reward: -44\n",
      "Episode 1140, Total Reward: -45\n",
      "Episode 1160, Total Reward: -38\n",
      "Episode 1180, Total Reward: -41\n",
      "Episode 1200, Total Reward: -48\n",
      "Episode 1220, Total Reward: -45\n",
      "Episode 1240, Total Reward: -43\n",
      "Episode 1260, Total Reward: -37\n",
      "Episode 1280, Total Reward: -40\n",
      "Episode 1300, Total Reward: -33\n",
      "Episode 1320, Total Reward: -43\n",
      "Episode 1340, Total Reward: -41\n",
      "Episode 1360, Total Reward: -41\n",
      "Episode 1380, Total Reward: -43\n",
      "Episode 1400, Total Reward: -42\n",
      "Episode 1420, Total Reward: -43\n",
      "Episode 1440, Total Reward: -39\n",
      "Episode 1460, Total Reward: -41\n",
      "Episode 1480, Total Reward: -41\n",
      "Episode 1500, Total Reward: -37\n",
      "Episode 1520, Total Reward: -39\n",
      "Episode 1540, Total Reward: -40\n",
      "Episode 1560, Total Reward: -42\n",
      "Episode 1580, Total Reward: -40\n",
      "Episode 1600, Total Reward: -38\n",
      "Episode 1620, Total Reward: -61\n",
      "Episode 1640, Total Reward: -43\n",
      "Episode 1660, Total Reward: -52\n",
      "Episode 1680, Total Reward: -55\n",
      "Episode 1700, Total Reward: -51\n",
      "Episode 1720, Total Reward: -51\n",
      "Episode 1740, Total Reward: -48\n",
      "Episode 1760, Total Reward: -45\n",
      "Episode 1780, Total Reward: -47\n",
      "Episode 1800, Total Reward: -47\n",
      "Episode 1820, Total Reward: -40\n",
      "Episode 1840, Total Reward: -43\n",
      "Episode 1860, Total Reward: -43\n",
      "Episode 1880, Total Reward: -49\n",
      "Episode 1900, Total Reward: -44\n",
      "Episode 1920, Total Reward: -48\n",
      "Episode 1940, Total Reward: -48\n",
      "Episode 1960, Total Reward: -50\n",
      "Episode 1980, Total Reward: -43\n",
      "Episode 2000, Total Reward: -51\n",
      "Episode 2020, Total Reward: -50\n",
      "Episode 2040, Total Reward: -43\n",
      "Episode 2060, Total Reward: -49\n",
      "Episode 2080, Total Reward: -41\n",
      "Episode 2100, Total Reward: -46\n",
      "Episode 2120, Total Reward: -48\n",
      "Episode 2140, Total Reward: -43\n",
      "Episode 2160, Total Reward: -43\n",
      "Episode 2180, Total Reward: -45\n",
      "Episode 2200, Total Reward: -50\n",
      "Episode 2220, Total Reward: -66\n",
      "Episode 2240, Total Reward: -44\n",
      "Episode 2260, Total Reward: -46\n",
      "Episode 2280, Total Reward: -43\n",
      "Episode 2300, Total Reward: -40\n",
      "Episode 2320, Total Reward: -45\n",
      "Episode 2340, Total Reward: -42\n",
      "Episode 2360, Total Reward: -51\n",
      "Episode 2380, Total Reward: -51\n",
      "Episode 2400, Total Reward: -52\n",
      "Episode 2420, Total Reward: -45\n",
      "Episode 2440, Total Reward: -51\n",
      "Episode 2460, Total Reward: -54\n",
      "Episode 2480, Total Reward: -45\n",
      "Episode 2500, Total Reward: -42\n",
      "Episode 2520, Total Reward: -40\n",
      "Episode 2540, Total Reward: -50\n",
      "Episode 2560, Total Reward: -49\n",
      "Episode 2580, Total Reward: -51\n",
      "Episode 2600, Total Reward: -48\n",
      "Episode 2620, Total Reward: -41\n",
      "Episode 2640, Total Reward: -40\n",
      "Episode 2660, Total Reward: -44\n",
      "Episode 2680, Total Reward: -40\n",
      "Episode 2700, Total Reward: -48\n",
      "Episode 2720, Total Reward: -56\n",
      "Episode 2740, Total Reward: -43\n",
      "Episode 2760, Total Reward: -42\n",
      "Episode 2780, Total Reward: -47\n",
      "Episode 2800, Total Reward: -47\n",
      "Episode 2820, Total Reward: -47\n",
      "Episode 2840, Total Reward: -44\n",
      "Episode 2860, Total Reward: -42\n",
      "Episode 2880, Total Reward: -59\n",
      "Episode 2900, Total Reward: -42\n",
      "Episode 2920, Total Reward: -41\n",
      "Episode 2940, Total Reward: -46\n",
      "Episode 2960, Total Reward: -44\n",
      "Episode 2980, Total Reward: -45\n",
      "Episode 3000, Total Reward: -51\n",
      "Episode 3020, Total Reward: -38\n",
      "Episode 3040, Total Reward: -58\n",
      "Episode 3060, Total Reward: -40\n",
      "Episode 3080, Total Reward: -47\n",
      "Episode 3100, Total Reward: -40\n",
      "Episode 3120, Total Reward: -57\n",
      "Episode 3140, Total Reward: -47\n",
      "Episode 3160, Total Reward: -46\n",
      "Episode 3180, Total Reward: -56\n",
      "Episode 3200, Total Reward: -41\n",
      "Episode 3220, Total Reward: -42\n",
      "Episode 3240, Total Reward: -51\n",
      "Episode 3260, Total Reward: -41\n",
      "Episode 3280, Total Reward: -40\n",
      "Episode 3300, Total Reward: -45\n",
      "Episode 3320, Total Reward: -51\n",
      "Episode 3340, Total Reward: -43\n",
      "Episode 3360, Total Reward: -39\n",
      "Episode 3380, Total Reward: -41\n",
      "Episode 3400, Total Reward: -45\n",
      "Episode 3420, Total Reward: -40\n",
      "Episode 3440, Total Reward: -49\n",
      "Episode 3460, Total Reward: -41\n",
      "Episode 3480, Total Reward: -52\n",
      "Episode 3500, Total Reward: -49\n",
      "Episode 3520, Total Reward: -42\n",
      "Episode 3540, Total Reward: -47\n",
      "Episode 3560, Total Reward: -41\n",
      "Episode 3580, Total Reward: -285.52339240240696\n",
      "Episode 3600, Total Reward: -37\n",
      "Episode 3620, Total Reward: -50\n",
      "Episode 3640, Total Reward: -37\n",
      "Episode 3660, Total Reward: -42\n",
      "Episode 3680, Total Reward: -39\n",
      "Episode 3700, Total Reward: -39\n",
      "Episode 3720, Total Reward: -46\n",
      "Episode 3740, Total Reward: -49\n",
      "Episode 3760, Total Reward: -52\n",
      "Episode 3780, Total Reward: -50\n",
      "Episode 3800, Total Reward: -43\n",
      "Episode 3820, Total Reward: -39\n",
      "Episode 3840, Total Reward: -38\n",
      "Episode 3860, Total Reward: -48\n",
      "Episode 3880, Total Reward: -43\n",
      "Episode 3900, Total Reward: -48\n",
      "Episode 3920, Total Reward: -49\n",
      "Episode 3940, Total Reward: -54\n",
      "Episode 3960, Total Reward: -43\n",
      "Episode 3980, Total Reward: -45\n",
      "Episode 4000, Total Reward: -45\n",
      "Episode 4020, Total Reward: -43\n",
      "Episode 4040, Total Reward: -42\n",
      "Episode 4060, Total Reward: -50\n",
      "Episode 4080, Total Reward: -42\n",
      "Episode 4100, Total Reward: -45\n",
      "Episode 4120, Total Reward: -38\n",
      "Episode 4140, Total Reward: -47\n",
      "Episode 4160, Total Reward: -45\n",
      "Episode 4180, Total Reward: -38\n",
      "Episode 4200, Total Reward: -41\n",
      "Episode 4220, Total Reward: -41\n",
      "Episode 4240, Total Reward: -41\n",
      "Episode 4260, Total Reward: -45\n",
      "Episode 4280, Total Reward: -40\n",
      "Episode 4300, Total Reward: -44\n",
      "Episode 4320, Total Reward: -52\n",
      "Episode 4340, Total Reward: -41\n",
      "Episode 4360, Total Reward: -46\n",
      "Episode 4380, Total Reward: -41\n",
      "Episode 4400, Total Reward: -39\n",
      "Episode 4420, Total Reward: -43\n",
      "Episode 4440, Total Reward: -44\n",
      "Episode 4460, Total Reward: -46\n",
      "Episode 4480, Total Reward: -45\n",
      "Episode 4500, Total Reward: -44\n",
      "Episode 4520, Total Reward: -37\n",
      "Episode 4540, Total Reward: -45\n",
      "Episode 4560, Total Reward: -47\n",
      "Episode 4580, Total Reward: -38\n",
      "Episode 4600, Total Reward: -47\n",
      "Episode 4620, Total Reward: -45\n",
      "Episode 4640, Total Reward: -41\n",
      "Episode 4660, Total Reward: -43\n",
      "Episode 4680, Total Reward: -42\n",
      "Episode 4700, Total Reward: -42\n",
      "Episode 4720, Total Reward: -43\n",
      "Episode 4740, Total Reward: -40\n",
      "Episode 4760, Total Reward: -48\n",
      "Episode 4780, Total Reward: -37\n",
      "Episode 4800, Total Reward: -43\n",
      "Episode 4820, Total Reward: -37\n",
      "Episode 4840, Total Reward: -46\n",
      "Episode 4860, Total Reward: -41\n",
      "Episode 4880, Total Reward: -42\n",
      "Episode 4900, Total Reward: -37\n",
      "Episode 4920, Total Reward: -51\n",
      "Episode 4940, Total Reward: -41\n",
      "Episode 4960, Total Reward: -48\n",
      "Episode 4980, Total Reward: -42\n",
      "Episode 5000, Total Reward: -49\n",
      "Episode 5020, Total Reward: -43\n",
      "Episode 5040, Total Reward: -43\n",
      "Episode 5060, Total Reward: -45\n",
      "Episode 5080, Total Reward: -42\n",
      "Episode 5100, Total Reward: -42\n",
      "Episode 5120, Total Reward: -46\n",
      "Episode 5140, Total Reward: -43\n",
      "Episode 5160, Total Reward: -40\n",
      "Episode 5180, Total Reward: -39\n",
      "Episode 5200, Total Reward: -38\n",
      "Episode 5220, Total Reward: -40\n",
      "Episode 5240, Total Reward: -40\n",
      "Episode 5260, Total Reward: -38\n",
      "Episode 5280, Total Reward: -45\n",
      "Episode 5300, Total Reward: -48\n",
      "Episode 5320, Total Reward: -45\n",
      "Episode 5340, Total Reward: -45\n",
      "Episode 5360, Total Reward: -44\n",
      "Episode 5380, Total Reward: -39\n",
      "Episode 5400, Total Reward: -38\n",
      "Episode 5420, Total Reward: -52\n",
      "Episode 5440, Total Reward: -42\n",
      "Episode 5460, Total Reward: -42\n",
      "Episode 5480, Total Reward: -45\n",
      "Episode 5500, Total Reward: -44\n",
      "Episode 5520, Total Reward: -45\n",
      "Episode 5540, Total Reward: -44\n",
      "Episode 5560, Total Reward: -42\n",
      "Episode 5580, Total Reward: -45\n",
      "Episode 5600, Total Reward: -42\n",
      "Episode 5620, Total Reward: -46\n",
      "Episode 5640, Total Reward: -48\n",
      "Episode 5660, Total Reward: -51\n",
      "Episode 5680, Total Reward: -43\n",
      "Episode 5700, Total Reward: -45\n",
      "Episode 5720, Total Reward: -44\n",
      "Episode 5740, Total Reward: -46\n",
      "Episode 5760, Total Reward: -41\n",
      "Episode 5780, Total Reward: -41\n",
      "Episode 5800, Total Reward: -50\n",
      "Episode 5820, Total Reward: -41\n",
      "Episode 5840, Total Reward: -44\n",
      "Episode 5860, Total Reward: -48\n",
      "Episode 5880, Total Reward: -43\n",
      "Episode 5900, Total Reward: -40\n",
      "Episode 5920, Total Reward: -51\n",
      "Episode 5940, Total Reward: -42\n",
      "Episode 5960, Total Reward: -40\n",
      "Episode 5980, Total Reward: -50\n",
      "Episode 6000, Total Reward: -39\n",
      "Episode 6020, Total Reward: -42\n",
      "Episode 6040, Total Reward: -39\n",
      "Episode 6060, Total Reward: -40\n",
      "Episode 6080, Total Reward: -44\n",
      "Episode 6100, Total Reward: -44\n",
      "Episode 6120, Total Reward: -41\n",
      "Episode 6140, Total Reward: -40\n",
      "Episode 6160, Total Reward: -40\n",
      "Episode 6180, Total Reward: -47\n",
      "Episode 6200, Total Reward: -50\n",
      "Episode 6220, Total Reward: -41\n",
      "Episode 6240, Total Reward: -41\n",
      "Episode 6260, Total Reward: -43\n",
      "Episode 6280, Total Reward: -45\n",
      "Episode 6300, Total Reward: -51\n",
      "Episode 6320, Total Reward: -45\n",
      "Episode 6340, Total Reward: -39\n",
      "Episode 6360, Total Reward: -36\n",
      "Episode 6380, Total Reward: -49\n",
      "Episode 6400, Total Reward: -44\n",
      "Episode 6420, Total Reward: -36\n",
      "Episode 6440, Total Reward: -41\n",
      "Episode 6460, Total Reward: -37\n",
      "Episode 6480, Total Reward: -46\n",
      "Episode 6500, Total Reward: -44\n",
      "Episode 6520, Total Reward: -43\n",
      "Episode 6540, Total Reward: -44\n",
      "Episode 6560, Total Reward: -38\n",
      "Episode 6580, Total Reward: -46\n",
      "Episode 6600, Total Reward: -47\n",
      "Episode 6620, Total Reward: -62\n",
      "Episode 6640, Total Reward: -38\n",
      "Episode 6660, Total Reward: -41\n",
      "Episode 6680, Total Reward: -58\n",
      "Episode 6700, Total Reward: -52\n",
      "Episode 6720, Total Reward: -38\n",
      "Episode 6740, Total Reward: -36\n",
      "Episode 6760, Total Reward: -49\n",
      "Episode 6780, Total Reward: -46\n",
      "Episode 6800, Total Reward: -41\n",
      "Episode 6820, Total Reward: -50\n",
      "Episode 6840, Total Reward: -38\n",
      "Episode 6860, Total Reward: -37\n",
      "Episode 6880, Total Reward: -41\n",
      "Episode 6900, Total Reward: -52\n",
      "Episode 6920, Total Reward: -40\n",
      "Episode 6940, Total Reward: -44\n",
      "Episode 6960, Total Reward: -38\n",
      "Episode 6980, Total Reward: -39\n",
      "Episode 7000, Total Reward: -41\n",
      "Episode 7020, Total Reward: -41\n",
      "Episode 7040, Total Reward: -45\n",
      "Episode 7060, Total Reward: -48\n",
      "Episode 7080, Total Reward: -43\n",
      "Episode 7100, Total Reward: -40\n",
      "Episode 7120, Total Reward: -43\n",
      "Episode 7140, Total Reward: -40\n",
      "Episode 7160, Total Reward: -40\n",
      "Episode 7180, Total Reward: -41\n",
      "Episode 7200, Total Reward: -49\n",
      "Episode 7220, Total Reward: -44\n",
      "Episode 7240, Total Reward: -48\n",
      "Episode 7260, Total Reward: -39\n",
      "Episode 7280, Total Reward: -45\n",
      "Episode 7300, Total Reward: -39\n",
      "Episode 7320, Total Reward: -38\n",
      "Episode 7340, Total Reward: -45\n",
      "Episode 7360, Total Reward: -44\n",
      "Episode 7380, Total Reward: -44\n",
      "Episode 7400, Total Reward: -41\n",
      "Episode 7420, Total Reward: -41\n",
      "Episode 7440, Total Reward: -38\n",
      "Episode 7460, Total Reward: -45\n",
      "Episode 7480, Total Reward: -40\n",
      "Episode 7500, Total Reward: -46\n",
      "Episode 7520, Total Reward: -40\n",
      "Episode 7540, Total Reward: -50\n",
      "Episode 7560, Total Reward: -44\n",
      "Episode 7580, Total Reward: -46\n",
      "Episode 7600, Total Reward: -44\n",
      "Episode 7620, Total Reward: -40\n",
      "Episode 7640, Total Reward: -42\n",
      "Episode 7660, Total Reward: -54\n",
      "Episode 7680, Total Reward: -46\n",
      "Episode 7700, Total Reward: -40\n",
      "Episode 7720, Total Reward: -45\n",
      "Episode 7740, Total Reward: -48\n",
      "Episode 7760, Total Reward: -35\n",
      "Episode 7780, Total Reward: -42\n",
      "Episode 7800, Total Reward: -44\n",
      "Episode 7820, Total Reward: -41\n",
      "Episode 7840, Total Reward: -42\n",
      "Episode 7860, Total Reward: -54\n",
      "Episode 7880, Total Reward: -51\n",
      "Episode 7900, Total Reward: -47\n",
      "Episode 7920, Total Reward: -39\n",
      "Episode 7940, Total Reward: -39\n",
      "Episode 7960, Total Reward: -44\n",
      "Episode 7980, Total Reward: -38\n",
      "Episode 8000, Total Reward: -51\n",
      "Episode 8020, Total Reward: -45\n",
      "Episode 8040, Total Reward: -38\n",
      "Episode 8060, Total Reward: -43\n",
      "Episode 8080, Total Reward: -39\n",
      "Episode 8100, Total Reward: -40\n",
      "Episode 8120, Total Reward: -44\n",
      "Episode 8140, Total Reward: -44\n",
      "Episode 8160, Total Reward: -46\n",
      "Episode 8180, Total Reward: -38\n",
      "Episode 8200, Total Reward: -46\n",
      "Episode 8220, Total Reward: -46\n",
      "Episode 8240, Total Reward: -42\n",
      "Episode 8260, Total Reward: -41\n",
      "Episode 8280, Total Reward: -50\n",
      "Episode 8300, Total Reward: -38\n",
      "Episode 8320, Total Reward: -48\n",
      "Episode 8340, Total Reward: -45\n",
      "Episode 8360, Total Reward: -38\n",
      "Episode 8380, Total Reward: -46\n",
      "Episode 8400, Total Reward: -36\n",
      "Episode 8420, Total Reward: -56\n",
      "Episode 8440, Total Reward: -49\n",
      "Episode 8460, Total Reward: -46\n",
      "Episode 8480, Total Reward: -34\n",
      "Episode 8500, Total Reward: -44\n",
      "Episode 8520, Total Reward: -46\n",
      "Episode 8540, Total Reward: -44\n",
      "Episode 8560, Total Reward: -48\n",
      "Episode 8580, Total Reward: -42\n",
      "Episode 8600, Total Reward: -38\n",
      "Episode 8620, Total Reward: -51\n",
      "Episode 8640, Total Reward: -41\n",
      "Episode 8660, Total Reward: -44\n",
      "Episode 8680, Total Reward: -44\n",
      "Episode 8700, Total Reward: -47\n",
      "Episode 8720, Total Reward: -42\n",
      "Episode 8740, Total Reward: -42\n",
      "Episode 8760, Total Reward: -42\n",
      "Episode 8780, Total Reward: -51\n",
      "Episode 8800, Total Reward: -43\n",
      "Episode 8820, Total Reward: -34\n",
      "Episode 8840, Total Reward: -40\n",
      "Episode 8860, Total Reward: -43\n",
      "Episode 8880, Total Reward: -36\n",
      "Episode 8900, Total Reward: -39\n",
      "Episode 8920, Total Reward: -44\n",
      "Episode 8940, Total Reward: -42\n",
      "Episode 8960, Total Reward: -39\n",
      "Episode 8980, Total Reward: -39\n",
      "Episode 9000, Total Reward: -35\n",
      "Episode 9020, Total Reward: -39\n",
      "Episode 9040, Total Reward: -44\n",
      "Episode 9060, Total Reward: -41\n",
      "Episode 9080, Total Reward: -44\n",
      "Episode 9100, Total Reward: -45\n",
      "Episode 9120, Total Reward: -42\n",
      "Episode 9140, Total Reward: -41\n",
      "Episode 9160, Total Reward: -40\n",
      "Episode 9180, Total Reward: -38\n",
      "Episode 9200, Total Reward: -41\n",
      "Episode 9220, Total Reward: -36\n",
      "Episode 9240, Total Reward: -47\n",
      "Episode 9260, Total Reward: -37\n",
      "Episode 9280, Total Reward: -44\n",
      "Episode 9300, Total Reward: -41\n",
      "Episode 9320, Total Reward: -42\n",
      "Episode 9340, Total Reward: -42\n",
      "Episode 9360, Total Reward: -42\n",
      "Episode 9380, Total Reward: -39\n",
      "Episode 9400, Total Reward: -41\n",
      "Episode 9420, Total Reward: -41\n",
      "Episode 9440, Total Reward: -36\n",
      "Episode 9460, Total Reward: -42\n",
      "Episode 9480, Total Reward: -40\n",
      "Episode 9500, Total Reward: -37\n",
      "Episode 9520, Total Reward: -42\n",
      "Episode 9540, Total Reward: -46\n",
      "Episode 9560, Total Reward: -44\n",
      "Episode 9580, Total Reward: -44\n",
      "Episode 9600, Total Reward: -36\n",
      "Episode 9620, Total Reward: -43\n",
      "Episode 9640, Total Reward: -36\n",
      "Episode 9660, Total Reward: -42\n",
      "Episode 9680, Total Reward: -41\n",
      "Episode 9700, Total Reward: -47\n",
      "Episode 9720, Total Reward: -43\n",
      "Episode 9740, Total Reward: -46\n",
      "Episode 9760, Total Reward: -40\n",
      "Episode 9780, Total Reward: -40\n",
      "Episode 9800, Total Reward: -38\n",
      "Episode 9820, Total Reward: -47\n",
      "Episode 9840, Total Reward: -40\n",
      "Episode 9860, Total Reward: -52\n",
      "Episode 9880, Total Reward: -40\n",
      "Episode 9900, Total Reward: -38\n",
      "Episode 9920, Total Reward: -37\n",
      "Episode 9940, Total Reward: -42\n",
      "Episode 9960, Total Reward: -43\n",
      "Episode 9980, Total Reward: -42\n"
     ]
    }
   ],
   "source": [
    "## simulare the agent in the environment\n",
    "num_actions = len(env.get_wrapper_attr(\"actions\"))\n",
    "agent = tabularQlearningET(num_feature= num_features, num_actions=num_actions)\n",
    "\n",
    "num_episodes = 10000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, info = env.reset()\n",
    "    state = rep.get_features(obs)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    agent.erase_eligibility()\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_obs, reward, done, trunc, _ = env.step(action)\n",
    "        next_state = rep.get_features(next_obs)\n",
    "        agent.update(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    if episode % 20 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " t: 0, observation: [0.20286288 0.34125826], reward: -1\n",
      " t: 1, observation: [0.24526551 0.33682478], reward: -1\n",
      " t: 2, observation: [0.24115111 0.28597662], reward: -1\n",
      " t: 3, observation: [0.24857013 0.24874332], reward: -1\n",
      " t: 4, observation: [0.24405617 0.20384785], reward: -1\n",
      " t: 5, observation: [0.249758   0.25979119], reward: -1\n",
      " t: 6, observation: [0.24375847 0.22434944], reward: -1\n",
      " t: 7, observation: [0.2451925  0.17720605], reward: -1\n",
      " t: 8, observation: [0.30637279 0.15724924], reward: -1\n",
      " t: 9, observation: [0.36446822 0.15213001], reward: -1\n",
      " t: 10, observation: [0.40841844 0.16233217], reward: -1\n",
      " t: 11, observation: [0.35921752 0.16504246], reward: -1\n",
      " t: 12, observation: [0.40835509 0.16760173], reward: -1\n",
      " t: 13, observation: [0.4116669  0.21085044], reward: -1\n",
      " t: 14, observation: [0.36832805 0.20767923], reward: -1\n",
      " t: 15, observation: [0.42826317 0.21118161], reward: -1\n",
      " t: 16, observation: [0.35851445 0.21728525], reward: -1\n",
      " t: 17, observation: [0.37766765 0.17760496], reward: -1\n",
      " t: 18, observation: [0.37787555 0.13504132], reward: -1\n",
      " t: 19, observation: [0.43966358 0.10881321], reward: -1\n",
      " t: 20, observation: [0.48580822 0.09432792], reward: -1\n",
      " t: 21, observation: [0.53859015 0.09404887], reward: -1\n",
      " t: 22, observation: [0.57835759 0.09875469], reward: -1\n",
      " t: 23, observation: [0.63283517 0.11345902], reward: -1\n",
      " t: 24, observation: [0.66091014 0.11811218], reward: -1\n",
      " t: 25, observation: [0.65828617 0.17411258], reward: -1\n",
      " t: 26, observation: [0.65591395 0.21740353], reward: -1\n",
      " t: 27, observation: [0.70194883 0.21420572], reward: -1\n",
      " t: 28, observation: [0.74049767 0.23909054], reward: -1\n",
      " t: 29, observation: [0.74721743 0.30543633], reward: -1\n",
      " t: 30, observation: [0.76035116 0.36097119], reward: -1\n",
      " t: 31, observation: [0.75859484 0.42334388], reward: -1\n",
      " t: 32, observation: [0.81411915 0.43265506], reward: -1\n",
      " t: 33, observation: [0.84091451 0.47978779], reward: -1\n",
      " t: 34, observation: [0.82687768 0.53211216], reward: -1\n",
      " t: 35, observation: [0.836673   0.57162208], reward: -1\n",
      " t: 36, observation: [0.88911467 0.57174305], reward: -1\n",
      " t: 37, observation: [0.93574554 0.56849829], reward: -1\n",
      " t: 38, observation: [0.94813661 0.61867952], reward: -1\n",
      " t: 39, observation: [0.95342872 0.68036213], reward: -1\n",
      " t: 40, observation: [0.95587752 0.73690734], reward: -1\n",
      " t: 41, observation: [0.96180424 0.80228738], reward: -1\n",
      " t: 42, observation: [0.9578358  0.84043761], reward: -1\n",
      " t: 43, observation: [0.94948697 0.88334451], reward: -1\n",
      " t: 44, observation: [0.95757305 0.94184951], reward: -1\n",
      " t: 45, observation: [1.         0.93337659], reward: 0\n",
      "total reward in this episode: -45\n",
      "episode 0: reward: -45\n"
     ]
    }
   ],
   "source": [
    "obs, info = env.reset()\n",
    "total_reward = 0\n",
    "episode_rewards = []\n",
    "frames = []\n",
    "observation = obs\n",
    "\n",
    "max_video_length = 120\n",
    "\n",
    "\n",
    "for time_step in range(max_video_length):\n",
    "    frames.append(env.render())\n",
    "\n",
    "    #action = greedy_policy(get_features(observation))\n",
    "    action = agent.choose_action(rep.get_features(observation))\n",
    "    observation, reward, done, trunc, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    image = env.render()\n",
    "    #online_rendering(image) #uncomment this line to see the online rendering of the environment frame by frame\n",
    "    frames.append(image)\n",
    "\n",
    "    print(f\" t: {time_step}, observation: {observation}, reward: {reward}\") #uncomment this line to see the environment-agent interaction details\n",
    "\n",
    "    if done:\n",
    "      print(f\"total reward in this episode: {total_reward}\")\n",
    "      episode_rewards.append(total_reward)\n",
    "      total_reward = 0\n",
    "      break\n",
    "\n",
    "env.close()\n",
    "\n",
    "if episode_rewards == []:\n",
    "  print(\"no episode finished in this run.\")\n",
    "else:\n",
    "  for i, reward in enumerate(episode_rewards):\n",
    "    print(f\"episode {i}: reward: {reward}\")\n",
    "\n",
    "visualize(frames, \"./Video/q_learning_Kan_ET.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
