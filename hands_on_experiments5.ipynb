{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Growing Neural Gas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_puddle\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.dqn import MlpPolicy as DQNPolicy\n",
    "from stable_baselines3.ppo import MlpPolicy as PPOPolicy\n",
    "\n",
    "import time\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython import display\n",
    "import pyvirtualdisplay\n",
    "import cv2\n",
    "\n",
    "import libs.tiles3 as tc\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class GrowingNeuralGas:\n",
    "    def __init__(self, input_dim, max_nodes=100, epsilon_b=0.05, epsilon_n=0.0006, alpha=0.5, beta=0.999, delta=0.1):\n",
    "        self.input_dim = input_dim\n",
    "        self.max_nodes = max_nodes\n",
    "        self.epsilon_b = epsilon_b\n",
    "        self.epsilon_n = epsilon_n\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.delta = delta\n",
    "\n",
    "        # Initialize network with two nodes\n",
    "        self.nodes = []\n",
    "        self.nodes.append(Node(input_dim, np.zeros(input_dim)))\n",
    "        self.nodes.append(Node(input_dim, np.ones(input_dim)))\n",
    "        self.error = []\n",
    "\n",
    "    def fit(self, X, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(X)\n",
    "            for x in X:\n",
    "                winner, second_winner = self.find_closest_nodes(x)\n",
    "                self.update(winner, second_winner, x)\n",
    "                self.remove_old_connections()\n",
    "\n",
    "    def find_closest_nodes(self, x):\n",
    "        distances = [node.distance_to(x) for node in self.nodes]\n",
    "        winner_index = np.argmin(distances)\n",
    "        winner = self.nodes[winner_index]\n",
    "        distances[winner_index] = np.inf\n",
    "        second_winner_index = np.argmin(distances)\n",
    "        second_winner = self.nodes[second_winner_index]\n",
    "        return winner, second_winner\n",
    "\n",
    "    def update(self, winner, second_winner, x):\n",
    "        winner.move_towards(x, self.epsilon_b)\n",
    "        for node in self.nodes:\n",
    "            node.move_towards(x, self.epsilon_n * node.error)\n",
    "        self.update_edge_weights(winner)\n",
    "        self.nodes.append(Node(self.input_dim, x))\n",
    "        self.remove_old_connections()\n",
    "        self.error.append(winner.distance_to(x))\n",
    "\n",
    "    def update_edge_weights(self, winner):\n",
    "        for edge in winner.edges:\n",
    "            edge.age += 1\n",
    "            edge.weight += self.alpha * (edge.source.distance_to(edge.destination.position) - edge.weight)\n",
    "        for node in self.nodes:\n",
    "            if node != winner:\n",
    "                if winner.has_edge_to(node):\n",
    "                    winner.get_edge_to(node).age = 0\n",
    "                else:\n",
    "                    winner.edges.append(Edge(winner, node, 0))\n",
    "\n",
    "    def remove_old_connections(self):\n",
    "        edges_to_remove = []\n",
    "        for node in self.nodes:\n",
    "            for edge in node.edges:\n",
    "                if edge.age > self.beta * max([e.age for e in node.edges]):\n",
    "                    edges_to_remove.append(edge)\n",
    "        for edge in edges_to_remove:\n",
    "            edge.source.edges.remove(edge)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, input_dim, position):\n",
    "        self.position = position\n",
    "        self.edges = []\n",
    "        self.error = 0  # Add error attribute and initialize it to 0\n",
    "\n",
    "    def move_towards(self, x, step_size):\n",
    "        self.position += step_size * (x - self.position)\n",
    "        self.error = np.linalg.norm(x - self.position)  # Update error after movement\n",
    "\n",
    "    def distance_to(self, x):\n",
    "        return np.linalg.norm(self.position - x)\n",
    "\n",
    "    def has_edge_to(self, node):\n",
    "        for edge in self.edges:\n",
    "            if edge.destination == node:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_edge_to(self, node):\n",
    "        for edge in self.edges:\n",
    "            if edge.destination == node:\n",
    "                return edge\n",
    "        return None\n",
    "\n",
    "class Edge:\n",
    "    def __init__(self, source, destination, age):\n",
    "        self.source = source\n",
    "        self.destination = destination\n",
    "        self.age = age\n",
    "        self.weight = np.linalg.norm(self.source.position - self.destination.position)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define your environment and observation space\n",
    "    env = gym.make('CartPole-v1')\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "\n",
    "    # Create GNG instance\n",
    "    gng = GrowingNeuralGas(input_dim=observation_space)\n",
    "\n",
    "    # Generate some random data for training\n",
    "    data = np.random.rand(100, observation_space)\n",
    "\n",
    "    # Train the GNG\n",
    "    gng.fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active feature indexes: [0.2 0.7]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import networkx as nx\n",
    "\n",
    "class GNGStateCoder:\n",
    "    def __init__(self, observation_space: gym.spaces.Space, max_nodes: int = 1000):\n",
    "        \"\"\"\n",
    "        GNG State Coder utilizing Growing Neural Gas algorithm.\n",
    "\n",
    "        :param observation_space: space to approximate\n",
    "        :param max_nodes: maximum number of nodes in the GNG graph\n",
    "        \"\"\"\n",
    "        self.graph = nx.Graph()\n",
    "        self.observation_space = observation_space\n",
    "        self.max_nodes = max_nodes\n",
    "\n",
    "    def normalize(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Normalize the data to fit within the observation space.\n",
    "\n",
    "        :param data: data to normalize\n",
    "        :return: normalized data\n",
    "        \"\"\"\n",
    "        low = self.observation_space.low\n",
    "        high = self.observation_space.high\n",
    "        return (data - low) / (high - low)\n",
    "\n",
    "    def encode_state(self, observation: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encode observation into the GNG graph.\n",
    "\n",
    "        :param observation: input observation\n",
    "        \"\"\"\n",
    "        observation = self.normalize(observation)\n",
    "        self.graph.add_node(tuple(observation), age=0)  # Initialize age attribute for the new node\n",
    "\n",
    "        if len(self.graph.nodes) > self.max_nodes:\n",
    "            # If the number of nodes exceeds the limit, remove the oldest node\n",
    "            oldest_node = min(self.graph.nodes, key=lambda x: self.graph.nodes[x]['age'])\n",
    "            self.graph.remove_node(oldest_node)\n",
    "\n",
    "        # Increment age of all existing nodes\n",
    "        for node in self.graph.nodes:\n",
    "            if 'age' in self.graph.nodes[node]:  # Check if 'age' attribute exists\n",
    "                self.graph.nodes[node]['age'] += 1\n",
    "\n",
    "        # Connect new node to the nearest existing node\n",
    "        nearest_node = min(self.graph.nodes, key=lambda x: np.linalg.norm(np.array(x) - observation))\n",
    "        self.graph.add_edge(tuple(observation), nearest_node, age=0)\n",
    "\n",
    "\n",
    "    def extract_features(self, observation: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Extract features from the GNG graph based on the current observation.\n",
    "\n",
    "        :param observation: input observation\n",
    "        :return: array representing the position of the nearest node\n",
    "        \"\"\"\n",
    "        observation = self.normalize(observation)\n",
    "        nearest_node = min(self.graph.nodes, key=lambda x: np.linalg.norm(np.array(x) - observation))\n",
    "        return np.array(nearest_node)\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(2,), dtype=np.float64)\n",
    "state_coder = GNGStateCoder(observation_space)\n",
    "\n",
    "# Assuming 'observation' is the current observation\n",
    "observation = np.array([0.2, 0.7])\n",
    "state_coder.encode_state(observation)\n",
    "\n",
    "# Extract features from the encoded state\n",
    "features = state_coder.extract_features(observation)\n",
    "print(\"Active feature indexes:\", features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Observations:\n",
      "Observation 0: [0, 1]\n",
      "Observation 1: [0, 1]\n",
      "Observation 2: [0, 1]\n",
      "Observation 3: [0, 1]\n",
      "Observation 4: [0, 1]\n",
      "Observation 5: [0, 1]\n",
      "Observation 6: [0, 1]\n",
      "Observation 7: [0, 1]\n",
      "Observation 8: [0, 1]\n",
      "Observation 9: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import networkx as nx\n",
    "\n",
    "class GrowingNeuralGas:\n",
    "    def __init__(self, observation_space: gym.spaces.Space, max_nodes: int = 100):\n",
    "        self.observation_space = observation_space\n",
    "        self.max_nodes = max_nodes\n",
    "        self.graph = nx.Graph()\n",
    "        self.dimensions = observation_space.shape[0]\n",
    "        \n",
    "        # Initialize the graph with two random nodes\n",
    "        node1 = self._get_random_observation()\n",
    "        node2 = self._get_random_observation()\n",
    "        self.graph.add_node(0, pos=node1, error=0)\n",
    "        self.graph.add_node(1, pos=node2, error=0)\n",
    "        self.graph.add_edge(0, 1, age=0)\n",
    "\n",
    "    def _get_random_observation(self):\n",
    "        return self.observation_space.sample()\n",
    "\n",
    "    def _distance(self, a, b):\n",
    "        return np.sqrt(np.sum((a - b)**2))\n",
    "\n",
    "    def _closest_nodes(self, current_node):\n",
    "        distances = [(node, self._distance(current_node, self.graph.nodes[node]['pos'])) for node in self.graph.nodes()]\n",
    "        distances.sort(key=lambda x: x[1])\n",
    "        return distances[0][0], distances[1][0]\n",
    "\n",
    "    def _update_winner(self, current_node):\n",
    "        winner1, winner2 = self._closest_nodes(current_node)\n",
    "        winner_pos = self.graph.nodes[winner1]['pos']\n",
    "        move_delta = 0.05 * (current_node - winner_pos)\n",
    "        new_pos = winner_pos + move_delta\n",
    "        self.graph.add_node(winner1, pos=new_pos)\n",
    "\n",
    "    def encode_observation(self, observation):\n",
    "        self._update_winner(observation)\n",
    "        if len(self.graph.nodes()) < self.max_nodes:\n",
    "            # Add new node every lambda iterations\n",
    "            if len(self.graph.nodes()) % 100 == 0:\n",
    "                largest_error_node = max(self.graph.nodes(), key=lambda x: self.graph.nodes[x]['error'])\n",
    "                # Insert a new unit halfway between largest_error_node and its neighbor\n",
    "                max_error_neighbor = list(self.graph.neighbors(largest_error_node))[0]\n",
    "                new_pos = (self.graph.nodes[largest_error_node]['pos'] + self.graph.nodes[max_error_neighbor]['pos']) / 2\n",
    "                new_node_id = max(self.graph.nodes()) + 1\n",
    "                self.graph.add_node(new_node_id, pos=new_pos)\n",
    "                self.graph.add_edge(new_node_id, largest_error_node, age=0)\n",
    "                self.graph.add_edge(new_node_id, max_error_neighbor, age=0)\n",
    "                self.graph.remove_edge(largest_error_node, max_error_neighbor)\n",
    "                self.graph.nodes[largest_error_node]['error'] *= 0.5\n",
    "                self.graph.nodes[max_error_neighbor]['error'] *= 0.5\n",
    "        return list(self.graph.nodes())\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Create a test environment with a continuous observation space\n",
    "env = gym.make('Pendulum-v1')\n",
    "observation_space = env.observation_space\n",
    "\n",
    "# Create an instance of GrowingNeuralGas\n",
    "gng = GrowingNeuralGas(observation_space)\n",
    "\n",
    "# Generate some random observations\n",
    "observations = [observation_space.sample() for _ in range(1000)]\n",
    "\n",
    "# Encode each observation using GNG\n",
    "encoded_observations = [gng.encode_observation(observation) for observation in observations]\n",
    "\n",
    "# Print the encoded observations\n",
    "print(\"Encoded Observations:\")\n",
    "for i, encoded_observation in enumerate(encoded_observations[:10]):\n",
    "    print(\"Observation {}: {}\".format(i, encoded_observation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_seed = 0\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#some functions to help the visualization and interaction wit the environment\n",
    "\n",
    "def visualize(frames, video_name = \"/Video/video.mp4\"):\n",
    "    # Saves the frames as an mp4 video using cv2\n",
    "    video_path = video_name\n",
    "    height, width, _ = frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    video_writer = cv2.VideoWriter(video_path, fourcc, 30, (width, height))\n",
    "    for frame in frames:\n",
    "        video_writer.write(frame)\n",
    "    video_writer.release()\n",
    "\n",
    "def online_rendering(image):\n",
    "    #Visualize one frame of the image in a display\n",
    "    ax.axis('off')\n",
    "    img_with_frame = np.zeros((image.shape[0]+2, image.shape[1]+2, 3), dtype=np.uint8)\n",
    "    img_with_frame[1:-1, 1:-1, :] = image\n",
    "    ax.imshow(img_with_frame)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "\n",
    "def prepare_display():\n",
    "  #Prepares display for onine rendering of the frames in the game\n",
    "  _display = pyvirtualdisplay.Display(visible=False,size=(1400, 900))\n",
    "  _ = _display.start()\n",
    "  fig, ax = plt.subplots(figsize=(5, 5))\n",
    "  ax.axis('off')\n",
    "\n",
    "\n",
    "def get_action():\n",
    "    action = None\n",
    "    while action not in [\"w\", \"a\", \"s\", \"d\", \"W\", \"A\", \"S\", \"D\"]:\n",
    "        action = input(\"Enter action (w/a/s/d): \")\n",
    "    if action == \"w\":\n",
    "        return 3\n",
    "    elif action == \"a\":\n",
    "        return 0\n",
    "    elif action == \"s\":\n",
    "        return 2\n",
    "    elif action == \"d\":\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Different Environment Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "json_file = f\"/Users/hadiaghazadeh/Library/CloudStorage/OneDrive-UniversityofCalgary/@upperboundCompetition/gym-puddle/gym_puddle/env_configs/pw1.json\"\n",
    "\n",
    "with open(json_file) as f:\n",
    "  env_setup = json.load(f)\n",
    "\n",
    "\n",
    "env = gym.make(\n",
    "  \"PuddleWorld-v0\",\n",
    "  start=env_setup[\"start\"],\n",
    "  goal=env_setup[\"goal\"],\n",
    "  goal_threshold=env_setup[\"goal_threshold\"],\n",
    "  noise=env_setup[\"noise\"],\n",
    "  thrust=env_setup[\"thrust\"],\n",
    "  puddle_top_left=env_setup[\"puddle_top_left\"],\n",
    "  puddle_width=env_setup[\"puddle_width\"],\n",
    ")\n",
    "\n",
    "\n",
    "obs, info = env.reset()\n",
    "image = env.render()\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "online_rendering(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start position: [0.2 0.4]\n",
      "goal position: [1. 1.]\n",
      "goal threshold: 0.1\n",
      "action noise: 0\n",
      "agent's thrust: 0.05\n",
      "puddle top left positions: [array([0.  , 0.85]), array([0.35, 0.9 ])]\n",
      "puddle widths and heights: [array([0.55, 0.2 ]), array([0.2, 0.6])]\n",
      "action space: [array([-0.05,  0.  ]), array([0.05, 0.  ]), array([ 0.  , -0.05]), array([0.  , 0.05])]\n",
      "observation space: Box(0.0, 1.0, (2,), float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"start position:\", env.get_wrapper_attr(\"start\"))\n",
    "print(\"goal position:\", env.get_wrapper_attr(\"goal\"))\n",
    "print(\"goal threshold:\", env.get_wrapper_attr(\"goal_threshold\"))\n",
    "print(\"action noise:\", env.get_wrapper_attr(\"noise\"))\n",
    "print(\"agent's thrust:\", env.get_wrapper_attr(\"thrust\"))\n",
    "print(\"puddle top left positions:\", env.get_wrapper_attr(\"puddle_top_left\"))\n",
    "print(\"puddle widths and heights:\", env.get_wrapper_attr(\"puddle_width\"))\n",
    "print(\"action space:\", env.get_wrapper_attr(\"actions\"))\n",
    "print(\"observation space:\", env.get_wrapper_attr(\"observation_space\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kanerva coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([820, 286, 117, 555, 408, 278, 592,  32, 673, 241])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from util.kanerva import BaseKanervaCoder\n",
    "num_features = 1000\n",
    "n_closest = 10\n",
    "rep = BaseKanervaCoder(env.observation_space, n_prototypes= num_features, n_closest= n_closest, random_seed= selected_seed)\n",
    "rep.get_features(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tabularQlearning:\n",
    "    def __init__(self, num_feature, num_actions, alpha=0.1, gamma=0.9, epsilon=0.05, seed = selected_seed):\n",
    "        self.num_feature = num_feature\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Initialize Q-table with zeros\n",
    "        self.q_table = np.zeros((self.num_feature, num_actions))\n",
    "\n",
    "        self.seed = seed    \n",
    "        # Set random seed\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Choose an action using epsilon-greedy policy\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            q = self.q_table[state].sum(axis=0)\n",
    "            return q.argmax()\n",
    "    \n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"Update the Q-table using the Q-learning update rule\"\"\"\n",
    "        self.q_table[state, action] += self.alpha * (reward + self.gamma * np.max(self.q_table[next_state]) - self.q_table[state, action])\n",
    "    \n",
    "    def get_q_table(self):\n",
    "        return self.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -2208.0\n",
      "Episode 20, Total Reward: -579\n",
      "Episode 40, Total Reward: -32258.0\n",
      "Episode 60, Total Reward: -4528.0\n",
      "Episode 80, Total Reward: -396.0\n",
      "Episode 100, Total Reward: -61\n",
      "Episode 120, Total Reward: -73\n",
      "Episode 140, Total Reward: -489\n",
      "Episode 160, Total Reward: -75\n",
      "Episode 180, Total Reward: -63\n",
      "Episode 200, Total Reward: -35\n",
      "Episode 220, Total Reward: -51\n",
      "Episode 240, Total Reward: -82\n",
      "Episode 260, Total Reward: -35\n",
      "Episode 280, Total Reward: -37\n",
      "Episode 300, Total Reward: -37\n",
      "Episode 320, Total Reward: -43\n",
      "Episode 340, Total Reward: -33\n",
      "Episode 360, Total Reward: -33\n",
      "Episode 380, Total Reward: -43\n",
      "Episode 400, Total Reward: -39\n",
      "Episode 420, Total Reward: -432.0\n",
      "Episode 440, Total Reward: -37\n",
      "Episode 460, Total Reward: -43\n",
      "Episode 480, Total Reward: -33\n",
      "Episode 500, Total Reward: -35\n",
      "Episode 520, Total Reward: -39\n",
      "Episode 540, Total Reward: -37\n",
      "Episode 560, Total Reward: -380.0\n",
      "Episode 580, Total Reward: -35\n",
      "Episode 600, Total Reward: -33\n",
      "Episode 620, Total Reward: -33\n",
      "Episode 640, Total Reward: -35\n",
      "Episode 660, Total Reward: -37\n",
      "Episode 680, Total Reward: -35\n",
      "Episode 700, Total Reward: -35\n",
      "Episode 720, Total Reward: -33\n",
      "Episode 740, Total Reward: -33\n",
      "Episode 760, Total Reward: -41\n",
      "Episode 780, Total Reward: -43\n",
      "Episode 800, Total Reward: -37\n",
      "Episode 820, Total Reward: -847\n",
      "Episode 840, Total Reward: -33\n",
      "Episode 860, Total Reward: -35\n",
      "Episode 880, Total Reward: -33\n",
      "Episode 900, Total Reward: -35\n",
      "Episode 920, Total Reward: -39\n",
      "Episode 940, Total Reward: -37\n",
      "Episode 960, Total Reward: -33\n",
      "Episode 980, Total Reward: -39\n",
      "Episode 1000, Total Reward: -35\n",
      "Episode 1020, Total Reward: -33\n",
      "Episode 1040, Total Reward: -35\n",
      "Episode 1060, Total Reward: -33\n",
      "Episode 1080, Total Reward: -35\n",
      "Episode 1100, Total Reward: -43\n",
      "Episode 1120, Total Reward: -55\n",
      "Episode 1140, Total Reward: -589.0\n",
      "Episode 1160, Total Reward: -287\n",
      "Episode 1180, Total Reward: -103\n",
      "Episode 1200, Total Reward: -37\n",
      "Episode 1220, Total Reward: -37\n",
      "Episode 1240, Total Reward: -41\n",
      "Episode 1260, Total Reward: -49\n",
      "Episode 1280, Total Reward: -41\n",
      "Episode 1300, Total Reward: -39\n",
      "Episode 1320, Total Reward: -41\n",
      "Episode 1340, Total Reward: -35\n",
      "Episode 1360, Total Reward: -47\n",
      "Episode 1380, Total Reward: -39\n",
      "Episode 1400, Total Reward: -117\n",
      "Episode 1420, Total Reward: -47\n",
      "Episode 1440, Total Reward: -47\n",
      "Episode 1460, Total Reward: -37\n",
      "Episode 1480, Total Reward: -73\n",
      "Episode 1500, Total Reward: -37\n",
      "Episode 1520, Total Reward: -35\n",
      "Episode 1540, Total Reward: -617\n",
      "Episode 1560, Total Reward: -105\n",
      "Episode 1580, Total Reward: -179\n",
      "Episode 1600, Total Reward: -39\n",
      "Episode 1620, Total Reward: -47\n",
      "Episode 1640, Total Reward: -2015\n",
      "Episode 1660, Total Reward: -39\n",
      "Episode 1680, Total Reward: -67\n",
      "Episode 1700, Total Reward: -43\n",
      "Episode 1720, Total Reward: -39\n",
      "Episode 1740, Total Reward: -35\n",
      "Episode 1760, Total Reward: -41\n",
      "Episode 1780, Total Reward: -35\n",
      "Episode 1800, Total Reward: -255\n",
      "Episode 1820, Total Reward: -57\n",
      "Episode 1840, Total Reward: -227\n",
      "Episode 1860, Total Reward: -43\n",
      "Episode 1880, Total Reward: -41\n",
      "Episode 1900, Total Reward: -235\n",
      "Episode 1920, Total Reward: -39\n",
      "Episode 1940, Total Reward: -269\n",
      "Episode 1960, Total Reward: -37\n",
      "Episode 1980, Total Reward: -37\n",
      "Episode 2000, Total Reward: -37\n",
      "Episode 2020, Total Reward: -107\n",
      "Episode 2040, Total Reward: -83\n",
      "Episode 2060, Total Reward: -346.0\n",
      "Episode 2080, Total Reward: -273\n",
      "Episode 2100, Total Reward: -73\n",
      "Episode 2120, Total Reward: -37\n",
      "Episode 2140, Total Reward: -93\n",
      "Episode 2160, Total Reward: -39\n",
      "Episode 2180, Total Reward: -35\n",
      "Episode 2200, Total Reward: -63\n",
      "Episode 2220, Total Reward: -41\n",
      "Episode 2240, Total Reward: -35\n",
      "Episode 2260, Total Reward: -193\n",
      "Episode 2280, Total Reward: -133\n",
      "Episode 2300, Total Reward: -91\n",
      "Episode 2320, Total Reward: -73\n",
      "Episode 2340, Total Reward: -81\n",
      "Episode 2360, Total Reward: -171\n",
      "Episode 2380, Total Reward: -37\n",
      "Episode 2400, Total Reward: -37\n",
      "Episode 2420, Total Reward: -35\n",
      "Episode 2440, Total Reward: -39\n",
      "Episode 2460, Total Reward: -37\n",
      "Episode 2480, Total Reward: -37\n",
      "Episode 2500, Total Reward: -42\n",
      "Episode 2520, Total Reward: -47\n",
      "Episode 2540, Total Reward: -35\n",
      "Episode 2560, Total Reward: -41\n",
      "Episode 2580, Total Reward: -99\n",
      "Episode 2600, Total Reward: -43\n",
      "Episode 2620, Total Reward: -37\n",
      "Episode 2640, Total Reward: -35\n",
      "Episode 2660, Total Reward: -35\n",
      "Episode 2680, Total Reward: -35\n",
      "Episode 2700, Total Reward: -35\n",
      "Episode 2720, Total Reward: -39\n",
      "Episode 2740, Total Reward: -59\n",
      "Episode 2760, Total Reward: -39\n",
      "Episode 2780, Total Reward: -35\n",
      "Episode 2800, Total Reward: -35\n",
      "Episode 2820, Total Reward: -35\n",
      "Episode 2840, Total Reward: -39\n",
      "Episode 2860, Total Reward: -393\n",
      "Episode 2880, Total Reward: -39\n",
      "Episode 2900, Total Reward: -43\n",
      "Episode 2920, Total Reward: -39\n",
      "Episode 2940, Total Reward: -39\n",
      "Episode 2960, Total Reward: -39\n",
      "Episode 2980, Total Reward: -37\n",
      "Episode 3000, Total Reward: -37\n",
      "Episode 3020, Total Reward: -39\n",
      "Episode 3040, Total Reward: -39\n",
      "Episode 3060, Total Reward: -35\n",
      "Episode 3080, Total Reward: -35\n",
      "Episode 3100, Total Reward: -39\n",
      "Episode 3120, Total Reward: -37\n",
      "Episode 3140, Total Reward: -35\n",
      "Episode 3160, Total Reward: -41\n",
      "Episode 3180, Total Reward: -39\n",
      "Episode 3200, Total Reward: -39\n",
      "Episode 3220, Total Reward: -39\n",
      "Episode 3240, Total Reward: -35\n",
      "Episode 3260, Total Reward: -39\n",
      "Episode 3280, Total Reward: -37\n",
      "Episode 3300, Total Reward: -37\n",
      "Episode 3320, Total Reward: -35\n",
      "Episode 3340, Total Reward: -37\n",
      "Episode 3360, Total Reward: -43\n",
      "Episode 3380, Total Reward: -51\n",
      "Episode 3400, Total Reward: -37\n",
      "Episode 3420, Total Reward: -278.0\n",
      "Episode 3440, Total Reward: -37\n",
      "Episode 3460, Total Reward: -38\n",
      "Episode 3480, Total Reward: -37\n",
      "Episode 3500, Total Reward: -37\n",
      "Episode 3520, Total Reward: -37\n",
      "Episode 3540, Total Reward: -37\n",
      "Episode 3560, Total Reward: -35\n",
      "Episode 3580, Total Reward: -35\n",
      "Episode 3600, Total Reward: -41\n",
      "Episode 3620, Total Reward: -37\n",
      "Episode 3640, Total Reward: -35\n",
      "Episode 3660, Total Reward: -35\n",
      "Episode 3680, Total Reward: -41\n",
      "Episode 3700, Total Reward: -39\n",
      "Episode 3720, Total Reward: -45\n",
      "Episode 3740, Total Reward: -37\n",
      "Episode 3760, Total Reward: -37\n",
      "Episode 3780, Total Reward: -296.0\n",
      "Episode 3800, Total Reward: -65\n",
      "Episode 3820, Total Reward: -53\n",
      "Episode 3840, Total Reward: -149\n",
      "Episode 3860, Total Reward: -35\n",
      "Episode 3880, Total Reward: -125\n",
      "Episode 3900, Total Reward: -213\n",
      "Episode 3920, Total Reward: -41\n",
      "Episode 3940, Total Reward: -41\n",
      "Episode 3960, Total Reward: -39\n",
      "Episode 3980, Total Reward: -37\n",
      "Episode 4000, Total Reward: -35\n",
      "Episode 4020, Total Reward: -91\n",
      "Episode 4040, Total Reward: -37\n",
      "Episode 4060, Total Reward: -41\n",
      "Episode 4080, Total Reward: -37\n",
      "Episode 4100, Total Reward: -744.0\n",
      "Episode 4120, Total Reward: -53\n",
      "Episode 4140, Total Reward: -61\n",
      "Episode 4160, Total Reward: -41\n",
      "Episode 4180, Total Reward: -49\n",
      "Episode 4200, Total Reward: -45\n",
      "Episode 4220, Total Reward: -272.0\n",
      "Episode 4240, Total Reward: -263\n",
      "Episode 4260, Total Reward: -35\n",
      "Episode 4280, Total Reward: -37\n",
      "Episode 4300, Total Reward: -37\n",
      "Episode 4320, Total Reward: -35\n",
      "Episode 4340, Total Reward: -37\n",
      "Episode 4360, Total Reward: -35\n",
      "Episode 4380, Total Reward: -35\n",
      "Episode 4400, Total Reward: -35\n",
      "Episode 4420, Total Reward: -37\n",
      "Episode 4440, Total Reward: -37\n",
      "Episode 4460, Total Reward: -33\n",
      "Episode 4480, Total Reward: -39\n",
      "Episode 4500, Total Reward: -35\n",
      "Episode 4520, Total Reward: -33\n",
      "Episode 4540, Total Reward: -33\n",
      "Episode 4560, Total Reward: -35\n",
      "Episode 4580, Total Reward: -33\n",
      "Episode 4600, Total Reward: -37\n",
      "Episode 4620, Total Reward: -35\n",
      "Episode 4640, Total Reward: -35\n",
      "Episode 4660, Total Reward: -35\n",
      "Episode 4680, Total Reward: -33\n",
      "Episode 4700, Total Reward: -33\n",
      "Episode 4720, Total Reward: -35\n",
      "Episode 4740, Total Reward: -33\n",
      "Episode 4760, Total Reward: -35\n",
      "Episode 4780, Total Reward: -35\n",
      "Episode 4800, Total Reward: -33\n",
      "Episode 4820, Total Reward: -35\n",
      "Episode 4840, Total Reward: -37\n",
      "Episode 4860, Total Reward: -35\n",
      "Episode 4880, Total Reward: -37\n",
      "Episode 4900, Total Reward: -35\n",
      "Episode 4920, Total Reward: -33\n",
      "Episode 4940, Total Reward: -33\n",
      "Episode 4960, Total Reward: -35\n",
      "Episode 4980, Total Reward: -37\n",
      "Episode 5000, Total Reward: -33\n",
      "Episode 5020, Total Reward: -33\n",
      "Episode 5040, Total Reward: -35\n",
      "Episode 5060, Total Reward: -33\n",
      "Episode 5080, Total Reward: -37\n",
      "Episode 5100, Total Reward: -35\n",
      "Episode 5120, Total Reward: -33\n",
      "Episode 5140, Total Reward: -35\n",
      "Episode 5160, Total Reward: -39\n",
      "Episode 5180, Total Reward: -35\n",
      "Episode 5200, Total Reward: -37\n",
      "Episode 5220, Total Reward: -33\n",
      "Episode 5240, Total Reward: -35\n",
      "Episode 5260, Total Reward: -33\n",
      "Episode 5280, Total Reward: -37\n",
      "Episode 5300, Total Reward: -35\n",
      "Episode 5320, Total Reward: -37\n",
      "Episode 5340, Total Reward: -33\n",
      "Episode 5360, Total Reward: -37\n",
      "Episode 5380, Total Reward: -35\n",
      "Episode 5400, Total Reward: -33\n",
      "Episode 5420, Total Reward: -37\n",
      "Episode 5440, Total Reward: -33\n",
      "Episode 5460, Total Reward: -33\n",
      "Episode 5480, Total Reward: -33\n",
      "Episode 5500, Total Reward: -37\n",
      "Episode 5520, Total Reward: -37\n",
      "Episode 5540, Total Reward: -37\n",
      "Episode 5560, Total Reward: -35\n",
      "Episode 5580, Total Reward: -35\n",
      "Episode 5600, Total Reward: -37\n",
      "Episode 5620, Total Reward: -35\n",
      "Episode 5640, Total Reward: -35\n",
      "Episode 5660, Total Reward: -33\n",
      "Episode 5680, Total Reward: -37\n",
      "Episode 5700, Total Reward: -37\n",
      "Episode 5720, Total Reward: -33\n",
      "Episode 5740, Total Reward: -35\n",
      "Episode 5760, Total Reward: -35\n",
      "Episode 5780, Total Reward: -33\n",
      "Episode 5800, Total Reward: -33\n",
      "Episode 5820, Total Reward: -35\n",
      "Episode 5840, Total Reward: -35\n",
      "Episode 5860, Total Reward: -33\n",
      "Episode 5880, Total Reward: -33\n",
      "Episode 5900, Total Reward: -33\n",
      "Episode 5920, Total Reward: -33\n",
      "Episode 5940, Total Reward: -37\n",
      "Episode 5960, Total Reward: -35\n",
      "Episode 5980, Total Reward: -37\n",
      "Episode 6000, Total Reward: -33\n",
      "Episode 6020, Total Reward: -35\n",
      "Episode 6040, Total Reward: -35\n",
      "Episode 6060, Total Reward: -33\n",
      "Episode 6080, Total Reward: -37\n",
      "Episode 6100, Total Reward: -35\n",
      "Episode 6120, Total Reward: -33\n",
      "Episode 6140, Total Reward: -37\n",
      "Episode 6160, Total Reward: -33\n",
      "Episode 6180, Total Reward: -35\n",
      "Episode 6200, Total Reward: -33\n",
      "Episode 6220, Total Reward: -37\n",
      "Episode 6240, Total Reward: -35\n",
      "Episode 6260, Total Reward: -35\n",
      "Episode 6280, Total Reward: -39\n",
      "Episode 6300, Total Reward: -33\n",
      "Episode 6320, Total Reward: -37\n",
      "Episode 6340, Total Reward: -35\n",
      "Episode 6360, Total Reward: -35\n",
      "Episode 6380, Total Reward: -33\n",
      "Episode 6400, Total Reward: -33\n",
      "Episode 6420, Total Reward: -35\n",
      "Episode 6440, Total Reward: -39\n",
      "Episode 6460, Total Reward: -41\n",
      "Episode 6480, Total Reward: -35\n",
      "Episode 6500, Total Reward: -35\n",
      "Episode 6520, Total Reward: -39\n",
      "Episode 6540, Total Reward: -35\n",
      "Episode 6560, Total Reward: -33\n",
      "Episode 6580, Total Reward: -37\n",
      "Episode 6600, Total Reward: -33\n",
      "Episode 6620, Total Reward: -35\n",
      "Episode 6640, Total Reward: -39\n",
      "Episode 6660, Total Reward: -37\n",
      "Episode 6680, Total Reward: -33\n",
      "Episode 6700, Total Reward: -289\n",
      "Episode 6720, Total Reward: -39\n",
      "Episode 6740, Total Reward: -37\n",
      "Episode 6760, Total Reward: -37\n",
      "Episode 6780, Total Reward: -39\n",
      "Episode 6800, Total Reward: -35\n",
      "Episode 6820, Total Reward: -35\n",
      "Episode 6840, Total Reward: -35\n",
      "Episode 6860, Total Reward: -33\n",
      "Episode 6880, Total Reward: -35\n",
      "Episode 6900, Total Reward: -33\n",
      "Episode 6920, Total Reward: -37\n",
      "Episode 6940, Total Reward: -33\n",
      "Episode 6960, Total Reward: -33\n",
      "Episode 6980, Total Reward: -33\n",
      "Episode 7000, Total Reward: -33\n",
      "Episode 7020, Total Reward: -35\n",
      "Episode 7040, Total Reward: -37\n",
      "Episode 7060, Total Reward: -35\n",
      "Episode 7080, Total Reward: -37\n",
      "Episode 7100, Total Reward: -35\n",
      "Episode 7120, Total Reward: -35\n",
      "Episode 7140, Total Reward: -33\n",
      "Episode 7160, Total Reward: -35\n",
      "Episode 7180, Total Reward: -35\n",
      "Episode 7200, Total Reward: -33\n",
      "Episode 7220, Total Reward: -35\n",
      "Episode 7240, Total Reward: -41\n",
      "Episode 7260, Total Reward: -35\n",
      "Episode 7280, Total Reward: -35\n",
      "Episode 7300, Total Reward: -35\n",
      "Episode 7320, Total Reward: -372.0\n",
      "Episode 7340, Total Reward: -35\n",
      "Episode 7360, Total Reward: -33\n",
      "Episode 7380, Total Reward: -37\n",
      "Episode 7400, Total Reward: -41\n",
      "Episode 7420, Total Reward: -33\n",
      "Episode 7440, Total Reward: -33\n",
      "Episode 7460, Total Reward: -37\n",
      "Episode 7480, Total Reward: -35\n",
      "Episode 7500, Total Reward: -33\n",
      "Episode 7520, Total Reward: -37\n",
      "Episode 7540, Total Reward: -33\n",
      "Episode 7560, Total Reward: -35\n",
      "Episode 7580, Total Reward: -35\n",
      "Episode 7600, Total Reward: -41\n",
      "Episode 7620, Total Reward: -37\n",
      "Episode 7640, Total Reward: -35\n",
      "Episode 7660, Total Reward: -37\n",
      "Episode 7680, Total Reward: -37\n",
      "Episode 7700, Total Reward: -35\n",
      "Episode 7720, Total Reward: -35\n",
      "Episode 7740, Total Reward: -35\n",
      "Episode 7760, Total Reward: -33\n",
      "Episode 7780, Total Reward: -33\n",
      "Episode 7800, Total Reward: -37\n",
      "Episode 7820, Total Reward: -35\n",
      "Episode 7840, Total Reward: -39\n",
      "Episode 7860, Total Reward: -35\n",
      "Episode 7880, Total Reward: -35\n",
      "Episode 7900, Total Reward: -35\n",
      "Episode 7920, Total Reward: -35\n",
      "Episode 7940, Total Reward: -35\n",
      "Episode 7960, Total Reward: -33\n",
      "Episode 7980, Total Reward: -39\n",
      "Episode 8000, Total Reward: -33\n",
      "Episode 8020, Total Reward: -37\n",
      "Episode 8040, Total Reward: -39\n",
      "Episode 8060, Total Reward: -93\n",
      "Episode 8080, Total Reward: -33\n",
      "Episode 8100, Total Reward: -33\n",
      "Episode 8120, Total Reward: -35\n",
      "Episode 8140, Total Reward: -39\n",
      "Episode 8160, Total Reward: -35\n",
      "Episode 8180, Total Reward: -33\n",
      "Episode 8200, Total Reward: -37\n",
      "Episode 8220, Total Reward: -41\n",
      "Episode 8240, Total Reward: -35\n",
      "Episode 8260, Total Reward: -35\n",
      "Episode 8280, Total Reward: -37\n",
      "Episode 8300, Total Reward: -49\n",
      "Episode 8320, Total Reward: -41\n",
      "Episode 8340, Total Reward: -41\n",
      "Episode 8360, Total Reward: -35\n",
      "Episode 8380, Total Reward: -41\n",
      "Episode 8400, Total Reward: -39\n",
      "Episode 8420, Total Reward: -45\n",
      "Episode 8440, Total Reward: -37\n",
      "Episode 8460, Total Reward: -40\n",
      "Episode 8480, Total Reward: -37\n",
      "Episode 8500, Total Reward: -39\n",
      "Episode 8520, Total Reward: -35\n",
      "Episode 8540, Total Reward: -35\n",
      "Episode 8560, Total Reward: -37\n",
      "Episode 8580, Total Reward: -38\n",
      "Episode 8600, Total Reward: -35\n",
      "Episode 8620, Total Reward: -35\n",
      "Episode 8640, Total Reward: -33\n",
      "Episode 8660, Total Reward: -35\n",
      "Episode 8680, Total Reward: -33\n",
      "Episode 8700, Total Reward: -35\n",
      "Episode 8720, Total Reward: -34\n",
      "Episode 8740, Total Reward: -33\n",
      "Episode 8760, Total Reward: -35\n",
      "Episode 8780, Total Reward: -35\n",
      "Episode 8800, Total Reward: -33\n",
      "Episode 8820, Total Reward: -37\n",
      "Episode 8840, Total Reward: -41\n",
      "Episode 8860, Total Reward: -35\n",
      "Episode 8880, Total Reward: -33\n",
      "Episode 8900, Total Reward: -35\n",
      "Episode 8920, Total Reward: -33\n",
      "Episode 8940, Total Reward: -33\n",
      "Episode 8960, Total Reward: -35\n",
      "Episode 8980, Total Reward: -33\n",
      "Episode 9000, Total Reward: -39\n",
      "Episode 9020, Total Reward: -51\n",
      "Episode 9040, Total Reward: -37\n",
      "Episode 9060, Total Reward: -35\n",
      "Episode 9080, Total Reward: -35\n",
      "Episode 9100, Total Reward: -35\n",
      "Episode 9120, Total Reward: -33\n",
      "Episode 9140, Total Reward: -37\n",
      "Episode 9160, Total Reward: -33\n",
      "Episode 9180, Total Reward: -33\n",
      "Episode 9200, Total Reward: -33\n",
      "Episode 9220, Total Reward: -35\n",
      "Episode 9240, Total Reward: -37\n",
      "Episode 9260, Total Reward: -33\n",
      "Episode 9280, Total Reward: -35\n",
      "Episode 9300, Total Reward: -37\n",
      "Episode 9320, Total Reward: -35\n",
      "Episode 9340, Total Reward: -33\n",
      "Episode 9360, Total Reward: -37\n",
      "Episode 9380, Total Reward: -37\n",
      "Episode 9400, Total Reward: -33\n",
      "Episode 9420, Total Reward: -33\n",
      "Episode 9440, Total Reward: -33\n",
      "Episode 9460, Total Reward: -35\n",
      "Episode 9480, Total Reward: -35\n",
      "Episode 9500, Total Reward: -35\n",
      "Episode 9520, Total Reward: -33\n",
      "Episode 9540, Total Reward: -39\n",
      "Episode 9560, Total Reward: -33\n",
      "Episode 9580, Total Reward: -35\n",
      "Episode 9600, Total Reward: -43\n",
      "Episode 9620, Total Reward: -35\n",
      "Episode 9640, Total Reward: -35\n",
      "Episode 9660, Total Reward: -33\n",
      "Episode 9680, Total Reward: -35\n",
      "Episode 9700, Total Reward: -33\n",
      "Episode 9720, Total Reward: -39\n",
      "Episode 9740, Total Reward: -33\n",
      "Episode 9760, Total Reward: -35\n",
      "Episode 9780, Total Reward: -35\n",
      "Episode 9800, Total Reward: -35\n",
      "Episode 9820, Total Reward: -37\n",
      "Episode 9840, Total Reward: -33\n",
      "Episode 9860, Total Reward: -35\n",
      "Episode 9880, Total Reward: -33\n",
      "Episode 9900, Total Reward: -33\n",
      "Episode 9920, Total Reward: -37\n",
      "Episode 9940, Total Reward: -39\n",
      "Episode 9960, Total Reward: -37\n",
      "Episode 9980, Total Reward: -33\n",
      "Episode 10000, Total Reward: -39\n",
      "Episode 10020, Total Reward: -35\n",
      "Episode 10040, Total Reward: -33\n",
      "Episode 10060, Total Reward: -35\n",
      "Episode 10080, Total Reward: -39\n",
      "Episode 10100, Total Reward: -35\n",
      "Episode 10120, Total Reward: -47\n",
      "Episode 10140, Total Reward: -37\n",
      "Episode 10160, Total Reward: -35\n",
      "Episode 10180, Total Reward: -37\n",
      "Episode 10200, Total Reward: -33\n",
      "Episode 10220, Total Reward: -41\n",
      "Episode 10240, Total Reward: -37\n",
      "Episode 10260, Total Reward: -35\n",
      "Episode 10280, Total Reward: -35\n",
      "Episode 10300, Total Reward: -33\n",
      "Episode 10320, Total Reward: -35\n",
      "Episode 10340, Total Reward: -33\n",
      "Episode 10360, Total Reward: -37\n",
      "Episode 10380, Total Reward: -33\n",
      "Episode 10400, Total Reward: -37\n",
      "Episode 10420, Total Reward: -35\n",
      "Episode 10440, Total Reward: -35\n",
      "Episode 10460, Total Reward: -41\n",
      "Episode 10480, Total Reward: -37\n",
      "Episode 10500, Total Reward: -37\n",
      "Episode 10520, Total Reward: -33\n",
      "Episode 10540, Total Reward: -35\n",
      "Episode 10560, Total Reward: -35\n",
      "Episode 10580, Total Reward: -35\n",
      "Episode 10600, Total Reward: -37\n",
      "Episode 10620, Total Reward: -35\n",
      "Episode 10640, Total Reward: -33\n",
      "Episode 10660, Total Reward: -35\n",
      "Episode 10680, Total Reward: -35\n",
      "Episode 10700, Total Reward: -33\n",
      "Episode 10720, Total Reward: -33\n",
      "Episode 10740, Total Reward: -35\n",
      "Episode 10760, Total Reward: -37\n",
      "Episode 10780, Total Reward: -33\n",
      "Episode 10800, Total Reward: -33\n",
      "Episode 10820, Total Reward: -37\n",
      "Episode 10840, Total Reward: -33\n",
      "Episode 10860, Total Reward: -35\n",
      "Episode 10880, Total Reward: -33\n",
      "Episode 10900, Total Reward: -33\n",
      "Episode 10920, Total Reward: -35\n",
      "Episode 10940, Total Reward: -37\n",
      "Episode 10960, Total Reward: -39\n",
      "Episode 10980, Total Reward: -33\n",
      "Episode 11000, Total Reward: -33\n",
      "Episode 11020, Total Reward: -35\n",
      "Episode 11040, Total Reward: -33\n",
      "Episode 11060, Total Reward: -35\n",
      "Episode 11080, Total Reward: -37\n",
      "Episode 11100, Total Reward: -35\n",
      "Episode 11120, Total Reward: -37\n",
      "Episode 11140, Total Reward: -33\n",
      "Episode 11160, Total Reward: -41\n",
      "Episode 11180, Total Reward: -33\n",
      "Episode 11200, Total Reward: -37\n",
      "Episode 11220, Total Reward: -33\n",
      "Episode 11240, Total Reward: -33\n",
      "Episode 11260, Total Reward: -37\n",
      "Episode 11280, Total Reward: -33\n",
      "Episode 11300, Total Reward: -33\n",
      "Episode 11320, Total Reward: -39\n",
      "Episode 11340, Total Reward: -33\n",
      "Episode 11360, Total Reward: -33\n",
      "Episode 11380, Total Reward: -37\n",
      "Episode 11400, Total Reward: -41\n",
      "Episode 11420, Total Reward: -35\n",
      "Episode 11440, Total Reward: -35\n",
      "Episode 11460, Total Reward: -35\n",
      "Episode 11480, Total Reward: -39\n",
      "Episode 11500, Total Reward: -37\n",
      "Episode 11520, Total Reward: -35\n",
      "Episode 11540, Total Reward: -35\n",
      "Episode 11560, Total Reward: -35\n",
      "Episode 11580, Total Reward: -35\n",
      "Episode 11600, Total Reward: -37\n",
      "Episode 11620, Total Reward: -35\n",
      "Episode 11640, Total Reward: -35\n",
      "Episode 11660, Total Reward: -33\n",
      "Episode 11680, Total Reward: -35\n",
      "Episode 11700, Total Reward: -35\n",
      "Episode 11720, Total Reward: -37\n",
      "Episode 11740, Total Reward: -33\n",
      "Episode 11760, Total Reward: -35\n",
      "Episode 11780, Total Reward: -35\n",
      "Episode 11800, Total Reward: -35\n",
      "Episode 11820, Total Reward: -225\n",
      "Episode 11840, Total Reward: -39\n",
      "Episode 11860, Total Reward: -37\n",
      "Episode 11880, Total Reward: -37\n",
      "Episode 11900, Total Reward: -33\n",
      "Episode 11920, Total Reward: -35\n",
      "Episode 11940, Total Reward: -37\n",
      "Episode 11960, Total Reward: -39\n",
      "Episode 11980, Total Reward: -33\n",
      "Episode 12000, Total Reward: -39\n",
      "Episode 12020, Total Reward: -35\n",
      "Episode 12040, Total Reward: -35\n",
      "Episode 12060, Total Reward: -33\n",
      "Episode 12080, Total Reward: -37\n",
      "Episode 12100, Total Reward: -35\n",
      "Episode 12120, Total Reward: -37\n",
      "Episode 12140, Total Reward: -35\n",
      "Episode 12160, Total Reward: -33\n",
      "Episode 12180, Total Reward: -35\n",
      "Episode 12200, Total Reward: -33\n",
      "Episode 12220, Total Reward: -37\n",
      "Episode 12240, Total Reward: -37\n",
      "Episode 12260, Total Reward: -37\n",
      "Episode 12280, Total Reward: -33\n",
      "Episode 12300, Total Reward: -35\n",
      "Episode 12320, Total Reward: -37\n",
      "Episode 12340, Total Reward: -35\n",
      "Episode 12360, Total Reward: -35\n",
      "Episode 12380, Total Reward: -35\n",
      "Episode 12400, Total Reward: -35\n",
      "Episode 12420, Total Reward: -35\n",
      "Episode 12440, Total Reward: -35\n",
      "Episode 12460, Total Reward: -33\n",
      "Episode 12480, Total Reward: -33\n",
      "Episode 12500, Total Reward: -33\n",
      "Episode 12520, Total Reward: -35\n",
      "Episode 12540, Total Reward: -35\n",
      "Episode 12560, Total Reward: -33\n",
      "Episode 12580, Total Reward: -41\n",
      "Episode 12600, Total Reward: -35\n",
      "Episode 12620, Total Reward: -33\n",
      "Episode 12640, Total Reward: -33\n",
      "Episode 12660, Total Reward: -35\n",
      "Episode 12680, Total Reward: -39\n",
      "Episode 12700, Total Reward: -37\n",
      "Episode 12720, Total Reward: -35\n",
      "Episode 12740, Total Reward: -35\n",
      "Episode 12760, Total Reward: -33\n",
      "Episode 12780, Total Reward: -35\n",
      "Episode 12800, Total Reward: -33\n",
      "Episode 12820, Total Reward: -43\n",
      "Episode 12840, Total Reward: -39\n",
      "Episode 12860, Total Reward: -37\n",
      "Episode 12880, Total Reward: -35\n",
      "Episode 12900, Total Reward: -33\n",
      "Episode 12920, Total Reward: -35\n",
      "Episode 12940, Total Reward: -37\n",
      "Episode 12960, Total Reward: -39\n",
      "Episode 12980, Total Reward: -35\n",
      "Episode 13000, Total Reward: -39\n",
      "Episode 13020, Total Reward: -156.0\n",
      "Episode 13040, Total Reward: -35\n",
      "Episode 13060, Total Reward: -33\n",
      "Episode 13080, Total Reward: -33\n",
      "Episode 13100, Total Reward: -37\n",
      "Episode 13120, Total Reward: -41\n",
      "Episode 13140, Total Reward: -33\n",
      "Episode 13160, Total Reward: -33\n",
      "Episode 13180, Total Reward: -35\n",
      "Episode 13200, Total Reward: -41\n",
      "Episode 13220, Total Reward: -39\n",
      "Episode 13240, Total Reward: -35\n",
      "Episode 13260, Total Reward: -33\n",
      "Episode 13280, Total Reward: -37\n",
      "Episode 13300, Total Reward: -33\n",
      "Episode 13320, Total Reward: -41\n",
      "Episode 13340, Total Reward: -33\n",
      "Episode 13360, Total Reward: -35\n",
      "Episode 13380, Total Reward: -37\n",
      "Episode 13400, Total Reward: -35\n",
      "Episode 13420, Total Reward: -37\n",
      "Episode 13440, Total Reward: -37\n",
      "Episode 13460, Total Reward: -33\n",
      "Episode 13480, Total Reward: -35\n",
      "Episode 13500, Total Reward: -33\n",
      "Episode 13520, Total Reward: -39\n",
      "Episode 13540, Total Reward: -33\n",
      "Episode 13560, Total Reward: -33\n",
      "Episode 13580, Total Reward: -33\n",
      "Episode 13600, Total Reward: -35\n",
      "Episode 13620, Total Reward: -35\n",
      "Episode 13640, Total Reward: -37\n",
      "Episode 13660, Total Reward: -33\n",
      "Episode 13680, Total Reward: -35\n",
      "Episode 13700, Total Reward: -33\n",
      "Episode 13720, Total Reward: -33\n",
      "Episode 13740, Total Reward: -276.0\n",
      "Episode 13760, Total Reward: -37\n",
      "Episode 13780, Total Reward: -35\n",
      "Episode 13800, Total Reward: -33\n",
      "Episode 13820, Total Reward: -33\n",
      "Episode 13840, Total Reward: -35\n",
      "Episode 13860, Total Reward: -33\n",
      "Episode 13880, Total Reward: -35\n",
      "Episode 13900, Total Reward: -33\n",
      "Episode 13920, Total Reward: -37\n",
      "Episode 13940, Total Reward: -33\n",
      "Episode 13960, Total Reward: -33\n",
      "Episode 13980, Total Reward: -35\n",
      "Episode 14000, Total Reward: -33\n",
      "Episode 14020, Total Reward: -33\n",
      "Episode 14040, Total Reward: -35\n",
      "Episode 14060, Total Reward: -35\n",
      "Episode 14080, Total Reward: -35\n",
      "Episode 14100, Total Reward: -35\n",
      "Episode 14120, Total Reward: -41\n",
      "Episode 14140, Total Reward: -37\n",
      "Episode 14160, Total Reward: -39\n",
      "Episode 14180, Total Reward: -35\n",
      "Episode 14200, Total Reward: -35\n",
      "Episode 14220, Total Reward: -35\n",
      "Episode 14240, Total Reward: -33\n",
      "Episode 14260, Total Reward: -33\n",
      "Episode 14280, Total Reward: -33\n",
      "Episode 14300, Total Reward: -33\n",
      "Episode 14320, Total Reward: -41\n",
      "Episode 14340, Total Reward: -39\n",
      "Episode 14360, Total Reward: -35\n",
      "Episode 14380, Total Reward: -33\n",
      "Episode 14400, Total Reward: -33\n",
      "Episode 14420, Total Reward: -39\n",
      "Episode 14440, Total Reward: -33\n",
      "Episode 14460, Total Reward: -35\n",
      "Episode 14480, Total Reward: -37\n",
      "Episode 14500, Total Reward: -35\n",
      "Episode 14520, Total Reward: -33\n",
      "Episode 14540, Total Reward: -33\n",
      "Episode 14560, Total Reward: -33\n",
      "Episode 14580, Total Reward: -33\n",
      "Episode 14600, Total Reward: -33\n",
      "Episode 14620, Total Reward: -43\n",
      "Episode 14640, Total Reward: -35\n",
      "Episode 14660, Total Reward: -35\n",
      "Episode 14680, Total Reward: -39\n",
      "Episode 14700, Total Reward: -37\n",
      "Episode 14720, Total Reward: -39\n",
      "Episode 14740, Total Reward: -37\n",
      "Episode 14760, Total Reward: -35\n",
      "Episode 14780, Total Reward: -35\n",
      "Episode 14800, Total Reward: -35\n",
      "Episode 14820, Total Reward: -35\n",
      "Episode 14840, Total Reward: -33\n",
      "Episode 14860, Total Reward: -33\n",
      "Episode 14880, Total Reward: -33\n",
      "Episode 14900, Total Reward: -37\n",
      "Episode 14920, Total Reward: -33\n",
      "Episode 14940, Total Reward: -33\n",
      "Episode 14960, Total Reward: -37\n",
      "Episode 14980, Total Reward: -37\n"
     ]
    }
   ],
   "source": [
    "## simulare the agent in the environment\n",
    "num_actions = len(env.get_wrapper_attr(\"actions\"))\n",
    "agent = tabularQlearning(num_feature= num_features, num_actions=num_actions)\n",
    "\n",
    "num_episodes = 15000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, info = env.reset()\n",
    "    state = rep.get_features(obs)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_obs, reward, done, trunc, _ = env.step(action)\n",
    "        next_state = rep.get_features(next_obs)\n",
    "        agent.update(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    if episode % 20 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " t: 0, observation: [0.2  0.35], reward: -1\n",
      " t: 1, observation: [0.2 0.3], reward: -1\n",
      " t: 2, observation: [0.2  0.25], reward: -1\n",
      " t: 3, observation: [0.2 0.2], reward: -1\n",
      " t: 4, observation: [0.25 0.2 ], reward: -1\n",
      " t: 5, observation: [0.3 0.2], reward: -1\n",
      " t: 6, observation: [0.35 0.2 ], reward: -1\n",
      " t: 7, observation: [0.4 0.2], reward: -1\n",
      " t: 8, observation: [0.45 0.2 ], reward: -1\n",
      " t: 9, observation: [0.5 0.2], reward: -1\n",
      " t: 10, observation: [0.55 0.2 ], reward: -1\n",
      " t: 11, observation: [0.6 0.2], reward: -1\n",
      " t: 12, observation: [0.55 0.2 ], reward: -1\n",
      " t: 13, observation: [0.6 0.2], reward: -1\n",
      " t: 14, observation: [0.65 0.2 ], reward: -1\n",
      " t: 15, observation: [0.65 0.25], reward: -1\n",
      " t: 16, observation: [0.65 0.3 ], reward: -1\n",
      " t: 17, observation: [0.65 0.35], reward: -1\n",
      " t: 18, observation: [0.65 0.4 ], reward: -1\n",
      " t: 19, observation: [0.6 0.4], reward: -1\n",
      " t: 20, observation: [0.6  0.45], reward: -1\n",
      " t: 21, observation: [0.6 0.5], reward: -1\n",
      " t: 22, observation: [0.6  0.55], reward: -1\n",
      " t: 23, observation: [0.6 0.5], reward: -1\n",
      " t: 24, observation: [0.65 0.5 ], reward: -1\n",
      " t: 25, observation: [0.65 0.55], reward: -1\n",
      " t: 26, observation: [0.65 0.6 ], reward: -1\n",
      " t: 27, observation: [0.65 0.65], reward: -1\n",
      " t: 28, observation: [0.65 0.7 ], reward: -1\n",
      " t: 29, observation: [0.65 0.75], reward: -1\n",
      " t: 30, observation: [0.65 0.8 ], reward: -1\n",
      " t: 31, observation: [0.7 0.8], reward: -1\n",
      " t: 32, observation: [0.75 0.8 ], reward: -1\n",
      " t: 33, observation: [0.75 0.85], reward: -1\n",
      " t: 34, observation: [0.75 0.9 ], reward: -1\n",
      " t: 35, observation: [0.75 0.95], reward: -1\n",
      " t: 36, observation: [0.8  0.95], reward: -1\n",
      " t: 37, observation: [0.85 0.95], reward: -1\n",
      " t: 38, observation: [0.85 1.  ], reward: -1\n",
      " t: 39, observation: [0.9 1. ], reward: 0\n",
      "total reward in this episode: -39\n",
      "episode 0: reward: -39\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Test the trained model\n",
    "obs, info = env.reset()\n",
    "total_reward = 0\n",
    "episode_rewards = []\n",
    "frames = []\n",
    "observation = obs\n",
    "\n",
    "max_video_length = 120\n",
    "\n",
    "\n",
    "def greedy_policy(state):\n",
    "    state_index = agent.encode_state(state)\n",
    "    return np.argmax(agent.q_table[state_index])\n",
    "\n",
    "for time_step in range(max_video_length):\n",
    "    \n",
    "    frames.append(env.render())\n",
    "\n",
    "    #action = greedy_policy(get_features(observation))\n",
    "    action = agent.choose_action(rep.get_features(observation))\n",
    "    observation, reward, done, trunc, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    image = env.render()\n",
    "    #online_rendering(image) #uncomment this line to see the online rendering of the environment frame by frame\n",
    "    frames.append(image)\n",
    "\n",
    "    print(f\" t: {time_step}, observation: {observation}, reward: {reward}\") #uncomment this line to see the environment-agent interaction details\n",
    "\n",
    "    if done:\n",
    "      print(f\"total reward in this episode: {total_reward}\")\n",
    "      episode_rewards.append(total_reward)\n",
    "      total_reward = 0\n",
    "      break\n",
    "\n",
    "env.close()\n",
    "\n",
    "if episode_rewards == []:\n",
    "  print(\"no episode finished in this run.\")\n",
    "else:\n",
    "  for i, reward in enumerate(episode_rewards):\n",
    "    print(f\"episode {i}: reward: {reward}\")\n",
    "\n",
    "visualize(frames, \"./Video/q_learning_Kan.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning with Eligibility Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tabularQlearningET:\n",
    "    def __init__(self, num_feature, num_actions, alpha=0.1, gamma=0.99, epsilon=0.1, lambda_=0.9, seed = selected_seed):\n",
    "        self.num_feature = num_feature\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "        # Initialize Q-table with zeros\n",
    "        self.q_table = np.zeros((self.num_feature, num_actions))\n",
    "        self.eligibility = np.zeros(self.num_feature)\n",
    "\n",
    "        self.seed = seed\n",
    "        # Set random seed\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Choose an action using epsilon-greedy policy\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            q = self.q_table[state].sum(axis=0)\n",
    "            return q.argmax()\n",
    "    \n",
    "    def update(self, state, action, reward, next_state):\n",
    "        self.eligibility *= self.gamma * self.lambda_\n",
    "        self.eligibility[state] =1\n",
    "\n",
    "        v_next = self.gamma * self.q_table[next_state].sum(axis=0).max()\n",
    "\n",
    "        td_error = reward + v_next - self.q_table[state, action].sum()\n",
    "\n",
    "        alpha = self.alpha / len(state)\n",
    "\n",
    "        self.q_table[:, action] += alpha * td_error * self.eligibility\n",
    "\n",
    "    def erase_eligibility(self):\n",
    "        self.eligibility = np.zeros(self.num_feature)\n",
    "        \n",
    "    def get_q_table(self):\n",
    "        return self.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -2689.912530835113\n",
      "Episode 20, Total Reward: -72\n",
      "Episode 40, Total Reward: -44\n",
      "Episode 60, Total Reward: -56\n",
      "Episode 80, Total Reward: -64\n",
      "Episode 100, Total Reward: -74\n",
      "Episode 120, Total Reward: -66\n",
      "Episode 140, Total Reward: -66\n",
      "Episode 160, Total Reward: -376\n",
      "Episode 180, Total Reward: -74\n",
      "Episode 200, Total Reward: -52\n",
      "Episode 220, Total Reward: -55\n",
      "Episode 240, Total Reward: -48\n",
      "Episode 260, Total Reward: -41\n",
      "Episode 280, Total Reward: -44\n",
      "Episode 300, Total Reward: -50\n",
      "Episode 320, Total Reward: -278.1864330418383\n",
      "Episode 340, Total Reward: -49\n",
      "Episode 360, Total Reward: -54\n",
      "Episode 380, Total Reward: -46\n",
      "Episode 400, Total Reward: -44\n",
      "Episode 420, Total Reward: -48\n",
      "Episode 440, Total Reward: -50\n",
      "Episode 460, Total Reward: -52\n",
      "Episode 480, Total Reward: -47\n",
      "Episode 500, Total Reward: -43\n",
      "Episode 520, Total Reward: -44\n",
      "Episode 540, Total Reward: -43\n",
      "Episode 560, Total Reward: -48\n",
      "Episode 580, Total Reward: -45\n",
      "Episode 600, Total Reward: -42\n",
      "Episode 620, Total Reward: -41\n",
      "Episode 640, Total Reward: -41\n",
      "Episode 660, Total Reward: -45\n",
      "Episode 680, Total Reward: -47\n",
      "Episode 700, Total Reward: -46\n",
      "Episode 720, Total Reward: -54\n",
      "Episode 740, Total Reward: -45\n",
      "Episode 760, Total Reward: -46\n",
      "Episode 780, Total Reward: -57\n",
      "Episode 800, Total Reward: -47\n",
      "Episode 820, Total Reward: -42\n",
      "Episode 840, Total Reward: -42\n",
      "Episode 860, Total Reward: -40\n",
      "Episode 880, Total Reward: -38\n",
      "Episode 900, Total Reward: -56\n",
      "Episode 920, Total Reward: -45\n",
      "Episode 940, Total Reward: -48\n",
      "Episode 960, Total Reward: -52\n",
      "Episode 980, Total Reward: -42\n",
      "Episode 1000, Total Reward: -42\n",
      "Episode 1020, Total Reward: -41\n",
      "Episode 1040, Total Reward: -42\n",
      "Episode 1060, Total Reward: -43\n",
      "Episode 1080, Total Reward: -37\n",
      "Episode 1100, Total Reward: -39\n",
      "Episode 1120, Total Reward: -44\n",
      "Episode 1140, Total Reward: -45\n",
      "Episode 1160, Total Reward: -38\n",
      "Episode 1180, Total Reward: -41\n",
      "Episode 1200, Total Reward: -48\n",
      "Episode 1220, Total Reward: -45\n",
      "Episode 1240, Total Reward: -43\n",
      "Episode 1260, Total Reward: -37\n",
      "Episode 1280, Total Reward: -40\n",
      "Episode 1300, Total Reward: -33\n",
      "Episode 1320, Total Reward: -43\n",
      "Episode 1340, Total Reward: -41\n",
      "Episode 1360, Total Reward: -41\n",
      "Episode 1380, Total Reward: -43\n",
      "Episode 1400, Total Reward: -42\n",
      "Episode 1420, Total Reward: -43\n",
      "Episode 1440, Total Reward: -39\n",
      "Episode 1460, Total Reward: -41\n",
      "Episode 1480, Total Reward: -41\n",
      "Episode 1500, Total Reward: -37\n",
      "Episode 1520, Total Reward: -39\n",
      "Episode 1540, Total Reward: -40\n",
      "Episode 1560, Total Reward: -42\n",
      "Episode 1580, Total Reward: -40\n",
      "Episode 1600, Total Reward: -38\n",
      "Episode 1620, Total Reward: -61\n",
      "Episode 1640, Total Reward: -43\n",
      "Episode 1660, Total Reward: -52\n",
      "Episode 1680, Total Reward: -55\n",
      "Episode 1700, Total Reward: -51\n",
      "Episode 1720, Total Reward: -51\n",
      "Episode 1740, Total Reward: -48\n",
      "Episode 1760, Total Reward: -45\n",
      "Episode 1780, Total Reward: -47\n",
      "Episode 1800, Total Reward: -47\n",
      "Episode 1820, Total Reward: -40\n",
      "Episode 1840, Total Reward: -43\n",
      "Episode 1860, Total Reward: -43\n",
      "Episode 1880, Total Reward: -49\n",
      "Episode 1900, Total Reward: -44\n",
      "Episode 1920, Total Reward: -48\n",
      "Episode 1940, Total Reward: -48\n",
      "Episode 1960, Total Reward: -50\n",
      "Episode 1980, Total Reward: -43\n",
      "Episode 2000, Total Reward: -51\n",
      "Episode 2020, Total Reward: -50\n",
      "Episode 2040, Total Reward: -43\n",
      "Episode 2060, Total Reward: -49\n",
      "Episode 2080, Total Reward: -41\n",
      "Episode 2100, Total Reward: -46\n",
      "Episode 2120, Total Reward: -48\n",
      "Episode 2140, Total Reward: -43\n",
      "Episode 2160, Total Reward: -43\n",
      "Episode 2180, Total Reward: -45\n",
      "Episode 2200, Total Reward: -50\n",
      "Episode 2220, Total Reward: -66\n",
      "Episode 2240, Total Reward: -44\n",
      "Episode 2260, Total Reward: -46\n",
      "Episode 2280, Total Reward: -43\n",
      "Episode 2300, Total Reward: -40\n",
      "Episode 2320, Total Reward: -45\n",
      "Episode 2340, Total Reward: -42\n",
      "Episode 2360, Total Reward: -51\n",
      "Episode 2380, Total Reward: -51\n",
      "Episode 2400, Total Reward: -52\n",
      "Episode 2420, Total Reward: -45\n",
      "Episode 2440, Total Reward: -51\n",
      "Episode 2460, Total Reward: -54\n",
      "Episode 2480, Total Reward: -45\n",
      "Episode 2500, Total Reward: -42\n",
      "Episode 2520, Total Reward: -40\n",
      "Episode 2540, Total Reward: -50\n",
      "Episode 2560, Total Reward: -49\n",
      "Episode 2580, Total Reward: -51\n",
      "Episode 2600, Total Reward: -48\n",
      "Episode 2620, Total Reward: -41\n",
      "Episode 2640, Total Reward: -40\n",
      "Episode 2660, Total Reward: -44\n",
      "Episode 2680, Total Reward: -40\n",
      "Episode 2700, Total Reward: -48\n",
      "Episode 2720, Total Reward: -56\n",
      "Episode 2740, Total Reward: -43\n",
      "Episode 2760, Total Reward: -42\n",
      "Episode 2780, Total Reward: -47\n",
      "Episode 2800, Total Reward: -47\n",
      "Episode 2820, Total Reward: -47\n",
      "Episode 2840, Total Reward: -44\n",
      "Episode 2860, Total Reward: -42\n",
      "Episode 2880, Total Reward: -59\n",
      "Episode 2900, Total Reward: -42\n",
      "Episode 2920, Total Reward: -41\n",
      "Episode 2940, Total Reward: -46\n",
      "Episode 2960, Total Reward: -44\n",
      "Episode 2980, Total Reward: -45\n",
      "Episode 3000, Total Reward: -51\n",
      "Episode 3020, Total Reward: -38\n",
      "Episode 3040, Total Reward: -58\n",
      "Episode 3060, Total Reward: -40\n",
      "Episode 3080, Total Reward: -47\n",
      "Episode 3100, Total Reward: -40\n",
      "Episode 3120, Total Reward: -57\n",
      "Episode 3140, Total Reward: -47\n",
      "Episode 3160, Total Reward: -46\n",
      "Episode 3180, Total Reward: -56\n",
      "Episode 3200, Total Reward: -41\n",
      "Episode 3220, Total Reward: -42\n",
      "Episode 3240, Total Reward: -51\n",
      "Episode 3260, Total Reward: -41\n",
      "Episode 3280, Total Reward: -40\n",
      "Episode 3300, Total Reward: -45\n",
      "Episode 3320, Total Reward: -51\n",
      "Episode 3340, Total Reward: -43\n",
      "Episode 3360, Total Reward: -39\n",
      "Episode 3380, Total Reward: -41\n",
      "Episode 3400, Total Reward: -45\n",
      "Episode 3420, Total Reward: -40\n",
      "Episode 3440, Total Reward: -49\n",
      "Episode 3460, Total Reward: -41\n",
      "Episode 3480, Total Reward: -52\n",
      "Episode 3500, Total Reward: -49\n",
      "Episode 3520, Total Reward: -42\n",
      "Episode 3540, Total Reward: -47\n",
      "Episode 3560, Total Reward: -41\n",
      "Episode 3580, Total Reward: -285.52339240240696\n",
      "Episode 3600, Total Reward: -37\n",
      "Episode 3620, Total Reward: -50\n",
      "Episode 3640, Total Reward: -37\n",
      "Episode 3660, Total Reward: -42\n",
      "Episode 3680, Total Reward: -39\n",
      "Episode 3700, Total Reward: -39\n",
      "Episode 3720, Total Reward: -46\n",
      "Episode 3740, Total Reward: -49\n",
      "Episode 3760, Total Reward: -52\n",
      "Episode 3780, Total Reward: -50\n",
      "Episode 3800, Total Reward: -43\n",
      "Episode 3820, Total Reward: -39\n",
      "Episode 3840, Total Reward: -38\n",
      "Episode 3860, Total Reward: -48\n",
      "Episode 3880, Total Reward: -43\n",
      "Episode 3900, Total Reward: -48\n",
      "Episode 3920, Total Reward: -49\n",
      "Episode 3940, Total Reward: -54\n",
      "Episode 3960, Total Reward: -43\n",
      "Episode 3980, Total Reward: -45\n",
      "Episode 4000, Total Reward: -45\n",
      "Episode 4020, Total Reward: -43\n",
      "Episode 4040, Total Reward: -42\n",
      "Episode 4060, Total Reward: -50\n",
      "Episode 4080, Total Reward: -42\n",
      "Episode 4100, Total Reward: -45\n",
      "Episode 4120, Total Reward: -38\n",
      "Episode 4140, Total Reward: -47\n",
      "Episode 4160, Total Reward: -45\n",
      "Episode 4180, Total Reward: -38\n",
      "Episode 4200, Total Reward: -41\n",
      "Episode 4220, Total Reward: -41\n",
      "Episode 4240, Total Reward: -41\n",
      "Episode 4260, Total Reward: -45\n",
      "Episode 4280, Total Reward: -40\n",
      "Episode 4300, Total Reward: -44\n",
      "Episode 4320, Total Reward: -52\n",
      "Episode 4340, Total Reward: -41\n",
      "Episode 4360, Total Reward: -46\n",
      "Episode 4380, Total Reward: -41\n",
      "Episode 4400, Total Reward: -39\n",
      "Episode 4420, Total Reward: -43\n",
      "Episode 4440, Total Reward: -44\n",
      "Episode 4460, Total Reward: -46\n",
      "Episode 4480, Total Reward: -45\n",
      "Episode 4500, Total Reward: -44\n",
      "Episode 4520, Total Reward: -37\n",
      "Episode 4540, Total Reward: -45\n",
      "Episode 4560, Total Reward: -47\n",
      "Episode 4580, Total Reward: -38\n",
      "Episode 4600, Total Reward: -47\n",
      "Episode 4620, Total Reward: -45\n",
      "Episode 4640, Total Reward: -41\n",
      "Episode 4660, Total Reward: -43\n",
      "Episode 4680, Total Reward: -42\n",
      "Episode 4700, Total Reward: -42\n",
      "Episode 4720, Total Reward: -43\n",
      "Episode 4740, Total Reward: -40\n",
      "Episode 4760, Total Reward: -48\n",
      "Episode 4780, Total Reward: -37\n",
      "Episode 4800, Total Reward: -43\n",
      "Episode 4820, Total Reward: -37\n",
      "Episode 4840, Total Reward: -46\n",
      "Episode 4860, Total Reward: -41\n",
      "Episode 4880, Total Reward: -42\n",
      "Episode 4900, Total Reward: -37\n",
      "Episode 4920, Total Reward: -51\n",
      "Episode 4940, Total Reward: -41\n",
      "Episode 4960, Total Reward: -48\n",
      "Episode 4980, Total Reward: -42\n",
      "Episode 5000, Total Reward: -49\n",
      "Episode 5020, Total Reward: -43\n",
      "Episode 5040, Total Reward: -43\n",
      "Episode 5060, Total Reward: -45\n",
      "Episode 5080, Total Reward: -42\n",
      "Episode 5100, Total Reward: -42\n",
      "Episode 5120, Total Reward: -46\n",
      "Episode 5140, Total Reward: -43\n",
      "Episode 5160, Total Reward: -40\n",
      "Episode 5180, Total Reward: -39\n",
      "Episode 5200, Total Reward: -38\n",
      "Episode 5220, Total Reward: -40\n",
      "Episode 5240, Total Reward: -40\n",
      "Episode 5260, Total Reward: -38\n",
      "Episode 5280, Total Reward: -45\n",
      "Episode 5300, Total Reward: -48\n",
      "Episode 5320, Total Reward: -45\n",
      "Episode 5340, Total Reward: -45\n",
      "Episode 5360, Total Reward: -44\n",
      "Episode 5380, Total Reward: -39\n",
      "Episode 5400, Total Reward: -38\n",
      "Episode 5420, Total Reward: -52\n",
      "Episode 5440, Total Reward: -42\n",
      "Episode 5460, Total Reward: -42\n",
      "Episode 5480, Total Reward: -45\n",
      "Episode 5500, Total Reward: -44\n",
      "Episode 5520, Total Reward: -45\n",
      "Episode 5540, Total Reward: -44\n",
      "Episode 5560, Total Reward: -42\n",
      "Episode 5580, Total Reward: -45\n",
      "Episode 5600, Total Reward: -42\n",
      "Episode 5620, Total Reward: -46\n",
      "Episode 5640, Total Reward: -48\n",
      "Episode 5660, Total Reward: -51\n",
      "Episode 5680, Total Reward: -43\n",
      "Episode 5700, Total Reward: -45\n",
      "Episode 5720, Total Reward: -44\n",
      "Episode 5740, Total Reward: -46\n",
      "Episode 5760, Total Reward: -41\n",
      "Episode 5780, Total Reward: -41\n",
      "Episode 5800, Total Reward: -50\n",
      "Episode 5820, Total Reward: -41\n",
      "Episode 5840, Total Reward: -44\n",
      "Episode 5860, Total Reward: -48\n",
      "Episode 5880, Total Reward: -43\n",
      "Episode 5900, Total Reward: -40\n",
      "Episode 5920, Total Reward: -51\n",
      "Episode 5940, Total Reward: -42\n",
      "Episode 5960, Total Reward: -40\n",
      "Episode 5980, Total Reward: -50\n",
      "Episode 6000, Total Reward: -39\n",
      "Episode 6020, Total Reward: -42\n",
      "Episode 6040, Total Reward: -39\n",
      "Episode 6060, Total Reward: -40\n",
      "Episode 6080, Total Reward: -44\n",
      "Episode 6100, Total Reward: -44\n",
      "Episode 6120, Total Reward: -41\n",
      "Episode 6140, Total Reward: -40\n",
      "Episode 6160, Total Reward: -40\n",
      "Episode 6180, Total Reward: -47\n",
      "Episode 6200, Total Reward: -50\n",
      "Episode 6220, Total Reward: -41\n",
      "Episode 6240, Total Reward: -41\n",
      "Episode 6260, Total Reward: -43\n",
      "Episode 6280, Total Reward: -45\n",
      "Episode 6300, Total Reward: -51\n",
      "Episode 6320, Total Reward: -45\n",
      "Episode 6340, Total Reward: -39\n",
      "Episode 6360, Total Reward: -36\n",
      "Episode 6380, Total Reward: -49\n",
      "Episode 6400, Total Reward: -44\n",
      "Episode 6420, Total Reward: -36\n",
      "Episode 6440, Total Reward: -41\n",
      "Episode 6460, Total Reward: -37\n",
      "Episode 6480, Total Reward: -46\n",
      "Episode 6500, Total Reward: -44\n",
      "Episode 6520, Total Reward: -43\n",
      "Episode 6540, Total Reward: -44\n",
      "Episode 6560, Total Reward: -38\n",
      "Episode 6580, Total Reward: -46\n",
      "Episode 6600, Total Reward: -47\n",
      "Episode 6620, Total Reward: -62\n",
      "Episode 6640, Total Reward: -38\n",
      "Episode 6660, Total Reward: -41\n",
      "Episode 6680, Total Reward: -58\n",
      "Episode 6700, Total Reward: -52\n",
      "Episode 6720, Total Reward: -38\n",
      "Episode 6740, Total Reward: -36\n",
      "Episode 6760, Total Reward: -49\n",
      "Episode 6780, Total Reward: -46\n",
      "Episode 6800, Total Reward: -41\n",
      "Episode 6820, Total Reward: -50\n",
      "Episode 6840, Total Reward: -38\n",
      "Episode 6860, Total Reward: -37\n",
      "Episode 6880, Total Reward: -41\n",
      "Episode 6900, Total Reward: -52\n",
      "Episode 6920, Total Reward: -40\n",
      "Episode 6940, Total Reward: -44\n",
      "Episode 6960, Total Reward: -38\n",
      "Episode 6980, Total Reward: -39\n",
      "Episode 7000, Total Reward: -41\n",
      "Episode 7020, Total Reward: -41\n",
      "Episode 7040, Total Reward: -45\n",
      "Episode 7060, Total Reward: -48\n",
      "Episode 7080, Total Reward: -43\n",
      "Episode 7100, Total Reward: -40\n",
      "Episode 7120, Total Reward: -43\n",
      "Episode 7140, Total Reward: -40\n",
      "Episode 7160, Total Reward: -40\n",
      "Episode 7180, Total Reward: -41\n",
      "Episode 7200, Total Reward: -49\n",
      "Episode 7220, Total Reward: -44\n",
      "Episode 7240, Total Reward: -48\n",
      "Episode 7260, Total Reward: -39\n",
      "Episode 7280, Total Reward: -45\n",
      "Episode 7300, Total Reward: -39\n",
      "Episode 7320, Total Reward: -38\n",
      "Episode 7340, Total Reward: -45\n",
      "Episode 7360, Total Reward: -44\n",
      "Episode 7380, Total Reward: -44\n",
      "Episode 7400, Total Reward: -41\n",
      "Episode 7420, Total Reward: -41\n",
      "Episode 7440, Total Reward: -38\n",
      "Episode 7460, Total Reward: -45\n",
      "Episode 7480, Total Reward: -40\n",
      "Episode 7500, Total Reward: -46\n",
      "Episode 7520, Total Reward: -40\n",
      "Episode 7540, Total Reward: -50\n",
      "Episode 7560, Total Reward: -44\n",
      "Episode 7580, Total Reward: -46\n",
      "Episode 7600, Total Reward: -44\n",
      "Episode 7620, Total Reward: -40\n",
      "Episode 7640, Total Reward: -42\n",
      "Episode 7660, Total Reward: -54\n",
      "Episode 7680, Total Reward: -46\n",
      "Episode 7700, Total Reward: -40\n",
      "Episode 7720, Total Reward: -45\n",
      "Episode 7740, Total Reward: -48\n",
      "Episode 7760, Total Reward: -35\n",
      "Episode 7780, Total Reward: -42\n",
      "Episode 7800, Total Reward: -44\n",
      "Episode 7820, Total Reward: -41\n",
      "Episode 7840, Total Reward: -42\n",
      "Episode 7860, Total Reward: -54\n",
      "Episode 7880, Total Reward: -51\n",
      "Episode 7900, Total Reward: -47\n",
      "Episode 7920, Total Reward: -39\n",
      "Episode 7940, Total Reward: -39\n",
      "Episode 7960, Total Reward: -44\n",
      "Episode 7980, Total Reward: -38\n",
      "Episode 8000, Total Reward: -51\n",
      "Episode 8020, Total Reward: -45\n",
      "Episode 8040, Total Reward: -38\n",
      "Episode 8060, Total Reward: -43\n",
      "Episode 8080, Total Reward: -39\n",
      "Episode 8100, Total Reward: -40\n",
      "Episode 8120, Total Reward: -44\n",
      "Episode 8140, Total Reward: -44\n",
      "Episode 8160, Total Reward: -46\n",
      "Episode 8180, Total Reward: -38\n",
      "Episode 8200, Total Reward: -46\n",
      "Episode 8220, Total Reward: -46\n",
      "Episode 8240, Total Reward: -42\n",
      "Episode 8260, Total Reward: -41\n",
      "Episode 8280, Total Reward: -50\n",
      "Episode 8300, Total Reward: -38\n",
      "Episode 8320, Total Reward: -48\n",
      "Episode 8340, Total Reward: -45\n",
      "Episode 8360, Total Reward: -38\n",
      "Episode 8380, Total Reward: -46\n",
      "Episode 8400, Total Reward: -36\n",
      "Episode 8420, Total Reward: -56\n",
      "Episode 8440, Total Reward: -49\n",
      "Episode 8460, Total Reward: -46\n",
      "Episode 8480, Total Reward: -34\n",
      "Episode 8500, Total Reward: -44\n",
      "Episode 8520, Total Reward: -46\n",
      "Episode 8540, Total Reward: -44\n",
      "Episode 8560, Total Reward: -48\n",
      "Episode 8580, Total Reward: -42\n",
      "Episode 8600, Total Reward: -38\n",
      "Episode 8620, Total Reward: -51\n",
      "Episode 8640, Total Reward: -41\n",
      "Episode 8660, Total Reward: -44\n",
      "Episode 8680, Total Reward: -44\n",
      "Episode 8700, Total Reward: -47\n",
      "Episode 8720, Total Reward: -42\n",
      "Episode 8740, Total Reward: -42\n",
      "Episode 8760, Total Reward: -42\n",
      "Episode 8780, Total Reward: -51\n",
      "Episode 8800, Total Reward: -43\n",
      "Episode 8820, Total Reward: -34\n",
      "Episode 8840, Total Reward: -40\n",
      "Episode 8860, Total Reward: -43\n",
      "Episode 8880, Total Reward: -36\n",
      "Episode 8900, Total Reward: -39\n",
      "Episode 8920, Total Reward: -44\n",
      "Episode 8940, Total Reward: -42\n",
      "Episode 8960, Total Reward: -39\n",
      "Episode 8980, Total Reward: -39\n",
      "Episode 9000, Total Reward: -35\n",
      "Episode 9020, Total Reward: -39\n",
      "Episode 9040, Total Reward: -44\n",
      "Episode 9060, Total Reward: -41\n",
      "Episode 9080, Total Reward: -44\n",
      "Episode 9100, Total Reward: -45\n",
      "Episode 9120, Total Reward: -42\n",
      "Episode 9140, Total Reward: -41\n",
      "Episode 9160, Total Reward: -40\n",
      "Episode 9180, Total Reward: -38\n",
      "Episode 9200, Total Reward: -41\n",
      "Episode 9220, Total Reward: -36\n",
      "Episode 9240, Total Reward: -47\n",
      "Episode 9260, Total Reward: -37\n",
      "Episode 9280, Total Reward: -44\n",
      "Episode 9300, Total Reward: -41\n",
      "Episode 9320, Total Reward: -42\n",
      "Episode 9340, Total Reward: -42\n",
      "Episode 9360, Total Reward: -42\n",
      "Episode 9380, Total Reward: -39\n",
      "Episode 9400, Total Reward: -41\n",
      "Episode 9420, Total Reward: -41\n",
      "Episode 9440, Total Reward: -36\n",
      "Episode 9460, Total Reward: -42\n",
      "Episode 9480, Total Reward: -40\n",
      "Episode 9500, Total Reward: -37\n",
      "Episode 9520, Total Reward: -42\n",
      "Episode 9540, Total Reward: -46\n",
      "Episode 9560, Total Reward: -44\n",
      "Episode 9580, Total Reward: -44\n",
      "Episode 9600, Total Reward: -36\n",
      "Episode 9620, Total Reward: -43\n",
      "Episode 9640, Total Reward: -36\n",
      "Episode 9660, Total Reward: -42\n",
      "Episode 9680, Total Reward: -41\n",
      "Episode 9700, Total Reward: -47\n",
      "Episode 9720, Total Reward: -43\n",
      "Episode 9740, Total Reward: -46\n",
      "Episode 9760, Total Reward: -40\n",
      "Episode 9780, Total Reward: -40\n",
      "Episode 9800, Total Reward: -38\n",
      "Episode 9820, Total Reward: -47\n",
      "Episode 9840, Total Reward: -40\n",
      "Episode 9860, Total Reward: -52\n",
      "Episode 9880, Total Reward: -40\n",
      "Episode 9900, Total Reward: -38\n",
      "Episode 9920, Total Reward: -37\n",
      "Episode 9940, Total Reward: -42\n",
      "Episode 9960, Total Reward: -43\n",
      "Episode 9980, Total Reward: -42\n"
     ]
    }
   ],
   "source": [
    "## simulare the agent in the environment\n",
    "num_actions = len(env.get_wrapper_attr(\"actions\"))\n",
    "agent = tabularQlearningET(num_feature= num_features, num_actions=num_actions)\n",
    "\n",
    "num_episodes = 10000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, info = env.reset()\n",
    "    state = rep.get_features(obs)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    agent.erase_eligibility()\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_obs, reward, done, trunc, _ = env.step(action)\n",
    "        next_state = rep.get_features(next_obs)\n",
    "        agent.update(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    if episode % 20 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " t: 0, observation: [0.20286288 0.34125826], reward: -1\n",
      " t: 1, observation: [0.24526551 0.33682478], reward: -1\n",
      " t: 2, observation: [0.24115111 0.28597662], reward: -1\n",
      " t: 3, observation: [0.24857013 0.24874332], reward: -1\n",
      " t: 4, observation: [0.24405617 0.20384785], reward: -1\n",
      " t: 5, observation: [0.249758   0.25979119], reward: -1\n",
      " t: 6, observation: [0.24375847 0.22434944], reward: -1\n",
      " t: 7, observation: [0.2451925  0.17720605], reward: -1\n",
      " t: 8, observation: [0.30637279 0.15724924], reward: -1\n",
      " t: 9, observation: [0.36446822 0.15213001], reward: -1\n",
      " t: 10, observation: [0.40841844 0.16233217], reward: -1\n",
      " t: 11, observation: [0.35921752 0.16504246], reward: -1\n",
      " t: 12, observation: [0.40835509 0.16760173], reward: -1\n",
      " t: 13, observation: [0.4116669  0.21085044], reward: -1\n",
      " t: 14, observation: [0.36832805 0.20767923], reward: -1\n",
      " t: 15, observation: [0.42826317 0.21118161], reward: -1\n",
      " t: 16, observation: [0.35851445 0.21728525], reward: -1\n",
      " t: 17, observation: [0.37766765 0.17760496], reward: -1\n",
      " t: 18, observation: [0.37787555 0.13504132], reward: -1\n",
      " t: 19, observation: [0.43966358 0.10881321], reward: -1\n",
      " t: 20, observation: [0.48580822 0.09432792], reward: -1\n",
      " t: 21, observation: [0.53859015 0.09404887], reward: -1\n",
      " t: 22, observation: [0.57835759 0.09875469], reward: -1\n",
      " t: 23, observation: [0.63283517 0.11345902], reward: -1\n",
      " t: 24, observation: [0.66091014 0.11811218], reward: -1\n",
      " t: 25, observation: [0.65828617 0.17411258], reward: -1\n",
      " t: 26, observation: [0.65591395 0.21740353], reward: -1\n",
      " t: 27, observation: [0.70194883 0.21420572], reward: -1\n",
      " t: 28, observation: [0.74049767 0.23909054], reward: -1\n",
      " t: 29, observation: [0.74721743 0.30543633], reward: -1\n",
      " t: 30, observation: [0.76035116 0.36097119], reward: -1\n",
      " t: 31, observation: [0.75859484 0.42334388], reward: -1\n",
      " t: 32, observation: [0.81411915 0.43265506], reward: -1\n",
      " t: 33, observation: [0.84091451 0.47978779], reward: -1\n",
      " t: 34, observation: [0.82687768 0.53211216], reward: -1\n",
      " t: 35, observation: [0.836673   0.57162208], reward: -1\n",
      " t: 36, observation: [0.88911467 0.57174305], reward: -1\n",
      " t: 37, observation: [0.93574554 0.56849829], reward: -1\n",
      " t: 38, observation: [0.94813661 0.61867952], reward: -1\n",
      " t: 39, observation: [0.95342872 0.68036213], reward: -1\n",
      " t: 40, observation: [0.95587752 0.73690734], reward: -1\n",
      " t: 41, observation: [0.96180424 0.80228738], reward: -1\n",
      " t: 42, observation: [0.9578358  0.84043761], reward: -1\n",
      " t: 43, observation: [0.94948697 0.88334451], reward: -1\n",
      " t: 44, observation: [0.95757305 0.94184951], reward: -1\n",
      " t: 45, observation: [1.         0.93337659], reward: 0\n",
      "total reward in this episode: -45\n",
      "episode 0: reward: -45\n"
     ]
    }
   ],
   "source": [
    "obs, info = env.reset()\n",
    "total_reward = 0\n",
    "episode_rewards = []\n",
    "frames = []\n",
    "observation = obs\n",
    "\n",
    "max_video_length = 120\n",
    "\n",
    "\n",
    "for time_step in range(max_video_length):\n",
    "    frames.append(env.render())\n",
    "\n",
    "    #action = greedy_policy(get_features(observation))\n",
    "    action = agent.choose_action(rep.get_features(observation))\n",
    "    observation, reward, done, trunc, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    image = env.render()\n",
    "    #online_rendering(image) #uncomment this line to see the online rendering of the environment frame by frame\n",
    "    frames.append(image)\n",
    "\n",
    "    print(f\" t: {time_step}, observation: {observation}, reward: {reward}\") #uncomment this line to see the environment-agent interaction details\n",
    "\n",
    "    if done:\n",
    "      print(f\"total reward in this episode: {total_reward}\")\n",
    "      episode_rewards.append(total_reward)\n",
    "      total_reward = 0\n",
    "      break\n",
    "\n",
    "env.close()\n",
    "\n",
    "if episode_rewards == []:\n",
    "  print(\"no episode finished in this run.\")\n",
    "else:\n",
    "  for i, reward in enumerate(episode_rewards):\n",
    "    print(f\"episode {i}: reward: {reward}\")\n",
    "\n",
    "visualize(frames, \"./Video/q_learning_Kan_ET.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
