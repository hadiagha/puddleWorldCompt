{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_puddle\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.dqn import MlpPolicy as DQNPolicy\n",
    "from stable_baselines3.ppo import MlpPolicy as PPOPolicy\n",
    "\n",
    "import time\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython import display\n",
    "import pyvirtualdisplay\n",
    "import cv2\n",
    "\n",
    "import libs.tiles3 as tc\n",
    "import random\n",
    "from util.kanerva import BaseKanervaCoder\n",
    "\n",
    "selected_seed = 0\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#some functions to help the visualization and interaction wit the environment\n",
    "\n",
    "def visualize(frames, video_name = \"/Video/video.mp4\"):\n",
    "    # Saves the frames as an mp4 video using cv2\n",
    "    video_path = video_name\n",
    "    height, width, _ = frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    video_writer = cv2.VideoWriter(video_path, fourcc, 30, (width, height))\n",
    "    for frame in frames:\n",
    "        video_writer.write(frame)\n",
    "    video_writer.release()\n",
    "\n",
    "def online_rendering(image):\n",
    "    #Visualize one frame of the image in a display\n",
    "    ax.axis('off')\n",
    "    img_with_frame = np.zeros((image.shape[0]+2, image.shape[1]+2, 3), dtype=np.uint8)\n",
    "    img_with_frame[1:-1, 1:-1, :] = image\n",
    "    ax.imshow(img_with_frame)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "\n",
    "def prepare_display():\n",
    "  #Prepares display for onine rendering of the frames in the game\n",
    "  _display = pyvirtualdisplay.Display(visible=False,size=(1400, 900))\n",
    "  _ = _display.start()\n",
    "  fig, ax = plt.subplots(figsize=(5, 5))\n",
    "  ax.axis('off')\n",
    "\n",
    "\n",
    "def get_action():\n",
    "    action = None\n",
    "    while action not in [\"w\", \"a\", \"s\", \"d\", \"W\", \"A\", \"S\", \"D\"]:\n",
    "        action = input(\"Enter action (w/a/s/d): \")\n",
    "    if action == \"w\":\n",
    "        return 3\n",
    "    elif action == \"a\":\n",
    "        return 0\n",
    "    elif action == \"s\":\n",
    "        return 2\n",
    "    elif action == \"d\":\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tabularQlearning:\n",
    "    def __init__(self, num_feature, num_actions, alpha=0.1, gamma=0.9, epsilon=0.05, seed = selected_seed):\n",
    "        self.num_feature = num_feature\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Initialize Q-table with zeros\n",
    "        self.q_table = np.zeros((self.num_feature, num_actions))\n",
    "\n",
    "        self.seed = seed    \n",
    "        # Set random seed\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Choose an action using epsilon-greedy policy\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            q = self.q_table[state].sum(axis=0)\n",
    "            return q.argmax()\n",
    "    \n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"Update the Q-table using the Q-learning update rule\"\"\"\n",
    "        self.q_table[state, action] += self.alpha * (reward + self.gamma * np.max(self.q_table[next_state]) - self.q_table[state, action])\n",
    "    \n",
    "    def get_q_table(self):\n",
    "        return self.q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scenario_episode_rewards = {}\n",
    "num_scenarios = 5\n",
    "num_features = 1500\n",
    "n_closest = 8\n",
    "num_episodes = 15000\n",
    "\n",
    "number_test = 100\n",
    "\n",
    "max_video_length = 70\n",
    "\n",
    "def e_greedy_policy(state, epsilon=0.01):\n",
    "    q_table = agent.get_q_table()\n",
    "    q = q_table[state].sum(axis=0)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(num_actions)\n",
    "    else:\n",
    "        return q.argmax()\n",
    "\n",
    "for gms in range(1, num_scenarios+1):\n",
    "\n",
    "    np.random.seed(gms)\n",
    "    json_file = '/Users/hadiaghazadeh/Library/CloudStorage/OneDrive-UniversityofCalgary/@upperboundCompetition/gym-puddle/gym_puddle/env_configs/pw{}.json'.format(gms)\n",
    "\n",
    "    with open(json_file) as f:\n",
    "        env_setup = json.load(f)\n",
    "\n",
    "\n",
    "    env = gym.make(\n",
    "    \"PuddleWorld-v0\",\n",
    "    start=env_setup[\"start\"],\n",
    "    goal=env_setup[\"goal\"],\n",
    "    goal_threshold=env_setup[\"goal_threshold\"],\n",
    "    noise=env_setup[\"noise\"],\n",
    "    thrust=env_setup[\"thrust\"],\n",
    "    puddle_top_left=env_setup[\"puddle_top_left\"],\n",
    "    puddle_width=env_setup[\"puddle_width\"],\n",
    "    )\n",
    "    \n",
    "    rep = BaseKanervaCoder(env.observation_space, n_prototypes= num_features, n_closest= n_closest, random_seed= gms)\n",
    "\n",
    "\n",
    "    ## simulare the agent in the environment\n",
    "    num_actions = len(env.get_wrapper_attr(\"actions\"))\n",
    "    agent = tabularQlearning(num_feature= num_features, num_actions=num_actions,alpha=0.1, gamma=0.9, epsilon=0.05, seed = gms)\n",
    "\n",
    "\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        state = rep.get_features(obs)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_obs, reward, done, trunc, _ = env.step(action)\n",
    "            next_state = rep.get_features(next_obs)\n",
    "            agent.update(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "    \n",
    "    #Test the trained model\n",
    "\n",
    "    scenario_episode_rewards[gms] = [0]*number_test\n",
    "\n",
    "    counter  = 0\n",
    "    while counter < number_test:\n",
    "\n",
    "        np.random.seed(counter)\n",
    "\n",
    "        obs, info = env.reset()\n",
    "        total_reward = 0\n",
    "        episode_rewards = []\n",
    "        frames = []\n",
    "        observation = obs\n",
    "\n",
    "        for time_step in range(max_video_length):\n",
    "            \n",
    "            frames.append(env.render())\n",
    "\n",
    "            action = e_greedy_policy(rep.get_features(observation))\n",
    "            \n",
    "            #action = agent.choose_action(rep.get_features(observation))\n",
    "            observation, reward, done, trunc, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            image = env.render()\n",
    "            #online_rendering(image) #uncomment this line to see the online rendering of the environment frame by frame\n",
    "            frames.append(image)\n",
    "\n",
    "            if done:\n",
    "                episode_rewards.append(total_reward)\n",
    "                total_reward = 0\n",
    "                break\n",
    "\n",
    "        env.close()\n",
    "\n",
    "        if episode_rewards:\n",
    "            \n",
    "            scenario_episode_rewards[gms][counter] = float(round(episode_rewards[0],2))\n",
    "            counter += 1\n",
    "    \n",
    "    print(f\"Scenario {gms} is done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning with Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    " \n",
    "scenario_episode_rewards = {}\n",
    "num_scenarios = 5\n",
    "num_features = 1500\n",
    "n_closest = 8\n",
    "num_episodes = 15000\n",
    "\n",
    "number_test = 100\n",
    "\n",
    "number_test_param = 5000\n",
    "max_video_length = 70\n",
    "# Function to perform e-greedy policy\n",
    "def e_greedy_policy(state, epsilon=0.01):\n",
    "    q_table = agent.get_q_table()\n",
    "    q = q_table[state].sum(axis=0)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(num_actions)\n",
    "    else:\n",
    "        return q.argmax()\n",
    "\n",
    "# Define the objective function for Bayesian optimization\n",
    "@use_named_args([\n",
    "    Real(0.01, 0.2, name='alpha'),\n",
    "    Real(0.8, 0.99, name='gamma'),\n",
    "    Real(0.01, 0.2, name='epsilon'),\n",
    "])\n",
    "def objective(alpha, gamma, epsilon):\n",
    "    total_rewards = []\n",
    "    \n",
    "    agent = tabularQlearning(num_feature=num_features, num_actions=num_actions, alpha=alpha, gamma=gamma, epsilon=epsilon, seed=gms)\n",
    "    episode_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        state = rep.get_features(obs)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_obs, reward, done, _, _ = env.step(action)\n",
    "            next_state = rep.get_features(next_obs)\n",
    "            agent.update(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        episode_rewards.append(total_reward)\n",
    "    total_rewards.append(np.mean(episode_rewards))\n",
    "    \n",
    "\n",
    "    # test the model for parameter tuning\n",
    "    obs, info = env.reset()\n",
    "    total_reward = 0\n",
    "    episode_rewards = []\n",
    "    frames = []\n",
    "    observation = obs\n",
    "\n",
    "    for time_step in range(max_video_length):\n",
    "        \n",
    "        frames.append(env.render())\n",
    "\n",
    "        action = e_greedy_policy(rep.get_features(observation))\n",
    "        \n",
    "        #action = best_agent.choose_action(rep.get_features(observation))\n",
    "        observation, reward, done, trunc, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        image = env.render()\n",
    "        #online_rendering(image) #uncomment this line to see the online rendering of the environment frame by frame\n",
    "        frames.append(image)\n",
    "\n",
    "        if done:\n",
    "            episode_rewards.append(total_reward)\n",
    "            total_reward = 0\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return -float(episode_rewards[0])  # We use negative mean rewards as we want to maximize rewards\n",
    "\n",
    "# Perform Bayesian optimization for each scenario\n",
    "for gms in range(1, num_scenarios+1):\n",
    "\n",
    "    np.random.seed(gms)\n",
    "    json_file = '/Users/hadiaghazadeh/Library/CloudStorage/OneDrive-UniversityofCalgary/@upperboundCompetition/gym-puddle/gym_puddle/env_configs/pw{}.json'.format(gms)\n",
    "\n",
    "    with open(json_file) as f:\n",
    "        env_setup = json.load(f)\n",
    "\n",
    "    env = gym.make(\n",
    "        \"PuddleWorld-v0\",\n",
    "        start=env_setup[\"start\"],\n",
    "        goal=env_setup[\"goal\"],\n",
    "        goal_threshold=env_setup[\"goal_threshold\"],\n",
    "        noise=env_setup[\"noise\"],\n",
    "        thrust=env_setup[\"thrust\"],\n",
    "        puddle_top_left=env_setup[\"puddle_top_left\"],\n",
    "        puddle_width=env_setup[\"puddle_width\"],\n",
    "    )\n",
    "\n",
    "    rep = BaseKanervaCoder(env.observation_space, n_prototypes=num_features, n_closest=n_closest, random_seed=gms)\n",
    "\n",
    "    num_actions = len(env.get_wrapper_attr(\"actions\"))\n",
    "\n",
    "    # Perform Bayesian optimization for this scenario\n",
    "    res = gp_minimize(objective,                  # the function to minimize\n",
    "                      [(0.01, 0.2),              # the bounds on each dimension of x\n",
    "                       (0.8, 0.99),\n",
    "                       (0.01, 0.2)],\n",
    "                      acq_func=\"EI\",            # the acquisition function\n",
    "                      n_calls=10,                # the number of evaluations of f\n",
    "                      random_state=gms)          # the random seed\n",
    "\n",
    "    best_alpha, best_gamma, best_epsilon = res.x\n",
    "    print(f\"Best parameters for Scenario {gms}: alpha={best_alpha}, gamma={best_gamma}, epsilon={best_epsilon}\")\n",
    "\n",
    "    # Test the trained model with the best parameters\n",
    "    best_agent = tabularQlearning(num_feature=num_features, num_actions=num_actions, alpha=best_alpha, gamma=best_gamma, epsilon=best_epsilon, seed=gms)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        state = rep.get_features(obs)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = best_agent.choose_action(state)\n",
    "            next_obs, reward, done, trunc, _ = env.step(action)\n",
    "            next_state = rep.get_features(next_obs)\n",
    "            agent.update(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "    \n",
    "    #Test the trained model\n",
    "\n",
    "    scenario_episode_rewards[gms] = [0]*number_test\n",
    "\n",
    "    counter  = 0\n",
    "    while counter < number_test:\n",
    "\n",
    "        np.random.seed(counter)\n",
    "\n",
    "        obs, info = env.reset()\n",
    "        total_reward = 0\n",
    "        episode_rewards = []\n",
    "        frames = []\n",
    "        observation = obs\n",
    "\n",
    "        for time_step in range(max_video_length):\n",
    "            \n",
    "            frames.append(env.render())\n",
    "\n",
    "            #action = e_greedy_policy(rep.get_features(observation))\n",
    "            \n",
    "            action = best_agent.choose_action(rep.get_features(observation))\n",
    "            observation, reward, done, trunc, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            image = env.render()\n",
    "            #online_rendering(image) #uncomment this line to see the online rendering of the environment frame by frame\n",
    "            frames.append(image)\n",
    "\n",
    "            if done:\n",
    "                episode_rewards.append(total_reward)\n",
    "                total_reward = 0\n",
    "                break\n",
    "\n",
    "        env.close()\n",
    "\n",
    "        if episode_rewards:\n",
    "            scenario_episode_rewards[gms][counter] = float(round(episode_rewards[0],3))\n",
    "            counter += 1\n",
    "    \n",
    "    print(f\"Scenario {gms} is done\")\n",
    "\n",
    "print(\"Parameter tuning and testing for all scenarios are done.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the column names\n",
    "columns = ['seed_ID', 'ep_reward_pw1', 'ep_reward_pw2', 'ep_reward_pw3', 'ep_reward_pw4', 'ep_reward_pw5']\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Fill the DataFrame with the scenario_episode_rewards\n",
    "for i in range(number_test):\n",
    "    df.loc[i] = [i+1] + [scenario_episode_rewards[j][i] for j in range(1, num_scenarios+1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall mean and std\n",
    "mean = df.mean()\n",
    "std = df.std()\n",
    "\n",
    "np.mean(mean[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the file name for saving the results\n",
    "csv_file_name = \"submission2.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(csv_file_name, index=False)\n",
    "\n",
    "print(\"Results saved successfully to\", csv_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
