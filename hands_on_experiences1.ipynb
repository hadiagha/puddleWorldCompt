{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_puddle\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.dqn import MlpPolicy as DQNPolicy\n",
    "from stable_baselines3.ppo import MlpPolicy as PPOPolicy\n",
    "\n",
    "import time\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython import display\n",
    "import pyvirtualdisplay\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puddle World Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start position: [0.2 0.4]\n",
      "goal position: [1. 1.]\n",
      "goal threshold: 0.1\n",
      "action noise: 0.01\n",
      "agent's thrust: 0.05\n",
      "puddle top left positions: [array([0.  , 0.85]), array([0.35, 0.9 ])]\n",
      "puddle widths and heights: [array([0.55, 0.2 ]), array([0.2, 0.6])]\n",
      "action space: [array([-0.05,  0.  ]), array([0.05, 0.  ]), array([ 0.  , -0.05]), array([0.  , 0.05])]\n",
      "observation space: Box(0.0, 1.0, (2,), float64)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"PuddleWorld-v0\")\n",
    "\n",
    "print(\"start position:\", env.get_wrapper_attr(\"start\"))\n",
    "print(\"goal position:\", env.get_wrapper_attr(\"goal\"))\n",
    "print(\"goal threshold:\", env.get_wrapper_attr(\"goal_threshold\"))\n",
    "print(\"action noise:\", env.get_wrapper_attr(\"noise\"))\n",
    "print(\"agent's thrust:\", env.get_wrapper_attr(\"thrust\"))\n",
    "print(\"puddle top left positions:\", env.get_wrapper_attr(\"puddle_top_left\"))\n",
    "print(\"puddle widths and heights:\", env.get_wrapper_attr(\"puddle_width\"))\n",
    "print(\"action space:\", env.get_wrapper_attr(\"actions\"))\n",
    "print(\"observation space:\", env.get_wrapper_attr(\"observation_space\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#some functions to help the visualization and interaction wit the environment\n",
    "\n",
    "def visualize(frames, video_name = \"/results/video.mp4\"):\n",
    "    # Saves the frames as an mp4 video using cv2\n",
    "    video_path = video_name\n",
    "    height, width, _ = frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    video_writer = cv2.VideoWriter(video_path, fourcc, 30, (width, height))\n",
    "    for frame in frames:\n",
    "        video_writer.write(frame)\n",
    "    video_writer.release()\n",
    "\n",
    "def online_rendering(image):\n",
    "    #Visualize one frame of the image in a display\n",
    "    ax.axis('off')\n",
    "    img_with_frame = np.zeros((image.shape[0]+2, image.shape[1]+2, 3), dtype=np.uint8)\n",
    "    img_with_frame[1:-1, 1:-1, :] = image\n",
    "    ax.imshow(img_with_frame)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "\n",
    "def prepare_display():\n",
    "  #Prepares display for onine rendering of the frames in the game\n",
    "  _display = pyvirtualdisplay.Display(visible=False,size=(1400, 900))\n",
    "  _ = _display.start()\n",
    "  fig, ax = plt.subplots(figsize=(5, 5))\n",
    "  ax.axis('off')\n",
    "\n",
    "\n",
    "def get_action():\n",
    "    action = None\n",
    "    while action not in [\"w\", \"a\", \"s\", \"d\", \"W\", \"A\", \"S\", \"D\"]:\n",
    "        action = input(\"Enter action (w/a/s/d): \")\n",
    "    if action == \"w\":\n",
    "        return 3\n",
    "    elif action == \"a\":\n",
    "        return 0\n",
    "    elif action == \"s\":\n",
    "        return 2\n",
    "    elif action == \"d\":\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs, info = env.reset()\n",
    "image = env.render()\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "online_rendering(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Accessing Different Environment Configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "json_file = f\"/Users/hadiaghazadeh/Library/CloudStorage/OneDrive-UniversityofCalgary/@upperboundCompetition/gym-puddle/gym_puddle/env_configs/pw3.json\"\n",
    "\n",
    "with open(json_file) as f:\n",
    "  env_setup = json.load(f)\n",
    "\n",
    "env = gym.make(\n",
    "  \"PuddleWorld-v0\",\n",
    "  start=env_setup[\"start\"],\n",
    "  goal=env_setup[\"goal\"],\n",
    "  goal_threshold=env_setup[\"goal_threshold\"],\n",
    "  noise=env_setup[\"noise\"],\n",
    "  thrust=env_setup[\"thrust\"],\n",
    "  puddle_top_left=env_setup[\"puddle_top_left\"],\n",
    "  puddle_width=env_setup[\"puddle_width\"],\n",
    ")\n",
    "\n",
    "obs, info = env.reset()\n",
    "image = env.render()\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "online_rendering(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the Agents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 8640 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.29e+03     |\n",
      "|    ep_rew_mean          | -4.66e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5310         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 0            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012785005 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | -8.23e-06    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.15e+05     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00182     |\n",
      "|    value_loss           | 3.81e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.29e+03      |\n",
      "|    ep_rew_mean          | -4.66e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 4954          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 1             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012312178 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0.000489      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.73e+05      |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.000328     |\n",
      "|    value_loss           | 6.43e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.29e+03     |\n",
      "|    ep_rew_mean          | -4.66e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4807         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 1            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032658684 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.000289     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.74e+05     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00361     |\n",
      "|    value_loss           | 4.03e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.29e+03     |\n",
      "|    ep_rew_mean          | -4.66e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4726         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025274174 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0.000237     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.39e+05     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00222     |\n",
      "|    value_loss           | 3.7e+05      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4675        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011247073 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.000335    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.47e+03    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00742    |\n",
      "|    value_loss           | 1.3e+04     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.29e+03     |\n",
      "|    ep_rew_mean          | -4.66e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4655         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038226293 |\n",
      "|    clip_fraction        | 0.00923      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.31        |\n",
      "|    explained_variance   | -0.00443     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.92         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.000329    |\n",
      "|    value_loss           | 50.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.29e+03     |\n",
      "|    ep_rew_mean          | -4.66e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4627         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0138257975 |\n",
      "|    clip_fraction        | 0.071        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.31        |\n",
      "|    explained_variance   | -0.00122     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.55         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00365     |\n",
      "|    value_loss           | 37           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.29e+03     |\n",
      "|    ep_rew_mean          | -4.66e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4611         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0133797545 |\n",
      "|    clip_fraction        | 0.0803       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | -0.00189     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.52         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00348     |\n",
      "|    value_loss           | 26           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4601        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009268317 |\n",
      "|    clip_fraction        | 0.0568      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 3.33e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.63        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00543    |\n",
      "|    value_loss           | 902         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4602        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011421343 |\n",
      "|    clip_fraction        | 0.0485      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.000951    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.182       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.000821   |\n",
      "|    value_loss           | 10.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.29e+03     |\n",
      "|    ep_rew_mean          | -4.66e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4581         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063952506 |\n",
      "|    clip_fraction        | 0.0163       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | 0.000986     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00678      |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.000219    |\n",
      "|    value_loss           | 6.71         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.29e+03     |\n",
      "|    ep_rew_mean          | -4.66e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4563         |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020213681 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 1.31e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 95.8         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00213     |\n",
      "|    value_loss           | 1.22e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4532        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010771762 |\n",
      "|    clip_fraction        | 0.0853      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | -0.00126    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0127      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00272    |\n",
      "|    value_loss           | 1.77        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2.29e+03   |\n",
      "|    ep_rew_mean          | -4.66e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4534       |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 30720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00351515 |\n",
      "|    clip_fraction        | 0.0125     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.29      |\n",
      "|    explained_variance   | -0.00576   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0182     |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.000497  |\n",
      "|    value_loss           | 1.09       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4529        |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016980294 |\n",
      "|    clip_fraction        | 0.089       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | -0.000187   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0177     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00337    |\n",
      "|    value_loss           | 0.613       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4527        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012526939 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.0102      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0548     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00866    |\n",
      "|    value_loss           | 0.35        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4532        |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020796752 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | -0.361      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.016      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 0.203       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4533        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012423618 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | -0.0262     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0117     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00984    |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4531        |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008481732 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.00142     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0421     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00705    |\n",
      "|    value_loss           | 0.0741      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4528        |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014012607 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | -0.00153    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00605     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00519    |\n",
      "|    value_loss           | 0.0427      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4521        |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014588027 |\n",
      "|    clip_fraction        | 0.0593      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | -0.00602    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0284     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00218    |\n",
      "|    value_loss           | 0.0255      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2.29e+03   |\n",
      "|    ep_rew_mean          | -4.66e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4513       |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 47104      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00725774 |\n",
      "|    clip_fraction        | 0.0708     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1         |\n",
      "|    explained_variance   | 0.00537    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00995    |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.00386   |\n",
      "|    value_loss           | 0.0156     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2.29e+03   |\n",
      "|    ep_rew_mean          | -4.66e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4511       |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00905048 |\n",
      "|    clip_fraction        | 0.0638     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.988     |\n",
      "|    explained_variance   | -0.00256   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0123     |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.00456   |\n",
      "|    value_loss           | 0.0097     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4509        |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016787194 |\n",
      "|    clip_fraction        | 0.0667      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.938      |\n",
      "|    explained_variance   | -0.0175     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0206     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00224    |\n",
      "|    value_loss           | 0.00582     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4512        |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011437288 |\n",
      "|    clip_fraction        | 0.0259      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.877      |\n",
      "|    explained_variance   | -0.00487    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0448      |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0011     |\n",
      "|    value_loss           | 0.00366     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.29e+03     |\n",
      "|    ep_rew_mean          | -4.66e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4510         |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058050808 |\n",
      "|    clip_fraction        | 0.0238       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.859       |\n",
      "|    explained_variance   | -0.000692    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000739     |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00204     |\n",
      "|    value_loss           | 0.00237      |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2.29e+03   |\n",
      "|    ep_rew_mean          | -4.66e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4510       |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 57344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01059791 |\n",
      "|    clip_fraction        | 0.0514     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.807     |\n",
      "|    explained_variance   | -0.0157    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0108     |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.00142   |\n",
      "|    value_loss           | 0.00155    |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.29e+03     |\n",
      "|    ep_rew_mean          | -4.66e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4512         |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038888296 |\n",
      "|    clip_fraction        | 0.0107       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.779       |\n",
      "|    explained_variance   | -0.018       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00366      |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.000999    |\n",
      "|    value_loss           | 0.000979     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.29e+03     |\n",
      "|    ep_rew_mean          | -4.66e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4513         |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060069696 |\n",
      "|    clip_fraction        | 0.0271       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.767       |\n",
      "|    explained_variance   | -0.00262     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000982     |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00199     |\n",
      "|    value_loss           | 0.000661     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4511        |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008799126 |\n",
      "|    clip_fraction        | 0.00635     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.742      |\n",
      "|    explained_variance   | -0.0168     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00744    |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.000363   |\n",
      "|    value_loss           | 0.000453    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.29e+03      |\n",
      "|    ep_rew_mean          | -4.66e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 4512          |\n",
      "|    iterations           | 32            |\n",
      "|    time_elapsed         | 14            |\n",
      "|    total_timesteps      | 65536         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00095767877 |\n",
      "|    clip_fraction        | 0.00669       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.719        |\n",
      "|    explained_variance   | -0.0275       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.00128       |\n",
      "|    n_updates            | 310           |\n",
      "|    policy_gradient_loss | -9.29e-05     |\n",
      "|    value_loss           | 0.000293      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4512        |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004632541 |\n",
      "|    clip_fraction        | 0.0339      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.728      |\n",
      "|    explained_variance   | -0.0102     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0278     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00247    |\n",
      "|    value_loss           | 0.000204    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4511        |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006720109 |\n",
      "|    clip_fraction        | 0.0428      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.678      |\n",
      "|    explained_variance   | -0.0361     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00249    |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00164    |\n",
      "|    value_loss           | 0.000142    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4507        |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005505812 |\n",
      "|    clip_fraction        | 0.0364      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.688      |\n",
      "|    explained_variance   | -0.0525     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00314     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00341    |\n",
      "|    value_loss           | 0.000103    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4505        |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005392466 |\n",
      "|    clip_fraction        | 0.0116      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.679      |\n",
      "|    explained_variance   | -0.0324     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0114      |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00128    |\n",
      "|    value_loss           | 6.74e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4500        |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008553628 |\n",
      "|    clip_fraction        | 0.0467      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.735      |\n",
      "|    explained_variance   | -0.0499     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0332     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00415    |\n",
      "|    value_loss           | 4.74e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4496        |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013051255 |\n",
      "|    clip_fraction        | 0.0178      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.704      |\n",
      "|    explained_variance   | -0.0691     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0181      |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.000339   |\n",
      "|    value_loss           | 3.5e-05     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.29e+03     |\n",
      "|    ep_rew_mean          | -4.66e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4497         |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 17           |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068343114 |\n",
      "|    clip_fraction        | 0.0453       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.715       |\n",
      "|    explained_variance   | -0.08        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0189       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00472     |\n",
      "|    value_loss           | 2.29e-05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4497        |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008359215 |\n",
      "|    clip_fraction        | 0.0163      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.727      |\n",
      "|    explained_variance   | -0.105      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0172     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00166    |\n",
      "|    value_loss           | 1.59e-05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.29e+03     |\n",
      "|    ep_rew_mean          | -4.66e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4498         |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031737944 |\n",
      "|    clip_fraction        | 0.0199       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.708       |\n",
      "|    explained_variance   | -0.163       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00746     |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.000734    |\n",
      "|    value_loss           | 1.24e-05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4496        |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010415161 |\n",
      "|    clip_fraction        | 0.0082      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.707      |\n",
      "|    explained_variance   | -0.205      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0149     |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    value_loss           | 8.57e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.29e+03     |\n",
      "|    ep_rew_mean          | -4.66e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4498         |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0141844805 |\n",
      "|    clip_fraction        | 0.097        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.715       |\n",
      "|    explained_variance   | -0.263       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0178       |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.0077      |\n",
      "|    value_loss           | 5.66e-06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4494        |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013791683 |\n",
      "|    clip_fraction        | 0.00278     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.708      |\n",
      "|    explained_variance   | -0.448      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.021      |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.000478   |\n",
      "|    value_loss           | 3.59e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.29e+03     |\n",
      "|    ep_rew_mean          | -4.66e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4495         |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0101727005 |\n",
      "|    clip_fraction        | 0.0168       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.704       |\n",
      "|    explained_variance   | -0.359       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00192     |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    value_loss           | 3.13e-06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.29e+03      |\n",
      "|    ep_rew_mean          | -4.66e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 4493          |\n",
      "|    iterations           | 46            |\n",
      "|    time_elapsed         | 20            |\n",
      "|    total_timesteps      | 94208         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027859496 |\n",
      "|    clip_fraction        | 0.00425       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.701        |\n",
      "|    explained_variance   | -0.604        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -1.66e-05     |\n",
      "|    n_updates            | 450           |\n",
      "|    policy_gradient_loss | -0.000346     |\n",
      "|    value_loss           | 2.39e-06      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4493        |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013053589 |\n",
      "|    clip_fraction        | 0.0241      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.706      |\n",
      "|    explained_variance   | -0.737      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00115    |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00272    |\n",
      "|    value_loss           | 1.76e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4493        |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014457921 |\n",
      "|    clip_fraction        | 0.0323      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.697      |\n",
      "|    explained_variance   | -1.24       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0188      |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00527    |\n",
      "|    value_loss           | 1.13e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.29e+03    |\n",
      "|    ep_rew_mean          | -4.66e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4493        |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010339323 |\n",
      "|    clip_fraction        | 0.0063      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.702      |\n",
      "|    explained_variance   | -1.36       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.013       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -4.63e-05   |\n",
      "|    value_loss           | 9.84e-07    |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dqn_model = PPO(PPOPolicy, env, verbose=1)\n",
    "dqn_model.learn(total_timesteps=int(1e5))\n",
    "dqn_model.save(\"ppo_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no episode finished in this run.\n"
     ]
    }
   ],
   "source": [
    "ppo_model = PPO.load(\"ppo_model\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Create an empty list to store the frames\n",
    "frames = []\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(1):\n",
    "  total_reward = 0\n",
    "  done = False\n",
    "  num_steps = 0\n",
    "\n",
    "  while not done and num_steps <=1000: # to avoid infinite loops for the untuned DQN we set a truncation limit, but you should make your agent sophisticated enough to avoid infinite-step episodes\n",
    "      num_steps +=1\n",
    "      action, _states = ppo_model.predict(obs)\n",
    "      observation, reward, done, trunc, info = env.step(action)\n",
    "      total_reward += reward\n",
    "      if done == True:\n",
    "        print(\"here\")\n",
    "\n",
    "      image = env.render()\n",
    "      frames.append(image)\n",
    "\n",
    "      if done:\n",
    "        print(f\"total reward in this episode: {total_reward}\")\n",
    "        episode_rewards.append(total_reward)\n",
    "        total_reward = 0\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "if episode_rewards == []:\n",
    "  print(\"no episode finished in this run.\")\n",
    "else:\n",
    "  for i, reward in enumerate(episode_rewards):\n",
    "    print(f\"episode {i}: reward: {reward}\")\n",
    "\n",
    "visualize(frames, \"ppo.mp4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning with Tile coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20: Total Reward -10531.31961984268\n",
      "Episode 40: Total Reward -3078.436473608867\n",
      "Episode 60: Total Reward -1888.837715306261\n",
      "Episode 80: Total Reward -4131.092905098181\n",
      "Episode 100: Total Reward -4154.771033225917\n",
      "Episode 120: Total Reward -4090.0739545055976\n",
      "Episode 140: Total Reward -375.2643107378943\n",
      "Episode 160: Total Reward -41\n",
      "Episode 180: Total Reward -1997.2574867868154\n",
      "Episode 200: Total Reward -3152.2081793562215\n",
      "Episode 220: Total Reward -2694.576258076341\n",
      "Episode 240: Total Reward -3492.554895184431\n",
      "Episode 260: Total Reward -2542.114953129988\n",
      "Episode 280: Total Reward -494\n",
      "Episode 300: Total Reward -1686.6171462261657\n",
      "Episode 320: Total Reward -149\n",
      "Episode 340: Total Reward -573\n",
      "Episode 360: Total Reward -5428.566657706091\n",
      "Episode 380: Total Reward -2567.4999221982853\n",
      "Episode 400: Total Reward -77\n",
      "Episode 420: Total Reward -4261.2542544197495\n",
      "Episode 440: Total Reward -60\n",
      "Episode 460: Total Reward -4032.967623579161\n",
      "Episode 480: Total Reward -362\n",
      "Episode 500: Total Reward -1135\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Assuming necessary functions from your prior code\n",
    "def create_tiling(feat_range, bins, offset):\n",
    "    return np.linspace(feat_range[0], feat_range[1], bins + 1)[1:-1] + offset\n",
    "\n",
    "def create_tilings(feature_ranges, number_tilings, bins, offsets):\n",
    "    tilings = []\n",
    "    for tile_i in range(number_tilings):\n",
    "        tiling = [create_tiling(feature_ranges[feat_i], bins[tile_i][feat_i], offsets[tile_i][feat_i])\n",
    "                  for feat_i in range(len(feature_ranges))]\n",
    "        tilings.append(tiling)\n",
    "    return np.array(tilings)\n",
    "\n",
    "def get_tile_coding(feature, tilings):\n",
    "    num_dims = len(feature)\n",
    "    feat_codings = []\n",
    "    for tiling in tilings:\n",
    "        feat_coding = [np.digitize(feature[i], tiling[i]) for i in range(num_dims)]\n",
    "        feat_codings.append(feat_coding)\n",
    "    return np.array(feat_codings)\n",
    "\n",
    "class QValueFunction:\n",
    "    def __init__(self, tilings, num_actions, lr):\n",
    "        self.tilings = tilings\n",
    "        self.num_tilings = len(self.tilings)\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.state_sizes = [tuple(len(splits) + 1 for splits in tiling) for tiling in self.tilings]\n",
    "        self.q_tables = [np.zeros(shape=(*state_size, num_actions)) for state_size in self.state_sizes]\n",
    "\n",
    "    def value(self, state, action):\n",
    "        state_codings = get_tile_coding(state, self.tilings)\n",
    "        value = 0\n",
    "        for coding, q_table in zip(state_codings, self.q_tables):\n",
    "            value += q_table[tuple(coding) + (action,)]\n",
    "        return value / self.num_tilings\n",
    "\n",
    "    def update(self, state, action, target):\n",
    "        state_codings = get_tile_coding(state, self.tilings)\n",
    "        for coding, q_table in zip(state_codings, self.q_tables):\n",
    "            q_value = q_table[tuple(coding) + (action,)]\n",
    "            q_table[tuple(coding) + (action,)] += self.lr * (target - q_value)\n",
    "\n",
    "def simulate_episode(env, q_func, eps, gamma):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    trunc = False  # To capture truncation condition as well\n",
    "\n",
    "    while not done and not trunc:  # We check both done and truncated\n",
    "        if np.random.random() < eps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            q_values = [q_func.value(state, i) for i in range(env.action_space.n)]\n",
    "            action = np.argmax(q_values)\n",
    "\n",
    "        next_state, reward, done, trunc, _ = env.step(action)  # Update here\n",
    "        total_reward += reward\n",
    "        \n",
    "        if not done and not trunc:  # Consider next state only if not done and not truncated\n",
    "            next_q_values = [q_func.value(next_state, i) for i in range(env.action_space.n)]\n",
    "            max_next_q = max(next_q_values)\n",
    "        else:\n",
    "            max_next_q = 0\n",
    "        \n",
    "        target = reward + gamma * max_next_q\n",
    "        q_func.update(state, action, target)\n",
    "        state = next_state\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "\n",
    "tilings = create_tilings([env.observation_space.low, env.observation_space.high], 3,\n",
    "                        [[8, 8], [8, 8], [8, 8]], [[0, 0], [0.2, 0.2], [0.4, 0.4]])\n",
    "q_func = QValueFunction(tilings, env.action_space.n, 0.01)\n",
    "\n",
    "num_episodes = 500\n",
    "eps = 0.1\n",
    "gamma = 0.95\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    reward = simulate_episode(env, q_func, eps, gamma)\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"Episode {i + 1}: Total Reward {reward}\")\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Decay e-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20: Total Reward -54113.66950823726\n",
      "Episode 40: Total Reward -6874.542286666987\n",
      "Episode 60: Total Reward -20660.908445610043\n",
      "Episode 80: Total Reward -17483.36664422168\n",
      "Episode 100: Total Reward -4424.529531507773\n",
      "Episode 120: Total Reward -20686.822033674693\n",
      "Episode 140: Total Reward -6462.314569742265\n",
      "Episode 160: Total Reward -3395.824328982037\n",
      "Episode 180: Total Reward -1112.5961555804354\n",
      "Episode 200: Total Reward -501.8008869110547\n",
      "Episode 220: Total Reward -616.2570335229641\n",
      "Episode 240: Total Reward -991.531136441432\n",
      "Episode 260: Total Reward -468\n",
      "Episode 280: Total Reward -43\n",
      "Episode 300: Total Reward -877.2484846697746\n",
      "Episode 320: Total Reward -1784.746295843612\n",
      "Episode 340: Total Reward -3551.0912429206246\n",
      "Episode 360: Total Reward -8539.127848996282\n",
      "Episode 380: Total Reward -40\n",
      "Episode 400: Total Reward -12504.119234228408\n",
      "Episode 420: Total Reward -3210.5217600410215\n",
      "Episode 440: Total Reward -2069\n",
      "Episode 460: Total Reward -96546.15249980814\n",
      "Episode 480: Total Reward -17256.33856741574\n",
      "Episode 500: Total Reward -9827.158137903903\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Assuming necessary functions from your prior code\n",
    "def create_tiling(feat_range, bins, offset):\n",
    "    return np.linspace(feat_range[0], feat_range[1], bins + 1)[1:-1] + offset\n",
    "\n",
    "def create_tilings(feature_ranges, number_tilings, bins, offsets):\n",
    "    tilings = []\n",
    "    for tile_i in range(number_tilings):\n",
    "        tiling = [create_tiling(feature_ranges[feat_i], bins[tile_i][feat_i], offsets[tile_i][feat_i])\n",
    "                  for feat_i in range(len(feature_ranges))]\n",
    "        tilings.append(tiling)\n",
    "    return np.array(tilings)\n",
    "\n",
    "def get_tile_coding(feature, tilings):\n",
    "    num_dims = len(feature)\n",
    "    feat_codings = []\n",
    "    for tiling in tilings:\n",
    "        feat_coding = [np.digitize(feature[i], tiling[i]) for i in range(num_dims)]\n",
    "        feat_codings.append(feat_coding)\n",
    "    return np.array(feat_codings)\n",
    "\n",
    "class QValueFunction:\n",
    "    def __init__(self, tilings, num_actions, lr, initial_epsilon, epsilon_decay):\n",
    "        self.tilings = tilings\n",
    "        self.num_tilings = len(self.tilings)\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.state_sizes = [tuple(len(splits) + 1 for splits in tiling) for tiling in self.tilings]\n",
    "        self.q_tables = [np.zeros(shape=(*state_size, num_actions)) for state_size in self.state_sizes]\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "    def value(self, state, action):\n",
    "        state_codings = get_tile_coding(state, self.tilings)\n",
    "        value = 0\n",
    "        for coding, q_table in zip(state_codings, self.q_tables):\n",
    "            value += q_table[tuple(coding) + (action,)]\n",
    "        return value / self.num_tilings\n",
    "\n",
    "    def update(self, state, action, target):\n",
    "        state_codings = get_tile_coding(state, self.tilings)\n",
    "        for coding, q_table in zip(state_codings, self.q_tables):\n",
    "            q_value = q_table[tuple(coding) + (action,)]\n",
    "            q_table[tuple(coding) + (action,)] += self.lr * (target - q_value)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            q_values = [self.value(state, i) for i in range(self.num_actions)]\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "\n",
    "def simulate_episode(env, q_func, gamma):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    trunc = False\n",
    "\n",
    "    while not done and not trunc:\n",
    "        action = q_func.choose_action(state)\n",
    "        next_state, reward, done, trunc, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        if not done and not trunc:\n",
    "            next_q_values = [q_func.value(next_state, i) for i in range(env.action_space.n)]\n",
    "            max_next_q = max(next_q_values)\n",
    "        else:\n",
    "            max_next_q = 0\n",
    "\n",
    "        target = reward + gamma * max_next_q\n",
    "        q_func.update(state, action, target)\n",
    "        state = next_state\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "tilings = create_tilings([env.observation_space.low, env.observation_space.high], 3,\n",
    "                        [[7, 7], [7, 7], [7, 7]], [[0, 0], [0.2, 0.2], [0.4, 0.4]])\n",
    "\n",
    "# Initialization\n",
    "initial_epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "q_func = QValueFunction(tilings, env.action_space.n, 0.05, initial_epsilon, epsilon_decay)\n",
    "\n",
    "\n",
    "num_episodes = 500\n",
    "eps = 0.1\n",
    "gamma = 0.95\n",
    "\n",
    "# Training loop\n",
    "for i in range(num_episodes):\n",
    "    reward = simulate_episode(env, q_func, gamma)\n",
    "    q_func.update_epsilon()  # Update epsilon at each episode\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"Episode {i + 1}: Total Reward {reward}\")\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20: Total Reward -139\n",
      "Episode 40: Total Reward -696\n",
      "Episode 60: Total Reward -793.4190219186393\n",
      "Episode 80: Total Reward -4670.0929897196465\n",
      "Episode 100: Total Reward -8168.248191893292\n",
      "Episode 120: Total Reward -514\n",
      "Episode 140: Total Reward -607\n",
      "Episode 160: Total Reward -8958.31504811569\n",
      "Episode 180: Total Reward -1907.4683787985487\n",
      "Episode 200: Total Reward -1931\n",
      "Episode 220: Total Reward -164\n",
      "Episode 240: Total Reward -121\n",
      "Episode 260: Total Reward -6452.553842218335\n",
      "Episode 280: Total Reward -2197.527381381121\n",
      "Episode 300: Total Reward -169\n",
      "Episode 320: Total Reward -657.8522275932631\n",
      "Episode 340: Total Reward -807.6356222471732\n",
      "Episode 360: Total Reward -34\n",
      "Episode 380: Total Reward -828.447881634472\n",
      "Episode 400: Total Reward -18556.517570182594\n",
      "Episode 420: Total Reward -18777.21413540925\n",
      "Episode 440: Total Reward -17582.17281036765\n",
      "Episode 460: Total Reward -2288.654230743785\n",
      "Episode 480: Total Reward -9741.224507118657\n",
      "Episode 500: Total Reward -784.3692811882651\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Assuming necessary functions from your prior code\n",
    "def create_tiling(feat_range, bins, offset):\n",
    "    return np.linspace(feat_range[0], feat_range[1], bins + 1)[1:-1] + offset\n",
    "\n",
    "def create_tilings(feature_ranges, number_tilings, bins, offsets):\n",
    "    tilings = []\n",
    "    for tile_i in range(number_tilings):\n",
    "        tiling = [create_tiling(feature_ranges[feat_i], bins[tile_i][feat_i], offsets[tile_i][feat_i])\n",
    "                  for feat_i in range(len(feature_ranges))]\n",
    "        tilings.append(tiling)\n",
    "    return np.array(tilings)\n",
    "\n",
    "def get_tile_coding(feature, tilings):\n",
    "    num_dims = len(feature)\n",
    "    feat_codings = []\n",
    "    for tiling in tilings:\n",
    "        feat_coding = [np.digitize(feature[i], tiling[i]) for i in range(num_dims)]\n",
    "        feat_codings.append(feat_coding)\n",
    "    return np.array(feat_codings)\n",
    "\n",
    "class QValueFunction:\n",
    "    def __init__(self, tilings, num_actions, lr):\n",
    "        self.tilings = tilings\n",
    "        self.num_tilings = len(self.tilings)\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.state_sizes = [tuple(len(splits) + 1 for splits in tiling) for tiling in self.tilings]\n",
    "        self.q_tables = [np.zeros(shape=(*state_size, num_actions)) for state_size in self.state_sizes]\n",
    "\n",
    "    def value(self, state, action):\n",
    "        state_codings = get_tile_coding(state, self.tilings)\n",
    "        value = 0\n",
    "        for coding, q_table in zip(state_codings, self.q_tables):\n",
    "            value += q_table[tuple(coding) + (action,)]\n",
    "        return value / self.num_tilings\n",
    "\n",
    "    def update(self, state, action, target):\n",
    "        state_codings = get_tile_coding(state, self.tilings)\n",
    "        for coding, q_table in zip(state_codings, self.q_tables):\n",
    "            q_value = q_table[tuple(coding) + (action,)]\n",
    "            q_table[tuple(coding) + (action,)] += self.lr * (target - q_value)\n",
    "\n",
    "def simulate_episode(env, q_func, eps, gamma):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    trunc = False  # To capture truncation condition as well\n",
    "\n",
    "    # Choose action using epsilon-greedy policy\n",
    "    if np.random.random() < eps:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        q_values = [q_func.value(state, i) for i in range(env.action_space.n)]\n",
    "        action = np.argmax(q_values)\n",
    "\n",
    "    while not done and not trunc:  # We check both done and truncated\n",
    "        next_state, reward, done, trunc, _ = env.step(action)  # Update here\n",
    "        total_reward += reward\n",
    "\n",
    "        # Choose next action using epsilon-greedy policy\n",
    "        if np.random.random() < eps:\n",
    "            next_action = env.action_space.sample()\n",
    "        else:\n",
    "            next_q_values = [q_func.value(next_state, i) for i in range(env.action_space.n)]\n",
    "            next_action = np.argmax(next_q_values)\n",
    "\n",
    "        if not done and not trunc:  # Consider next state only if not done and not truncated\n",
    "            next_q = q_func.value(next_state, next_action)\n",
    "        else:\n",
    "            next_q = 0\n",
    "        \n",
    "        # SARSA update\n",
    "        q_value = q_func.value(state, action)\n",
    "        target = reward + gamma * next_q\n",
    "        q_func.update(state, action, target)\n",
    "\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "\n",
    "tilings = create_tilings([env.observation_space.low, env.observation_space.high], 3,\n",
    "                        [[10, 10], [10, 10], [10,10]], [[0, 0], [0.2, 0.2], [0.3, 0.3]])\n",
    "q_func = QValueFunction(tilings, env.action_space.n, 0.05)\n",
    "\n",
    "num_episodes = 500\n",
    "eps = 0.15\n",
    "gamma = 0.95\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    reward = simulate_episode(env, q_func, eps, gamma)\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"Episode {i + 1}: Total Reward {reward}\")\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning with Tile coding and eligibility trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20: Total Reward -12917.15819750827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/73/1wstfjwn76s193_38n9l19_h0000gn/T/ipykernel_1579/2849309478.py:39: RuntimeWarning: overflow encountered in scalar add\n",
      "  value += q_table[tuple(coding) + (action,)]\n",
      "/var/folders/73/1wstfjwn76s193_38n9l19_h0000gn/T/ipykernel_1579/2849309478.py:48: RuntimeWarning: invalid value encountered in multiply\n",
      "  q_table += self.lr * delta * self.traces[i]\n",
      "/var/folders/73/1wstfjwn76s193_38n9l19_h0000gn/T/ipykernel_1579/2849309478.py:46: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  delta = target - q_value\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 96\u001b[0m\n\u001b[1;32m     94\u001b[0m gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.95\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m---> 96\u001b[0m     reward \u001b[38;5;241m=\u001b[39m simulate_episode(env, q_func, eps, gamma)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Total Reward \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[41], line 68\u001b[0m, in \u001b[0;36msimulate_episode\u001b[0;34m(env, q_func, eps, gamma)\u001b[0m\n\u001b[1;32m     66\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m [q_func\u001b[38;5;241m.\u001b[39mvalue(state, i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)]\n\u001b[1;32m     69\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(q_values)\n\u001b[1;32m     71\u001b[0m next_state, reward, done, trunc, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)  \u001b[38;5;66;03m# Update here\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[41], line 68\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m [q_func\u001b[38;5;241m.\u001b[39mvalue(state, i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)]\n\u001b[1;32m     69\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(q_values)\n\u001b[1;32m     71\u001b[0m next_state, reward, done, trunc, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)  \u001b[38;5;66;03m# Update here\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Assuming necessary functions from your prior code\n",
    "def create_tiling(feat_range, bins, offset):\n",
    "    return np.linspace(feat_range[0], feat_range[1], bins + 1)[1:-1] + offset\n",
    "\n",
    "def create_tilings(feature_ranges, number_tilings, bins, offsets):\n",
    "    tilings = []\n",
    "    for tile_i in range(number_tilings):\n",
    "        tiling = [create_tiling(feature_ranges[feat_i], bins[tile_i][feat_i], offsets[tile_i][feat_i])\n",
    "                  for feat_i in range(len(feature_ranges))]\n",
    "        tilings.append(tiling)\n",
    "    return np.array(tilings)\n",
    "\n",
    "def get_tile_coding(feature, tilings):\n",
    "    num_dims = len(feature)\n",
    "    feat_codings = []\n",
    "    for tiling in tilings:\n",
    "        feat_coding = [np.digitize(feature[i], tiling[i]) for i in range(num_dims)]\n",
    "        feat_codings.append(feat_coding)\n",
    "    return np.array(feat_codings)\n",
    "\n",
    "class QValueFunction:\n",
    "    def __init__(self, tilings, num_actions, lr, lambda_val):\n",
    "        self.tilings = tilings\n",
    "        self.num_tilings = len(self.tilings)\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.lambda_val = lambda_val  # λ value for eligibility trace\n",
    "        self.state_sizes = [tuple(len(splits) + 1 for splits in tiling) for tiling in self.tilings]\n",
    "        self.q_tables = [np.zeros(shape=(*state_size, num_actions)) for state_size in self.state_sizes]\n",
    "        self.traces = [np.zeros_like(q_table) for q_table in self.q_tables]\n",
    "\n",
    "    def value(self, state, action):\n",
    "        state_codings = get_tile_coding(state, self.tilings)\n",
    "        value = 0\n",
    "        for coding, q_table in zip(state_codings, self.q_tables):\n",
    "            value += q_table[tuple(coding) + (action,)]\n",
    "        return value / self.num_tilings\n",
    "\n",
    "    def update(self, state, action, target):\n",
    "        state_codings = get_tile_coding(state, self.tilings)\n",
    "        for i, (coding, q_table) in enumerate(zip(state_codings, self.q_tables)):\n",
    "            q_value = q_table[tuple(coding) + (action,)]\n",
    "            delta = target - q_value\n",
    "            self.traces[i][tuple(coding) + (action,)] += 1  # Accumulating traces\n",
    "            q_table += self.lr * delta * self.traces[i]\n",
    "            self.traces[i] *= self.lambda_val  # Decay traces by λγ\n",
    "\n",
    "    def reset_traces(self):\n",
    "        for trace in self.traces:\n",
    "            trace.fill(0)\n",
    "\n",
    "# Modify your simulation function to reset traces at the beginning of each episode\n",
    "def simulate_episode(env, q_func, eps, gamma):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    trunc = False  # To capture truncation condition as well\n",
    "\n",
    "    q_func.reset_traces()  # Reset traces at the beginning of each episode\n",
    "\n",
    "    while not done and not trunc:  # We check both done and truncated\n",
    "        if np.random.random() < eps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            q_values = [q_func.value(state, i) for i in range(env.action_space.n)]\n",
    "            action = np.argmax(q_values)\n",
    "\n",
    "        next_state, reward, done, trunc, _ = env.step(action)  # Update here\n",
    "        total_reward += reward\n",
    "        \n",
    "        if not done and not trunc:  # Consider next state only if not done and not truncated\n",
    "            next_q_values = [q_func.value(next_state, i) for i in range(env.action_space.n)]\n",
    "            max_next_q = max(next_q_values)\n",
    "        else:\n",
    "            max_next_q = 0\n",
    "        \n",
    "        \n",
    "        target = reward + gamma * max_next_q\n",
    "        q_func.update(state, action, target)\n",
    "        state = next_state\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "\n",
    "tilings = create_tilings([env.observation_space.low, env.observation_space.high], 3,\n",
    "                        [[5, 5], [5, 5],[5, 5]], [[0, 0], [0.1, 0.1],[0.2, 0.2]])\n",
    "q_func = QValueFunction(tilings, env.action_space.n, lr=0.05, lambda_val=0.9)\n",
    "\n",
    "num_episodes = 500\n",
    "eps = 0.2\n",
    "gamma = 0.95\n",
    "for i in range(num_episodes):\n",
    "    reward = simulate_episode(env, q_func, eps, gamma)\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"Episode {i + 1}: Total Reward {reward}\")\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " t: 0, observation: [0.21265458 0.3432404 ], reward: -1\n",
      " t: 1, observation: [0.19890826 0.29505526], reward: -1\n",
      " t: 2, observation: [0.20619876 0.24800973], reward: -1\n",
      " t: 3, observation: [0.20769342 0.19246139], reward: -1\n",
      " t: 4, observation: [0.20625985 0.15519452], reward: -1\n",
      " t: 5, observation: [0.19026465 0.10831192], reward: -1\n",
      " t: 6, observation: [0.17036853 0.05698193], reward: -1\n",
      " t: 7, observation: [0.18510501 0.        ], reward: -1\n",
      " t: 8, observation: [0.19091667 0.        ], reward: -1\n",
      " t: 9, observation: [0.18138339 0.        ], reward: -1\n",
      "no episode finished in this run.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "obs, info = env.reset()\n",
    "total_reward = 0\n",
    "episode_rewards = []\n",
    "frames = []\n",
    "observation = obs\n",
    "for time_step in range(100):\n",
    "    q_values = [q_func.value(observation, i) for i in range(env.action_space.n)]\n",
    "    action = np.argmax(q_values)\n",
    "    observation, reward, done, trunc, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "    image = env.render()\n",
    "    #online_rendering(image) #uncomment this line to see the online rendering of the environment frame by frame\n",
    "    frames.append(image)\n",
    "\n",
    "    print(f\" t: {time_step}, observation: {observation}, reward: {reward}\") #uncomment this line to see the environment-agent interaction details\n",
    "\n",
    "    if done:\n",
    "      print(f\"total reward in this episode: {total_reward}\")\n",
    "      episode_rewards.append(total_reward)\n",
    "      total_reward = 0\n",
    "      break\n",
    "\n",
    "env.close()\n",
    "\n",
    "if episode_rewards == []:\n",
    "  print(\"no episode finished in this run.\")\n",
    "else:\n",
    "  for i, reward in enumerate(episode_rewards):\n",
    "    print(f\"episode {i}: reward: {reward}\")\n",
    "\n",
    "visualize(frames, \"q_learning_ET.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def greedy_policy(state, q_function):\n",
    "    q_values = [q_function.value(state, i) for i in range(env.action_space.n)]\n",
    "    action = np.argmax(q_values)\n",
    "    return action\n",
    "\n",
    "def test_agent(env, q_function):\n",
    "    obs, info = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    trunc = False\n",
    "    while not done and not trunc:  # We check both done and truncated\n",
    "        action = greedy_policy(obs, q_function)\n",
    "        next_state, reward, done, trunc, _ = env.step(action)  # Update here\n",
    "        total_reward += reward\n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Test Reward: -12736.5\n"
     ]
    }
   ],
   "source": [
    "# Test the trained agent\n",
    "test_results = [test_agent(env, q_func) for _ in range(10)]\n",
    "avg_test_reward = np.mean(test_results)\n",
    "print(f\"Avg. Test Reward: {avg_test_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
