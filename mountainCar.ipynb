{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"MountainCarContinuous-v0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (1, 2) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Sample action according to current policy\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(scale_state(state), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     68\u001b[0m     mu, sigma \u001b[38;5;241m=\u001b[39m policy_net(state_tensor)\n\u001b[1;32m     69\u001b[0m     norm_dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mNormal(mu, sigma)\n",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m, in \u001b[0;36mscale_state\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscale_state\u001b[39m(state):\n\u001b[0;32m---> 39\u001b[0m     scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform([state])\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scaled[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:992\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    989\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    991\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m--> 992\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    993\u001b[0m     X,\n\u001b[1;32m    994\u001b[0m     reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    995\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    996\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    997\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[1;32m    998\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    999\u001b[0m )\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    883\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    182\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(array)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (1, 2) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample from state space for state normalization\n",
    "state_space_samples = np.array([env.observation_space.sample() for _ in range(10000)])\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(state_space_samples)\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, n_actions):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dims, 40)\n",
    "        self.fc2 = nn.Linear(40, 40)\n",
    "        self.mu = nn.Linear(40, n_actions)\n",
    "        self.sigma = nn.Linear(40, n_actions)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.tanh(self.fc1(state))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        mu = self.mu(x)\n",
    "        sigma = torch.nn.functional.softplus(self.sigma(x)) + 1e-5\n",
    "        return mu, sigma\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dims):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dims, 400)\n",
    "        self.fc2 = nn.Linear(400, 400)\n",
    "        self.V = nn.Linear(400, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.tanh(self.fc1(state))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        V = self.V(x)\n",
    "        return V\n",
    "\n",
    "\n",
    "# Function to normalize states\n",
    "def scale_state(state):\n",
    "    scaled = scaler.transform([state])\n",
    "    return scaled[0]\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "lr_actor = 0.00002\n",
    "lr_critic = 0.001\n",
    "gamma = 0.99\n",
    "num_episodes = 300\n",
    "input_dims = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.shape[0]\n",
    "\n",
    "# Initialize networks and optimizers\n",
    "policy_net = PolicyNetwork(input_dims, n_actions)\n",
    "value_net = ValueNetwork(input_dims)\n",
    "optimizer_actor = optim.Adam(policy_net.parameters(), lr=lr_actor)\n",
    "optimizer_critic = optim.Adam(value_net.parameters(), lr=lr_critic)\n",
    "\n",
    "# Training loop\n",
    "episode_history = []\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    reward_total = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Sample action according to current policy\n",
    "        state_tensor = torch.tensor(scale_state(state), dtype=torch.float32)\n",
    "        mu, sigma = policy_net(state_tensor)\n",
    "        norm_dist = torch.distributions.Normal(mu, sigma)\n",
    "        action_tensor = norm_dist.sample()\n",
    "        action = action_tensor.detach().numpy()\n",
    "\n",
    "        # Execute action and observe reward & next state\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        steps += 1\n",
    "        reward_total += reward\n",
    "\n",
    "        # Compute TD target\n",
    "        next_state_tensor = torch.tensor(scale_state(next_state), dtype=torch.float32)\n",
    "        V_next_state = value_net(next_state_tensor)\n",
    "        target = reward + gamma * V_next_state\n",
    "\n",
    "        # Compute TD error\n",
    "        V_state = value_net(state_tensor)\n",
    "        td_error = target - V_state\n",
    "\n",
    "        # Update actor (policy) network\n",
    "        loss_actor = -norm_dist.log_prob(action_tensor) * td_error.detach()\n",
    "        optimizer_actor.zero_grad()\n",
    "        loss_actor.mean().backward()\n",
    "        optimizer_actor.step()\n",
    "\n",
    "        # Update critic (value) network\n",
    "        loss_critic = nn.functional.mse_loss(V_state, target.detach())\n",
    "        optimizer_critic.zero_grad()\n",
    "        loss_critic.backward()\n",
    "        optimizer_critic.step()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    episode_history.append(reward_total)\n",
    "    print(\"Episode: {}, Number of Steps: {}, Cumulative reward: {:.2f}\".format(\n",
    "        episode, steps, reward_total))\n",
    "\n",
    "    # Check for solving criteria\n",
    "    if np.mean(episode_history[-100:]) > 90 and len(episode_history) >= 101:\n",
    "        print(\"****************Solved***************\")\n",
    "        print(\"Mean cumulative reward over 100 episodes: {:.2f}\".format(\n",
    "            np.mean(episode_history[-100:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mreset_default_graph()\n\u001b[1;32m      8\u001b[0m input_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 9\u001b[0m state_placeholder \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mplaceholder(tf\u001b[38;5;241m.\u001b[39mfloat32, [\u001b[38;5;28;01mNone\u001b[39;00m, input_dims]) \n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalue_function\u001b[39m(state):\n\u001b[1;32m     12\u001b[0m     n_hidden1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m  \n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym  #requires OpenAI gym installed\n",
    "env = gym.envs.make(\"MountainCarContinuous-v0\") \n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "input_dims = 2\n",
    "state_placeholder = tf.placeholder(tf.float32, [None, input_dims]) \n",
    "\n",
    "def value_function(state):\n",
    "    n_hidden1 = 400  \n",
    "    n_hidden2 = 400\n",
    "    n_outputs = 1\n",
    "    \n",
    "    with tf.variable_scope(\"value_network\"):\n",
    "        init_xavier = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        hidden1 = tf.layers.dense(state, n_hidden1, tf.nn.elu, init_xavier)\n",
    "        hidden2 = tf.layers.dense(hidden1, n_hidden2, tf.nn.elu, init_xavier) \n",
    "        V = tf.layers.dense(hidden2, n_outputs, None, init_xavier)\n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_network(state):\n",
    "    n_hidden1 = 40\n",
    "    n_hidden2 = 40\n",
    "    n_outputs = 1\n",
    "    \n",
    "    with tf.variable_scope(\"policy_network\"):\n",
    "        init_xavier = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        hidden1 = tf.layers.dense(state, n_hidden1, tf.nn.elu, init_xavier)\n",
    "        hidden2 = tf.layers.dense(hidden1, n_hidden2, tf.nn.elu, init_xavier)\n",
    "        mu = tf.layers.dense(hidden2, n_outputs, None, init_xavier)\n",
    "        sigma = tf.layers.dense(hidden2, n_outputs, None, init_xavier)\n",
    "        sigma = tf.nn.softplus(sigma) + 1e-5\n",
    "        norm_dist = tf.contrib.distributions.Normal(mu, sigma)\n",
    "        action_tf_var = tf.squeeze(norm_dist.sample(1), axis=0)\n",
    "        action_tf_var = tf.clip_by_value(\n",
    "            action_tf_var, env.action_space.low[0], \n",
    "            env.action_space.high[0])\n",
    "    return action_tf_var, norm_dist\n",
    "\n",
    "################################################################\n",
    "#sample from state space for state normalization\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "                                    \n",
    "state_space_samples = np.array(\n",
    "    [env.observation_space.sample() for x in range(10000)])\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(state_space_samples)\n",
    "\n",
    "#function to normalize states\n",
    "def scale_state(state):                 #requires input shape=(2,)\n",
    "    scaled = scaler.transform([state])\n",
    "    return scaled                       #returns shape =(1,2)   \n",
    "###################################################################\n",
    "\n",
    "lr_actor = 0.00002  #set learning rates\n",
    "lr_critic = 0.001\n",
    "\n",
    "# define required placeholders\n",
    "action_placeholder = tf.placeholder(tf.float32)\n",
    "delta_placeholder = tf.placeholder(tf.float32)\n",
    "target_placeholder = tf.placeholder(tf.float32)\n",
    "\n",
    "action_tf_var, norm_dist = policy_network(state_placeholder)\n",
    "V = value_function(state_placeholder)\n",
    "\n",
    "# define actor (policy) loss function\n",
    "loss_actor = -tf.log(norm_dist.prob(action_placeholder) + 1e-5) * delta_placeholder\n",
    "training_op_actor = tf.train.AdamOptimizer(\n",
    "    lr_actor, name='actor_optimizer').minimize(loss_actor)\n",
    "\n",
    "# define critic (state-value) loss function\n",
    "loss_critic = tf.reduce_mean(tf.squared_difference(\n",
    "                             tf.squeeze(V), target_placeholder))\n",
    "training_op_critic = tf.train.AdamOptimizer(\n",
    "        lr_critic, name='critic_optimizer').minimize(loss_critic)\n",
    "################################################################\n",
    "#Training loop\n",
    "gamma = 0.99        #discount factor\n",
    "num_episodes = 300\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    episode_history = []\n",
    "    for episode in range(num_episodes):\n",
    "        #receive initial state from E\n",
    "        state = env.reset()   # state.shape -> (2,)\n",
    "        reward_total = 0 \n",
    "        steps = 0\n",
    "        done = False\n",
    "        while (not done):\n",
    "                \n",
    "            #Sample action according to current policy\n",
    "            #action.shape = (1,1)\n",
    "            action  = sess.run(action_tf_var, feed_dict={\n",
    "                          state_placeholder: scale_state(state)})\n",
    "            #Execute action and observe reward & next state from E\n",
    "            # next_state shape=(2,)    \n",
    "            #env.step() requires input shape = (1,)\n",
    "            next_state, reward, done, _ = env.step(\n",
    "                                    np.squeeze(action, axis=0)) \n",
    "            steps +=1\n",
    "            reward_total += reward\n",
    "            #V_of_next_state.shape=(1,1)\n",
    "            V_of_next_state = sess.run(V, feed_dict = \n",
    "                    {state_placeholder: scale_state(next_state)})  \n",
    "            #Set TD Target\n",
    "            #target = r + gamma * V(next_state)     \n",
    "            target = reward + gamma * np.squeeze(V_of_next_state) \n",
    "            \n",
    "            # td_error = target - V(s)\n",
    "            #needed to feed delta_placeholder in actor training\n",
    "            td_error = target - np.squeeze(sess.run(V, feed_dict = \n",
    "                        {state_placeholder: scale_state(state)})) \n",
    "            \n",
    "            #Update actor by minimizing loss (Actor training)\n",
    "            _, loss_actor_val  = sess.run(\n",
    "                [training_op_actor, loss_actor], \n",
    "                feed_dict={action_placeholder: np.squeeze(action), \n",
    "                state_placeholder: scale_state(state), \n",
    "                delta_placeholder: td_error})\n",
    "            #Update critic by minimizinf loss  (Critic training)\n",
    "            _, loss_critic_val  = sess.run(\n",
    "                [training_op_critic, loss_critic], \n",
    "                feed_dict={state_placeholder: scale_state(state), \n",
    "                target_placeholder: target})\n",
    "            \n",
    "            state = next_state\n",
    "            #end while\n",
    "        episode_history.append(reward_total)\n",
    "        print(\"Episode: {}, Number of Steps : {}, Cumulative reward: {:0.2f}\".format(\n",
    "            episode, steps, reward_total))\n",
    "        \n",
    "        if np.mean(episode_history[-100:]) > 90 and len(episode_history) >= 101:\n",
    "            print(\"****************Solved***************\")\n",
    "            print(\"Mean cumulative reward over 100 episodes:{:0.2f}\" .format(\n",
    "                np.mean(episode_history[-100:])))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import and initialize Mountain Car Environment\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "\n",
    "# Define Q-learning function\n",
    "def QLearning(env, learning, discount, epsilon, min_eps, episodes):\n",
    "    # Determine size of discretized state space\n",
    "    num_states = (env.observation_space.high - env.observation_space.low)*\\\n",
    "                    np.array([10, 100])\n",
    "    num_states = np.round(num_states, 0).astype(int) + 1\n",
    "    \n",
    "    # Initialize Q table\n",
    "    Q = np.random.uniform(low = -1, high = 1, \n",
    "                          size = (num_states[0], num_states[1], \n",
    "                                  env.action_space.n))\n",
    "    \n",
    "    # Initialize variables to track rewards\n",
    "    reward_list = []\n",
    "    ave_reward_list = []\n",
    "    \n",
    "    # Calculate episodic reduction in epsilon\n",
    "    reduction = (epsilon - min_eps)/episodes\n",
    "    \n",
    "    # Run Q learning algorithm\n",
    "    for i in range(episodes):\n",
    "        # Initialize parameters\n",
    "        done = False\n",
    "        tot_reward, reward = 0,0\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Discretize state\n",
    "        state_adj = (state - env.observation_space.low)*np.array([10, 100])\n",
    "        state_adj = np.round(state_adj, 0).astype(int)\n",
    "    \n",
    "        while done != True:   \n",
    "            # Render environment for last five episodes\n",
    "            if i >= (episodes - 20):\n",
    "                env.render()\n",
    "                \n",
    "            # Determine next action - epsilon greedy strategy\n",
    "            if np.random.random() < 1 - epsilon:\n",
    "                action = np.argmax(Q[state_adj[0], state_adj[1]]) \n",
    "            else:\n",
    "                action = np.random.randint(0, env.action_space.n)\n",
    "                \n",
    "            # Get next state and reward\n",
    "            state2, reward, done, info = env.step(action) \n",
    "            \n",
    "            # Discretize state2\n",
    "            state2_adj = (state2 - env.observation_space.low)*np.array([10, 100])\n",
    "            state2_adj = np.round(state2_adj, 0).astype(int)\n",
    "            \n",
    "            #Allow for terminal states\n",
    "            if done and state2[0] >= 0.5:\n",
    "                Q[state_adj[0], state_adj[1], action] = reward\n",
    "                \n",
    "            # Adjust Q value for current state\n",
    "            else:\n",
    "                delta = learning*(reward + \n",
    "                                 discount*np.max(Q[state2_adj[0], \n",
    "                                                   state2_adj[1]]) - \n",
    "                                 Q[state_adj[0], state_adj[1],action])\n",
    "                Q[state_adj[0], state_adj[1],action] += delta\n",
    "                                     \n",
    "            # Update variables\n",
    "            tot_reward += reward\n",
    "            state_adj = state2_adj\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if epsilon > min_eps:\n",
    "            epsilon -= reduction\n",
    "        \n",
    "        # Track rewards\n",
    "        reward_list.append(tot_reward)\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            ave_reward = np.mean(reward_list)\n",
    "            ave_reward_list.append(ave_reward)\n",
    "            reward_list = []\n",
    "            \n",
    "    \n",
    "    \n",
    "    return ave_reward_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
